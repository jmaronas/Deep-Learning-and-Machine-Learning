{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fdf4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839da1f1",
   "metadata": {},
   "source": [
    "#### **REQUIREMENTS BEFORE IMPLEMENTATION**\n",
    "Before undergoing through this notebook you should download the dataset from the following link:\n",
    "- https://www.kaggle.com/code/mbalvi75/08-knn-diabetes-dataset/input\n",
    "\n",
    "Once you have the data you will divide it into two different files:\n",
    "- **\"train\"** = 3/4 of the data  \n",
    "- **\"test\"** = 1/4\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9253e80",
   "metadata": {},
   "source": [
    "### **KNN FUNDAMENTALS**\n",
    "\n",
    "**KNN ALGORITHM**\n",
    "KNN is one of the simplest machine learning algorithms. One of its advantages is that it is a non-parametric algorithm, which means that there is no assumption about the data distribution. It requires a dataset with training examples as input, and the output can be either a classification or used for regression purposes.\n",
    "\n",
    "- In **KNN classification**, the output is a class membership. The given data point is classified based on the majority of type of its neighbours. The data point is assigned to the most frequent class among its k nearest neighbours. Meaning for example that if K=1, then its class will be the same as it closest neighbour.\n",
    "\n",
    "**KNN INTUITION**\n",
    "The algorithm is very simple, it just calculates the distance between the k number of points from the training dataset and the point we want to analyze and assiging the class type of the closest ones.\n",
    "\n",
    "Example:\n",
    "- Suppose, we have a dataset with two variables which are classified as Red and Blue.\n",
    "\n",
    "- In kNN algorithm, k is the number of nearest neighbours. Generally, k is an odd number because it helps to decide the majority of the class. When k=1, then the algorithm is known as the nearest neighbour algorithm.\n",
    "\n",
    "- Now, we want to classify a new data point X into Blue class or Red class. Suppose the value of k is 3. The KNN algorithm starts by calculating the distance between X and all the other data points. It then finds the 3 nearest points with least distance to point X.\n",
    "\n",
    "- In the final step of the kNN algorithm, we assign the new data point X to the majority of the class of the 3 nearest points. If 2 of the 3 nearest points belong to the class Red while 1 belong to the class Blue, then we classify the new data point as Red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe214aa",
   "metadata": {},
   "source": [
    "### **KNN PRESENTATION**\n",
    "\n",
    "- Library declaration\n",
    "-First, we will create a class to declare the KNN algorithm\n",
    "-Then we will load a data model\n",
    "-Our model consists of different variables to predict whether a patient has or does not have diabetes\n",
    "-We will choose 3 combinations of variables to which we will apply KNN\n",
    "-We will load the test data model and analyze the performance for each combination\n",
    "\n",
    "\n",
    "**OBJECTIVE:** find the combination of variables that best predicts whether a patient does or does not have diabetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c83f31",
   "metadata": {},
   "source": [
    "#### **DEFINITION OF KNN CLASS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10532bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Calculate the Euclidean distance to the xi in the training set\n",
    "        distances = np.linalg.norm(self.X_train - x, axis=1)\n",
    "        # Obtains the indices of the k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        # k determine the labels of the k nearest neighbors \n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        \n",
    "        most_common = np.bincount(k_nearest_labels).argmax()\n",
    "        return most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6206f",
   "metadata": {},
   "source": [
    "### **CHOOSING VARIABLES AND LOADING THE TRAINING DATASET**\n",
    "\n",
    "- Loads the model test\n",
    "- In this case the following combinations of variables will be choosen to apply KNN algorithm:\n",
    "    1. Glucose y BloodPressure\n",
    "    2. SkinThickness y BMI\n",
    "    3. Glucose y BMI\n",
    "- Plots graphics of the training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f31bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "print(train.head(5))\n",
    "\n",
    "# First combination of variables (Glucose and BloodPressure)\n",
    "x1 = train[['Glucose', 'BloodPressure']].values\n",
    "y1 = train['Outcome'].values\n",
    "# Visualizes the distribution of the data in the training set\n",
    "colors = {0: 'cyan', 1: 'blue'}\n",
    "\n",
    "point_colors = [colors[cls] for cls in y1]\n",
    "plt.scatter(x1[:, 0], x1[:, 1], c=point_colors)\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('BloodPressure')\n",
    "plt.title('Glucose vs BloodPressure \"TRAIN DATA\"')\n",
    "plt.show()\n",
    "\n",
    "# Second combination of variables (SkinThickness and BMI)\n",
    "x2 = train[['SkinThickness', 'BMI']].values\n",
    "y2 = train['Outcome'].values\n",
    "# Visualizes the distribution of the data in the training set\n",
    "point_colors = [colors[cls] for cls in y2]\n",
    "plt.scatter(x2[:, 0], x2[:, 1], c=point_colors)\n",
    "plt.xlabel('SkinThickness')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('SkinThickness vs BMI \"TRAIN DATA\"')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Third combination of variables (Glucose and BMI)\n",
    "x3 = train[['Glucose', 'BMI']].values\n",
    "y3 = train['Outcome'].values\n",
    "\n",
    "# Visualizes the distribution of the data in the training set\n",
    "point_colors = [colors[cls] for cls in y3]\n",
    "plt.scatter(x3[:, 0], x3[:, 1], c=point_colors)\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('Glucose vs BMI \"TRAIN DATA\"')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed01d2",
   "metadata": {},
   "source": [
    "### **DATA TRAINING AND PREDICTION**\n",
    "\n",
    "- Train the model using the KNN fit function and selects the value of K (the number of neighbors) that we will consider.\n",
    "- Load the data from test.\n",
    "- Make predictions  using the test data variables.\n",
    "- Determines how accurate is our KNN for the selected number of K neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ccfe15",
   "metadata": {},
   "source": [
    "Evaluation of changes in K\n",
    "To evaluate diferent changes on the K value we are going to use the combitation between Glucose and BMI and we are going to see how the efficiency changes and the data plotted.\n",
    "\n",
    "#### **LOW K VALUE**\n",
    "\n",
    "A low K value can mean that the predicttion made isn't accuarate, for example taking  noisy data wich could lead to overfitting, also a low K means that the algorithm tries to memorize the data from the training dataset, another problem could be that the nearest neighbor belongs to the wrong class do to many causes such as randomness, noise or measurement errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN for the first combination of variables\n",
    "knn1 = KNN(k=1)\n",
    "knn1.fit(x1, y1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "x1_test = test[['Glucose', 'BloodPressure']].values\n",
    "y1_test = test['Outcome'].values\n",
    "predictions1 = knn1.predict(x1_test)\n",
    "rendiemiento1 = np.sum(predictions1 == y1_test) / len(y1_test)\n",
    "print(f'Performance for Glucose y BloodPressure: {rendiemiento1:.2f}')\n",
    "\n",
    "# Train KNN for the second combination of variables\n",
    "knn2 = KNN(k=1)\n",
    "knn2.fit(x2, y2)\n",
    "x2_test = test[['SkinThickness', 'BMI']].values\n",
    "y2_test = test['Outcome'].values\n",
    "predictions2 = knn2.predict(x2_test)\n",
    "rendiemiento2 = np.sum(predictions2 == y2_test) / len(y2_test)\n",
    "print(f'Performance for SkinThickness y BMI: {rendiemiento2:.2f}')\n",
    "\n",
    "# Train KNN for the third combination of variables\n",
    "knn3 = KNN(k=1)\n",
    "knn3.fit(x3, y3)\n",
    "x3_test = test[['Glucose', 'BMI']].values\n",
    "y3_test = test['Outcome'].values\n",
    "predictions3 = knn3.predict(x3_test)\n",
    "rendiemiento3 = np.sum(predictions3 == y3_test) / len(y3_test)\n",
    "print(f'Performance for Glucose y BMI: {rendiemiento3:.2f}')\n",
    "\n",
    "# Visualizes the predictions for the test set\n",
    "plt.scatter(\n",
    "    x1_test[:, 0],\n",
    "    x1_test[:, 1],\n",
    "    c=[colors[p] for p in predictions1]\n",
    ")\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('BloodPressure')\n",
    "plt.title('Glucose vs BloodPressure \"PREDICTED DATA\"')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(\n",
    "    x2_test[:, 0],\n",
    "    x2_test[:, 1],\n",
    "    c=[colors[p] for p in predictions2]\n",
    ")\n",
    "plt.xlabel('SkinThickness')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('SkinThickness vs BMI \"PREDICTED DATA\"')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(\n",
    "    x3_test[:, 0],\n",
    "    x3_test[:, 1],\n",
    "    c=[colors[p] for p in predictions3]\n",
    ")\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('Glucose vs BMI \"PREDICTED DATA\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35f2b2",
   "metadata": {},
   "source": [
    "#### **HIGH K EXAMPLE**\n",
    "\n",
    "A too big K means that we use a lot of the data from our training dataset, that makes the model too simple because it makes the average of the distance of all, meaning that you can be missing local structures leading to underfitting making the model unnable to capture complex class shapes.\n",
    "Also one of the most important problems of a high K is the dominnance of a class, because it will usually benefict the most common one.no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3634da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN for the first combination of variables\n",
    "knn1 = KNN(k=500)\n",
    "knn1.fit(x1, y1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "x1_test = test[['Glucose', 'BloodPressure']].values\n",
    "y1_test = test['Outcome'].values\n",
    "predictions1 = knn1.predict(x1_test)\n",
    "rendiemiento1 = np.sum(predictions1 == y1_test) / len(y1_test)\n",
    "print(f'Performance for Glucose y BloodPressure: {rendiemiento1:.2f}')\n",
    "\n",
    "# Train KNN for the second combination of variables\n",
    "knn2 = KNN(k=500)\n",
    "knn2.fit(x2, y2)\n",
    "x2_test = test[['SkinThickness', 'BMI']].values\n",
    "y2_test = test['Outcome'].values\n",
    "predictions2 = knn2.predict(x2_test)\n",
    "rendiemiento2 = np.sum(predictions2 == y2_test) / len(y2_test)\n",
    "print(f'Performance for SkinThickness y BMI: {rendiemiento2:.2f}')\n",
    "\n",
    "# Train KNN for the third combination of variables\n",
    "knn3 = KNN(k=500)\n",
    "knn3.fit(x3, y3)\n",
    "x3_test = test[['Glucose', 'BMI']].values\n",
    "y3_test = test['Outcome'].values\n",
    "predictions3 = knn3.predict(x3_test)\n",
    "rendiemiento3 = np.sum(predictions3 == y3_test) / len(y3_test)\n",
    "print(f'Performance for Glucose y BMI: {rendiemiento3:.2f}')\n",
    "\n",
    "# Visualizes the predictions for the test set\n",
    "plt.scatter(\n",
    "    x1_test[:, 0],\n",
    "    x1_test[:, 1],\n",
    "    c=[colors[p] for p in predictions1]\n",
    ")\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('BloodPressure')\n",
    "plt.title('Glucose vs BloodPressure \"PREDICTED DATA\"')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(\n",
    "    x2_test[:, 0],\n",
    "    x2_test[:, 1],\n",
    "    c=[colors[p] for p in predictions2]\n",
    ")\n",
    "plt.xlabel('SkinThickness')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('SkinThickness vs BMI \"PREDICTED DATA\"')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(\n",
    "    x3_test[:, 0],\n",
    "    x3_test[:, 1],\n",
    "    c=[colors[p] for p in predictions3]\n",
    ")\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('Glucose vs BMI \"PREDICTED DATA\"')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e2722",
   "metadata": {},
   "source": [
    "#### **OPTIMAL K VALUE**\n",
    "\n",
    "So whats the optimun number of K?\n",
    "A common rule is to make K the squared root of the number of samples:\n",
    "    K=√N\n",
    "Another important thing is to use odd numbers so you can avoid ties.\n",
    "And the best way to choose is by trying and measuring the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c81bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN for the first combination of variables\n",
    "knn1 = KNN(k=12)\n",
    "knn1.fit(x1, y1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "x1_test = test[['Glucose', 'BloodPressure']].values\n",
    "y1_test = test['Outcome'].values\n",
    "predictions1 = knn1.predict(x1_test)\n",
    "rendiemiento1 = np.sum(predictions1 == y1_test) / len(y1_test)\n",
    "print(f'Performance for Glucose y BloodPressure: {rendiemiento1:.2f}')\n",
    "\n",
    "# Train KNN for the second combination of variables\n",
    "knn2 = KNN(k=12)\n",
    "knn2.fit(x2, y2)\n",
    "x2_test = test[['SkinThickness', 'BMI']].values\n",
    "y2_test = test['Outcome'].values\n",
    "predictions2 = knn2.predict(x2_test)\n",
    "rendiemiento2 = np.sum(predictions2 == y2_test) / len(y2_test)\n",
    "print(f'Performance for SkinThickness y BMI: {rendiemiento2:.2f}')\n",
    "\n",
    "# Train KNN for the third combination of variables\n",
    "knn3 = KNN(k=12)\n",
    "knn3.fit(x3, y3)\n",
    "x3_test = test[['Glucose', 'BMI']].values\n",
    "y3_test = test['Outcome'].values\n",
    "predictions3 = knn3.predict(x3_test)\n",
    "rendiemiento3 = np.sum(predictions3 == y3_test) / len(y3_test)\n",
    "print(f'Performance for Glucose y BMI: {rendiemiento3:.2f}')\n",
    "\n",
    "# Visualizes the predictions for the test set\n",
    "plt.scatter(\n",
    "    x1_test[:, 0],\n",
    "    x1_test[:, 1],\n",
    "    c=[colors[p] for p in predictions1]\n",
    ")\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('BloodPressure')\n",
    "plt.title('Glucose vs BloodPressure \"PREDICTED DATA\"')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(\n",
    "    x2_test[:, 0],\n",
    "    x2_test[:, 1],\n",
    "    c=[colors[p] for p in predictions2]\n",
    ")\n",
    "plt.xlabel('SkinThickness')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('SkinThickness vs BMI \"PREDICTED DATA\"')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(\n",
    "    x3_test[:, 0],\n",
    "    x3_test[:, 1],\n",
    "    c=[colors[p] for p in predictions3]\n",
    ")\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('BMI')\n",
    "plt.title('Glucose vs BMI \"PREDICTED DATA\"')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8b9d1",
   "metadata": {},
   "source": [
    "### **CONCLUSION**\n",
    "After comparing different examples of K values we can clearly conclude that there is a significant difference on the accurancy between the values of K neighbours showing that the K=√N is the best solution for our problem.\n",
    "Based on the variables compared the combination that best predicts diabetes on this dataset is Glucose vs BMI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e29d1",
   "metadata": {},
   "source": [
    "### **AUTHORS**\n",
    "\n",
    "Miguel Moreton, Pablo Gutiérrez; Doble Grado ADEINF 2025-2026"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entjuan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
