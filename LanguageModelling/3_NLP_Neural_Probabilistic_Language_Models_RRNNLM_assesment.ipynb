{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98469540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "import re\n",
    "import pdfplumber\n",
    "import unicodedata\n",
    "from IPython.display import Markdown, display\n",
    "%load_ext jupyter_tikz\n",
    "\n",
    "assesment_draw_and_fill = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f906a1a-3ac1-4364-93ff-044f8042f1ae",
   "metadata": {},
   "source": [
    "**Disclaimer:** This notebook pretends to be didactic, introducing the ideas behind the concepts used to train neural probabilistic language models. This means that, modern models are implemented using advanced algorithms based on these ideas. Also, the RNN is overfitted otherwise with few data we do not get any cool response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6681b3-ceb7-4907-841f-1ab81ad5e07e",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Language Model with Dense Input Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559d2f5-be4d-4d34-ad27-3aeb3c7b1e3d",
   "metadata": {},
   "source": [
    "So far so good, we have seen how to convert text to numerical representations in various ways. These representations could be used as features to train classifiers on tasks such as, for example, detect racism in a text. \n",
    "\n",
    "We will see that, in the process of training such models, we will introduce a new concept for embedding. The task we train our model on will bias this embedding towards being good at representing different aspects of interest (semantic similarity, clustering, discrimination between two classes) and so on.\n",
    "\n",
    "One of such tasks we can implement is Language generation, where the goal is to generate text based on the already generated one. In this assesment we will implement the Recurrent Nueral Network Language Model from Mikolov `https://www.fit.vut.cz/research/group/speech/public/publi/2010/mikolov_interspeech2010_IS100722.pdf` but instead of using a one hot encoding embedding representation of the words, we will use a dense embedding representation, such as the one we learn in the CBOW model.\n",
    "\n",
    "This means that the input to the model instead of being a one-hot encoding vector will be a dense vector obtained from an embedding matrix. There are plenty choices for this embedding matrix:\n",
    "\n",
    "* Use the one obtained by learning first the CBOW or Skip-Gram model.\n",
    "* Learn and embedding matrix jointly with the model parameters itself. Note this embedding matrix will represent input words in a way that they are good representations for the task at hand.\n",
    "* Start from the embedding matrix obtained by learning first the CBOW or Skip-Gram model, and then learn this embedding matrix jointly with the model parameters itself.\n",
    "\n",
    "We will also see how the postprocessing step of Language modelling can be used within this task. Rememeber in language modeling the steps we perform are:\n",
    "\n",
    "* normalization/preprocessing\n",
    "* tokenization (pre-tokenization and training)\n",
    "* modelling\n",
    "* post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7aaa3-5919-4160-aec4-bad4299ff6ef",
   "metadata": {},
   "source": [
    "## A statistical model for Language Generation\n",
    "\n",
    "Note that the objective of the CBOW model is just to learn good representations of words, based on the ideas that the words in the surroundings are relevant, and thus we try to find and learn this statistical relationships.\n",
    "\n",
    "However, for language generation this is not valid, because if we want to generate a word at time $t$, this word only depends on what was generated in the past, since the future has not been generated yet. Thus, the way we frame this problem is to learn the probability of a word, given its context:\n",
    "\n",
    "$$\n",
    "p(w_t \\mid w_{t-1},\\dots,w_1)\n",
    "$$\n",
    "\n",
    "There are plenty of things here we need to consider. First of all, the model from the CBOW had a single matrix $A$, which might not be enough for representing this task. We can obviously gain expressiveness by making the model non-linear, and having a different matrix $A$ per word in the context. Forgetting about the computational complexity of doing this, the biggest problem is that since sentences in a language have different lengths, we'd have to restrict the context to a fix length in order to use a feedforward model such as the one used by the CBOW model. In other words, we'd only be able to learn:\n",
    "\n",
    "$$\n",
    "p(w_t \\mid w_{t-1},\\dots,w_{t-N})\n",
    "$$\n",
    "\n",
    "where $N$ is the context.\n",
    "\n",
    "Since we do not want to restrict on this, we will use a recurrent neural network, which deals with arbitrary input sequence lenghts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7316b6f-4b74-4695-ac7b-f27bbf2ffcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"400.948pt\" height=\"239.295pt\" viewBox=\"0 0 400.948 239.295\" version=\"1.2\">\n",
       "<defs>\n",
       "<g>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-0\">\n",
       "<path style=\"stroke:none;\" d=\"\"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-1\">\n",
       "<path style=\"stroke:none;\" d=\"M 6.09375 -6.015625 C 6.09375 -6.375 6.109375 -6.484375 6.875 -6.484375 L 7.109375 -6.484375 L 7.109375 -6.78125 C 6.765625 -6.75 6.03125 -6.75 5.65625 -6.75 C 5.28125 -6.75 4.53125 -6.75 4.1875 -6.78125 L 4.1875 -6.484375 L 4.421875 -6.484375 C 5.1875 -6.484375 5.203125 -6.375 5.203125 -6.015625 L 5.203125 -3.6875 L 2.234375 -3.6875 L 2.234375 -6.015625 C 2.234375 -6.375 2.25 -6.484375 3.015625 -6.484375 L 3.265625 -6.484375 L 3.265625 -6.78125 C 2.90625 -6.75 2.171875 -6.75 1.796875 -6.75 C 1.421875 -6.75 0.671875 -6.75 0.328125 -6.78125 L 0.328125 -6.484375 L 0.5625 -6.484375 C 1.328125 -6.484375 1.34375 -6.375 1.34375 -6.015625 L 1.34375 -0.78125 C 1.34375 -0.421875 1.328125 -0.3125 0.5625 -0.3125 L 0.328125 -0.3125 L 0.328125 0 C 0.671875 -0.03125 1.40625 -0.03125 1.78125 -0.03125 C 2.171875 -0.03125 2.90625 -0.03125 3.265625 0 L 3.265625 -0.3125 L 3.015625 -0.3125 C 2.25 -0.3125 2.234375 -0.421875 2.234375 -0.78125 L 2.234375 -3.375 L 5.203125 -3.375 L 5.203125 -0.78125 C 5.203125 -0.421875 5.1875 -0.3125 4.421875 -0.3125 L 4.1875 -0.3125 L 4.1875 0 C 4.53125 -0.03125 5.265625 -0.03125 5.640625 -0.03125 C 6.015625 -0.03125 6.765625 -0.03125 7.109375 0 L 7.109375 -0.3125 L 6.875 -0.3125 C 6.109375 -0.3125 6.09375 -0.421875 6.09375 -0.78125 Z M 6.09375 -6.015625 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-2\">\n",
       "<path style=\"stroke:none;\" d=\"M 0.5625 -3.40625 C 0.5625 -1.34375 2.171875 0.21875 4.015625 0.21875 C 5.640625 0.21875 6.609375 -1.15625 6.609375 -2.3125 C 6.609375 -2.421875 6.609375 -2.484375 6.484375 -2.484375 C 6.375 -2.484375 6.375 -2.421875 6.359375 -2.328125 C 6.28125 -0.90625 5.21875 -0.09375 4.140625 -0.09375 C 3.53125 -0.09375 1.578125 -0.421875 1.578125 -3.390625 C 1.578125 -6.359375 3.515625 -6.703125 4.125 -6.703125 C 5.203125 -6.703125 6.09375 -5.796875 6.296875 -4.34375 C 6.3125 -4.203125 6.3125 -4.171875 6.453125 -4.171875 C 6.609375 -4.171875 6.609375 -4.203125 6.609375 -4.40625 L 6.609375 -6.765625 C 6.609375 -6.9375 6.609375 -7 6.5 -7 C 6.453125 -7 6.421875 -7 6.34375 -6.890625 L 5.84375 -6.15625 C 5.46875 -6.515625 4.96875 -7 4.015625 -7 C 2.15625 -7 0.5625 -5.421875 0.5625 -3.40625 Z M 0.5625 -3.40625 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-3\">\n",
       "<path style=\"stroke:none;\" d=\"M 4 -3.828125 L 5.359375 -5.828125 C 5.578125 -6.15625 5.90625 -6.46875 6.796875 -6.484375 L 6.796875 -6.78125 C 6.40625 -6.78125 5.9375 -6.75 5.6875 -6.75 C 5.296875 -6.75 4.8125 -6.75 4.421875 -6.78125 L 4.421875 -6.484375 C 4.8125 -6.46875 5.03125 -6.25 5.03125 -6.015625 C 5.03125 -5.921875 5.015625 -5.90625 4.953125 -5.796875 L 3.8125 -4.109375 L 2.53125 -6.03125 C 2.515625 -6.0625 2.46875 -6.140625 2.46875 -6.1875 C 2.46875 -6.296875 2.6875 -6.46875 3.109375 -6.484375 L 3.109375 -6.78125 C 2.765625 -6.75 2.03125 -6.75 1.65625 -6.75 C 1.34375 -6.75 0.734375 -6.765625 0.375 -6.78125 L 0.375 -6.484375 L 0.5625 -6.484375 C 1.109375 -6.484375 1.296875 -6.40625 1.484375 -6.125 L 3.3125 -3.375 L 1.671875 -0.96875 C 1.546875 -0.765625 1.25 -0.3125 0.234375 -0.3125 L 0.234375 0 C 0.59375 -0.015625 1.015625 -0.03125 1.34375 -0.03125 C 1.703125 -0.03125 2.25 -0.03125 2.609375 0 L 2.609375 -0.3125 C 2.15625 -0.3125 1.984375 -0.59375 1.984375 -0.765625 C 1.984375 -0.859375 2.015625 -0.890625 2.09375 -1 L 3.5 -3.09375 L 5.0625 -0.71875 C 5.09375 -0.671875 5.125 -0.640625 5.125 -0.609375 C 5.125 -0.484375 4.90625 -0.3125 4.46875 -0.3125 L 4.46875 0 C 4.8125 -0.03125 5.546875 -0.03125 5.921875 -0.03125 C 6.34375 -0.03125 6.796875 -0.015625 7.21875 0 L 7.21875 -0.3125 L 7.03125 -0.3125 C 6.515625 -0.3125 6.296875 -0.359375 6.09375 -0.671875 Z M 4 -3.828125 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-4\">\n",
       "<path style=\"stroke:none;\" d=\"M 6.0625 -5.828125 C 6.421875 -6.4375 7.015625 -6.484375 7.328125 -6.484375 L 7.328125 -6.78125 C 7.03125 -6.765625 6.625 -6.75 6.359375 -6.75 C 6.046875 -6.75 5.46875 -6.78125 5.21875 -6.78125 L 5.21875 -6.484375 C 5.625 -6.484375 5.78125 -6.296875 5.78125 -6.09375 C 5.78125 -5.984375 5.71875 -5.859375 5.6875 -5.796875 L 4.046875 -3.125 L 2.25 -6.0625 C 2.171875 -6.15625 2.171875 -6.1875 2.171875 -6.234375 C 2.171875 -6.4375 2.421875 -6.484375 2.875 -6.484375 L 2.875 -6.78125 C 2.515625 -6.75 1.78125 -6.75 1.40625 -6.75 C 0.984375 -6.75 0.53125 -6.765625 0.109375 -6.78125 L 0.109375 -6.484375 L 0.28125 -6.484375 C 0.96875 -6.484375 1.046875 -6.375 1.203125 -6.109375 L 3.296875 -2.71875 L 3.296875 -0.78125 C 3.296875 -0.421875 3.265625 -0.3125 2.5 -0.3125 L 2.265625 -0.3125 L 2.265625 0 C 2.625 -0.03125 3.328125 -0.03125 3.71875 -0.03125 C 4.109375 -0.03125 4.8125 -0.03125 5.171875 0 L 5.171875 -0.3125 L 4.9375 -0.3125 C 4.171875 -0.3125 4.140625 -0.40625 4.140625 -0.796875 L 4.140625 -2.71875 Z M 6.0625 -5.828125 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-5\">\n",
       "<path style=\"stroke:none;\" d=\"M 2.859375 -2.328125 C 3.15625 -2.71875 3.53125 -3.203125 3.765625 -3.453125 C 4.078125 -3.8125 4.484375 -3.96875 4.953125 -3.96875 L 4.953125 -4.28125 C 4.6875 -4.265625 4.390625 -4.25 4.140625 -4.25 C 3.828125 -4.25 3.3125 -4.265625 3.1875 -4.28125 L 3.1875 -3.96875 C 3.390625 -3.953125 3.46875 -3.828125 3.46875 -3.671875 C 3.46875 -3.515625 3.375 -3.375 3.3125 -3.3125 L 2.703125 -2.546875 L 1.921875 -3.546875 C 1.84375 -3.640625 1.84375 -3.671875 1.84375 -3.734375 C 1.84375 -3.875 1.984375 -3.96875 2.1875 -3.96875 L 2.1875 -4.28125 C 1.921875 -4.265625 1.265625 -4.25 1.109375 -4.25 C 0.90625 -4.25 0.4375 -4.265625 0.171875 -4.28125 L 0.171875 -3.96875 C 0.859375 -3.96875 0.875 -3.96875 1.34375 -3.375 L 2.328125 -2.09375 L 1.390625 -0.90625 C 0.921875 -0.328125 0.328125 -0.3125 0.125 -0.3125 L 0.125 0 C 0.375 -0.015625 0.6875 -0.03125 0.9375 -0.03125 C 1.234375 -0.03125 1.65625 -0.015625 1.890625 0 L 1.890625 -0.3125 C 1.671875 -0.34375 1.59375 -0.46875 1.59375 -0.609375 C 1.59375 -0.828125 1.890625 -1.15625 2.5 -1.875 L 3.25 -0.890625 C 3.328125 -0.78125 3.453125 -0.609375 3.453125 -0.5625 C 3.453125 -0.46875 3.375 -0.3125 3.09375 -0.3125 L 3.09375 0 C 3.40625 -0.015625 3.953125 -0.03125 4.171875 -0.03125 C 4.4375 -0.03125 4.828125 -0.015625 5.125 0 L 5.125 -0.3125 C 4.59375 -0.3125 4.40625 -0.328125 4.1875 -0.609375 Z M 2.859375 -2.328125 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-6\">\n",
       "<path style=\"stroke:none;\" d=\"M 4.0625 -2.28125 L 6.84375 -2.28125 C 6.96875 -2.28125 7.171875 -2.28125 7.171875 -2.484375 C 7.171875 -2.6875 6.96875 -2.6875 6.84375 -2.6875 L 4.0625 -2.6875 L 4.0625 -5.46875 C 4.0625 -5.609375 4.0625 -5.796875 3.859375 -5.796875 C 3.671875 -5.796875 3.671875 -5.609375 3.671875 -5.46875 L 3.671875 -2.6875 L 0.890625 -2.6875 C 0.75 -2.6875 0.5625 -2.6875 0.5625 -2.484375 C 0.5625 -2.28125 0.75 -2.28125 0.890625 -2.28125 L 3.671875 -2.28125 L 3.671875 0.5 C 3.671875 0.640625 3.671875 0.828125 3.859375 0.828125 C 4.0625 0.828125 4.0625 0.640625 4.0625 0.5 Z M 4.0625 -2.28125 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-7\">\n",
       "<path style=\"stroke:none;\" d=\"M 1.71875 -3.96875 L 3.140625 -3.96875 L 3.140625 -4.28125 L 1.71875 -4.28125 L 1.71875 -6.109375 L 1.46875 -6.109375 C 1.453125 -5.296875 1.15625 -4.234375 0.1875 -4.1875 L 0.1875 -3.96875 L 1.03125 -3.96875 L 1.03125 -1.234375 C 1.03125 -0.015625 1.953125 0.109375 2.3125 0.109375 C 3.015625 0.109375 3.296875 -0.59375 3.296875 -1.234375 L 3.296875 -1.796875 L 3.046875 -1.796875 L 3.046875 -1.25 C 3.046875 -0.515625 2.75 -0.140625 2.390625 -0.140625 C 1.71875 -0.140625 1.71875 -1.046875 1.71875 -1.21875 Z M 1.71875 -3.96875 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-8\">\n",
       "<path style=\"stroke:none;\" d=\"M 3.3125 -0.75 C 3.34375 -0.359375 3.625 0.0625 4.078125 0.0625 C 4.296875 0.0625 4.90625 -0.078125 4.90625 -0.890625 L 4.90625 -1.4375 L 4.65625 -1.4375 L 4.65625 -0.890625 C 4.65625 -0.3125 4.40625 -0.25 4.296875 -0.25 C 3.96875 -0.25 3.921875 -0.703125 3.921875 -0.75 L 3.921875 -2.734375 C 3.921875 -3.15625 3.921875 -3.53125 3.5625 -3.90625 C 3.1875 -4.296875 2.6875 -4.453125 2.203125 -4.453125 C 1.390625 -4.453125 0.703125 -3.984375 0.703125 -3.328125 C 0.703125 -3.03125 0.90625 -2.859375 1.15625 -2.859375 C 1.4375 -2.859375 1.625 -3.0625 1.625 -3.3125 C 1.625 -3.4375 1.5625 -3.765625 1.109375 -3.78125 C 1.375 -4.125 1.875 -4.234375 2.1875 -4.234375 C 2.671875 -4.234375 3.234375 -3.84375 3.234375 -2.96875 L 3.234375 -2.59375 C 2.734375 -2.5625 2.03125 -2.53125 1.40625 -2.234375 C 0.671875 -1.890625 0.421875 -1.375 0.421875 -0.9375 C 0.421875 -0.140625 1.375 0.109375 2 0.109375 C 2.65625 0.109375 3.125 -0.28125 3.3125 -0.75 Z M 3.234375 -2.390625 L 3.234375 -1.390625 C 3.234375 -0.453125 2.53125 -0.109375 2.078125 -0.109375 C 1.59375 -0.109375 1.1875 -0.453125 1.1875 -0.953125 C 1.1875 -1.5 1.59375 -2.328125 3.234375 -2.390625 Z M 3.234375 -2.390625 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-9\">\n",
       "<path style=\"stroke:none;\" d=\"M 1.09375 -3.421875 L 1.09375 -0.75 C 1.09375 -0.3125 0.984375 -0.3125 0.3125 -0.3125 L 0.3125 0 C 0.671875 -0.015625 1.171875 -0.03125 1.4375 -0.03125 C 1.703125 -0.03125 2.21875 -0.015625 2.546875 0 L 2.546875 -0.3125 C 1.890625 -0.3125 1.78125 -0.3125 1.78125 -0.75 L 1.78125 -2.578125 C 1.78125 -3.625 2.484375 -4.171875 3.125 -4.171875 C 3.75 -4.171875 3.859375 -3.640625 3.859375 -3.078125 L 3.859375 -0.75 C 3.859375 -0.3125 3.75 -0.3125 3.078125 -0.3125 L 3.078125 0 C 3.421875 -0.015625 3.9375 -0.03125 4.203125 -0.03125 C 4.46875 -0.03125 4.984375 -0.015625 5.3125 0 L 5.3125 -0.3125 C 4.796875 -0.3125 4.546875 -0.3125 4.546875 -0.609375 L 4.546875 -2.5 C 4.546875 -3.359375 4.546875 -3.671875 4.234375 -4.03125 C 4.09375 -4.1875 3.765625 -4.390625 3.1875 -4.390625 C 2.46875 -4.390625 2 -3.96875 1.71875 -3.34375 L 1.71875 -4.390625 L 0.3125 -4.28125 L 0.3125 -3.96875 C 1.015625 -3.96875 1.09375 -3.90625 1.09375 -3.421875 Z M 1.09375 -3.421875 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph0-10\">\n",
       "<path style=\"stroke:none;\" d=\"M 1.09375 -0.75 C 1.09375 -0.3125 0.984375 -0.3125 0.3125 -0.3125 L 0.3125 0 C 0.671875 -0.015625 1.171875 -0.03125 1.4375 -0.03125 C 1.703125 -0.03125 2.21875 -0.015625 2.546875 0 L 2.546875 -0.3125 C 1.890625 -0.3125 1.78125 -0.3125 1.78125 -0.75 L 1.78125 -2.578125 C 1.78125 -3.625 2.484375 -4.171875 3.125 -4.171875 C 3.75 -4.171875 3.859375 -3.640625 3.859375 -3.078125 L 3.859375 -0.75 C 3.859375 -0.3125 3.75 -0.3125 3.078125 -0.3125 L 3.078125 0 C 3.421875 -0.015625 3.9375 -0.03125 4.203125 -0.03125 C 4.46875 -0.03125 4.984375 -0.015625 5.3125 0 L 5.3125 -0.3125 C 4.796875 -0.3125 4.546875 -0.3125 4.546875 -0.609375 L 4.546875 -2.5 C 4.546875 -3.359375 4.546875 -3.671875 4.234375 -4.03125 C 4.09375 -4.1875 3.765625 -4.390625 3.1875 -4.390625 C 2.359375 -4.390625 1.921875 -3.796875 1.75 -3.421875 L 1.75 -6.890625 L 0.3125 -6.78125 L 0.3125 -6.484375 C 1.015625 -6.484375 1.09375 -6.40625 1.09375 -5.921875 Z M 1.09375 -0.75 \"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph1-0\">\n",
       "<path style=\"stroke:none;\" d=\"\"/>\n",
       "</symbol>\n",
       "<symbol overflow=\"visible\" id=\"glyph1-1\">\n",
       "<path style=\"stroke:none;\" d=\"M 5.140625 -3.703125 C 5.28125 -3.703125 5.640625 -3.703125 5.640625 -4.046875 C 5.640625 -4.28125 5.421875 -4.28125 5.25 -4.28125 L 2.984375 -4.28125 C 1.484375 -4.28125 0.375 -2.640625 0.375 -1.453125 C 0.375 -0.59375 0.96875 0.109375 1.875 0.109375 C 3.046875 0.109375 4.359375 -1.09375 4.359375 -2.625 C 4.359375 -2.796875 4.359375 -3.265625 4.046875 -3.703125 Z M 1.875 -0.109375 C 1.390625 -0.109375 1 -0.46875 1 -1.1875 C 1 -1.484375 1.109375 -2.296875 1.453125 -2.875 C 1.875 -3.5625 2.46875 -3.703125 2.8125 -3.703125 C 3.640625 -3.703125 3.734375 -3.046875 3.734375 -2.75 C 3.734375 -2.28125 3.53125 -1.453125 3.1875 -0.953125 C 2.796875 -0.375 2.265625 -0.109375 1.875 -0.109375 Z M 1.875 -0.109375 \"/>\n",
       "</symbol>\n",
       "</g>\n",
       "<clipPath id=\"clip1\">\n",
       "  <path d=\"M 158 215 L 182 215 L 182 238.59375 L 158 238.59375 Z M 158 215 \"/>\n",
       "</clipPath>\n",
       "<clipPath id=\"clip2\">\n",
       "  <path d=\"M 147 204 L 193 204 L 193 238.59375 L 147 238.59375 Z M 147 204 \"/>\n",
       "</clipPath>\n",
       "<clipPath id=\"clip3\">\n",
       "  <path d=\"M 366 125 L 400.332031 125 L 400.332031 171 L 366 171 Z M 366 125 \"/>\n",
       "</clipPath>\n",
       "<clipPath id=\"clip4\">\n",
       "  <path d=\"M 366 68 L 400.332031 68 L 400.332031 114 L 366 114 Z M 366 68 \"/>\n",
       "</clipPath>\n",
       "</defs>\n",
       "<g id=\"surface1\">\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(89.99939%,89.99939%,100%);fill-opacity:1;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -85.038669 -56.695224 L 85.03906 -56.695224 L 85.03906 56.69254 L -85.038669 56.69254 Z M -85.038669 -56.695224 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -142.531773 -28.346324 C -142.531773 -22.085746 -147.609188 -17.008331 -153.869766 -17.008331 C -160.130344 -17.008331 -165.207759 -22.085746 -165.207759 -28.346324 C -165.207759 -34.61082 -160.130344 -39.684317 -153.869766 -39.684317 C -147.609188 -39.684317 -142.531773 -34.61082 -142.531773 -28.346324 Z M -142.531773 -28.346324 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-1\" x=\"78.07361\" y=\"150.953268\"/>\n",
       "</g>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -142.531773 28.347558 C -142.531773 34.608136 -147.609188 39.685551 -153.869766 39.685551 C -160.130344 39.685551 -165.207759 34.608136 -165.207759 28.347558 C -165.207759 22.083062 -160.130344 17.009565 -153.869766 17.009565 C -147.609188 17.009565 -142.531773 22.083062 -142.531773 28.347558 Z M -142.531773 28.347558 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-2\" x=\"78.212202\" y=\"94.426804\"/>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip1)\" clip-rule=\"nonzero\">\n",
       "<path style=\" stroke:none;fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;\" d=\"M 181.320312 226.890625 C 181.320312 220.648438 176.257812 215.585938 170.015625 215.585938 C 163.769531 215.585938 158.710938 220.648438 158.710938 226.890625 C 158.710938 233.132812 163.769531 238.195312 170.015625 238.195312 C 176.257812 238.195312 181.320312 233.132812 181.320312 226.890625 Z M 181.320312 226.890625 \"/>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip2)\" clip-rule=\"nonzero\">\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -54.053117 -107.91208 C -54.053117 -101.651502 -59.130532 -96.574087 -65.39111 -96.574087 C -71.655606 -96.574087 -76.729103 -101.651502 -76.729103 -107.91208 C -76.729103 -114.172658 -71.655606 -119.250073 -65.39111 -119.250073 C -59.130532 -119.250073 -54.053117 -114.172658 -54.053117 -107.91208 Z M -54.053117 -107.91208 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "</g>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-3\" x=\"166.291706\" y=\"230.283549\"/>\n",
       "</g>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 76.729494 107.913314 C 76.729494 114.173891 71.652079 119.251306 65.391501 119.251306 C 59.130923 119.251306 54.053508 114.173891 54.053508 107.913314 C 54.053508 101.648818 59.130923 96.571403 65.391501 96.571403 C 71.652079 96.571403 76.729494 101.648818 76.729494 107.913314 Z M 76.729494 107.913314 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-4\" x=\"296.688537\" y=\"15.096523\"/>\n",
       "</g>\n",
       "<path style=\" stroke:none;fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;\" d=\"M 399.9375 147.558594 C 399.9375 141.316406 394.875 136.253906 388.632812 136.253906 C 382.386719 136.253906 377.328125 141.316406 377.328125 147.558594 C 377.328125 153.804688 382.386719 158.863281 388.632812 158.863281 C 394.875 158.863281 399.9375 153.804688 399.9375 147.558594 Z M 399.9375 147.558594 \"/>\n",
       "<g clip-path=\"url(#clip3)\" clip-rule=\"nonzero\">\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 165.20815 -28.346324 C 165.20815 -22.085746 160.130735 -17.008331 153.870157 -17.008331 C 147.605662 -17.008331 142.532165 -22.085746 142.532165 -28.346324 C 142.532165 -34.61082 147.605662 -39.684317 153.870157 -39.684317 C 160.130735 -39.684317 165.20815 -34.61082 165.20815 -28.346324 Z M 165.20815 -28.346324 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "</g>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-1\" x=\"384.905636\" y=\"150.953268\"/>\n",
       "</g>\n",
       "<path style=\" stroke:none;fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;\" d=\"M 399.9375 91.03125 C 399.9375 84.789062 394.875 79.726562 388.632812 79.726562 C 382.386719 79.726562 377.328125 84.789062 377.328125 91.03125 C 377.328125 97.277344 382.386719 102.335938 388.632812 102.335938 C 394.875 102.335938 399.9375 97.277344 399.9375 91.03125 Z M 399.9375 91.03125 \"/>\n",
       "<g clip-path=\"url(#clip4)\" clip-rule=\"nonzero\">\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 165.20815 28.347558 C 165.20815 34.608136 160.130735 39.685551 153.870157 39.685551 C 147.605662 39.685551 142.532165 34.608136 142.532165 28.347558 C 142.532165 22.083062 147.605662 17.009565 153.870157 17.009565 C 160.130735 17.009565 165.20815 22.083062 165.20815 28.347558 Z M 165.20815 28.347558 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "</g>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-2\" x=\"385.044228\" y=\"94.426804\"/>\n",
       "</g>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -40.576028 28.347558 C -40.576028 32.798131 -44.184284 36.406387 -48.634857 36.406387 C -53.085431 36.406387 -56.693686 32.798131 -56.693686 28.347558 C -56.693686 23.896984 -53.085431 20.288729 -48.634857 20.288729 C -44.184284 20.288729 -40.576028 23.896984 -40.576028 28.347558 Z M -40.576028 28.347558 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-5\" x=\"184.102234\" y=\"93.171502\"/>\n",
       "</g>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 19.592905 28.347558 C 19.592905 33.757982 15.208934 38.145872 9.798509 38.145872 C 4.388085 38.145872 0.000195637 33.757982 0.000195637 28.347558 C 0.000195637 22.937134 4.388085 18.549244 9.798509 18.549244 C 15.208934 18.549244 19.592905 22.937134 19.592905 28.347558 Z M 19.592905 28.347558 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-6\" x=\"241.12025\" y=\"93.516486\"/>\n",
       "</g>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,99.488831%,90.000916%);fill-opacity:1;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -55.722082 -12.698797 L -41.547632 -12.698797 L -41.547632 -1.768251 L -55.722082 -1.768251 Z M -55.722082 -12.698797 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph1-1\" x=\"183.707397\" y=\"128.646986\"/>\n",
       "</g>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,99.488831%,90.000916%);fill-opacity:1;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -26.577877 -12.698797 L -12.403428 -12.698797 L -12.403428 -1.768251 L -26.577877 -1.768251 Z M -26.577877 -12.698797 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph1-1\" x=\"212.764789\" y=\"128.646986\"/>\n",
       "</g>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,99.488831%,90.000916%);fill-opacity:1;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 2.566327 -14.015164 L 29.132647 -14.015164 L 29.132647 -0.455802 L 2.566327 -0.455802 Z M 2.566327 -14.015164 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph0-7\" x=\"241.084356\" y=\"129.958123\"/>\n",
       "  <use xlink:href=\"#glyph0-8\" x=\"244.94743\" y=\"129.958123\"/>\n",
       "  <use xlink:href=\"#glyph0-9\" x=\"249.914097\" y=\"129.958123\"/>\n",
       "  <use xlink:href=\"#glyph0-10\" x=\"255.433058\" y=\"129.958123\"/>\n",
       "</g>\n",
       "<path style=\"fill-rule:nonzero;fill:rgb(100%,99.488831%,90.000916%);fill-opacity:1;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 44.102402 -12.698797 L 58.276852 -12.698797 L 58.276852 -1.768251 L 44.102402 -1.768251 Z M 44.102402 -12.698797 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<g style=\"fill:rgb(0%,0%,0%);fill-opacity:1;\">\n",
       "  <use xlink:href=\"#glyph1-1\" x=\"283.237167\" y=\"128.646986\"/>\n",
       "</g>\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -142.132162 -28.346324 L -87.142505 -28.346324 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.6376;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.555255 2.07297 C -1.425969 1.293336 0.0000952047 0.129762 0.387953 0.00047586 C 0.0000952047 -0.12881 -1.425969 -1.296302 -1.555255 -2.072018 \" transform=\"matrix(0.997062,0,0,-0.997062,148.32803,147.559068)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -142.132162 28.347558 L -58.797523 28.347558 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.6376;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.553175 2.073252 C -1.423889 1.293618 -0.00174266 0.130044 0.390033 0.000757889 C -0.00174266 -0.128528 -1.423889 -1.29602 -1.553175 -2.071736 \" transform=\"matrix(0.997062,0,0,-0.997062,176.591581,91.032006)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -65.39111 -96.174476 L -65.39111 -58.795143 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.6376;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.554713 2.071954 C -1.425427 1.296238 0.000637456 0.128746 0.388496 -0.000539927 C 0.000637456 -0.129826 -1.425427 -1.2934 -1.554713 -2.073034 \" transform=\"matrix(0,-0.997062,-0.997062,0,170.015087,177.918604)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -40.176417 28.347558 L -2.103641 28.347558 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.6376;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.552943 2.073252 C -1.423657 1.293618 -0.00151063 0.130044 0.390265 0.000757889 C -0.00151063 -0.128528 -1.423657 -1.29602 -1.552943 -2.071736 \" transform=\"matrix(0.997062,0,0,-0.997062,233.118694,91.032006)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 19.992517 28.347558 L 140.428328 28.347558 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.6376;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.555982 2.073252 C -1.422778 1.293618 -0.000631723 0.130044 0.387226 0.000757889 C -0.000631723 -0.128528 -1.422778 -1.29602 -1.555982 -2.071736 \" transform=\"matrix(0.997062,0,0,-0.997062,375.231099,91.032006)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 85.438671 28.347558 L 140.428328 28.347558 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.6376;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.555982 2.073252 C -1.422778 1.293618 -0.000631723 0.130044 0.387226 0.000757889 C -0.000631723 -0.128528 -1.422778 -1.29602 -1.555982 -2.071736 \" transform=\"matrix(0.997062,0,0,-0.997062,375.231099,91.032006)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 85.438671 -28.346324 L 140.428328 -28.346324 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.6376;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.555982 2.07297 C -1.422778 1.293336 -0.000631723 0.129762 0.387226 0.00047586 C -0.000631723 -0.12881 -1.422778 -1.296302 -1.555982 -2.072018 \" transform=\"matrix(0.997062,0,0,-0.997062,375.231099,147.559068)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.79701;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M 65.391501 57.092151 L 65.391501 94.471484 \" transform=\"matrix(0.997062,0,0,-0.997062,235.214649,119.295537)\"/>\n",
       "<path style=\"fill:none;stroke-width:0.6376;stroke-linecap:round;stroke-linejoin:round;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;\" d=\"M -1.554816 2.072643 C -1.42553 1.296927 0.000534423 0.129435 0.388393 0.000148652 C 0.000534423 -0.129137 -1.42553 -1.296629 -1.554816 -2.072346 \" transform=\"matrix(0,-0.997062,-0.997062,0,300.414211,25.102095)\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%tikz\n",
    "\n",
    "\n",
    "\\usetikzlibrary{shapes.geometric, arrows, positioning}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{tikzpicture}[shorten >=1pt, node distance=2cm, auto, thick]\n",
    "\n",
    "% Estilos de nodos\n",
    "\\tikzstyle{block} = [rectangle, draw=black, fill=blue!10, minimum height=4cm, minimum width=6cm, text centered]\n",
    "\\tikzstyle{small_block} = [rectangle, draw=black, fill=yellow!10, minimum height=0.25cm, minimum width=0.5cm, text centered]\n",
    "\\tikzstyle{circle_node} = [circle, draw=black, fill=white, minimum size=0.8cm, text centered]\n",
    "\\tikzstyle{small_circle} = [circle, draw=black, fill=white, minimum size=0.4cm, text centered]\n",
    "\n",
    "% Nodo de la celda LSTM (rectángulo horizontal)\n",
    "\\node[block] (lstm) {};\n",
    "\n",
    "% Entradas (en círculos)\n",
    "\\node[circle_node, left=2cm of lstm, yshift=-1cm] (H_in) {H}; \n",
    "\\node[circle_node, left=2cm of lstm, yshift=1cm] (C_in) {C};    \n",
    "\\node[circle_node, below left=1.5cm and -1cm of lstm] (X_in) {X}; % X en su posición original\n",
    "\n",
    "% Salidas (en círculos)\n",
    "\\node[circle_node, above right=1.5cm and -1cm of lstm] (Y_out) {Y};  % Y en su posición original\n",
    "\\node[circle_node, right=2cm of lstm, yshift=-1cm] (H_out) {H};  \n",
    "\\node[circle_node, right=2cm of lstm, yshift=1cm] (C_out) {C};    \n",
    "\n",
    "% Pequeños círculos dentro del rectángulo (entre las dos C)\n",
    "\\node[small_circle, right=3cm of C_in] (C1) {x};\n",
    "\\node[small_circle, right=5cm of C_in] (C2) {+};\n",
    "\n",
    "% Cuadrados no linearidades\n",
    "\\node[small_block, below=0.75cm of C1] (NL1) {$\\sigma$};\n",
    "\\node[small_block, right=0.5cm of NL1] (NL2) {$\\sigma$};\n",
    "\\node[small_block, right=0.5cm of NL2] (NL3) {$\\tanh$};\n",
    "\\node[small_block, right=0.5cm of NL3] (NL4) {$\\sigma$};\n",
    "\n",
    "% Conexiones de entrada a la celda LSTM\n",
    "\\draw[->] (H_in.east) -- (lstm.west|-H_in);\n",
    "\\draw[->] (C_in.east) -- (C1.west); % Flecha de C_in al primer círculo\n",
    "\\draw[->] (X_in.north) -- (X_in.north|-lstm.south);  % Flecha perpendicular de X\n",
    "\n",
    "% Conexiones entre los pequeños círculos y salida C\n",
    "\\draw[->] (C1.east) -- (C2.west);\n",
    "\\draw[->] (C2.east) -- (C_out.west); % Flecha del segundo círculo a la salida C\n",
    "\n",
    "% Conexiones de salida de la celda LSTM\n",
    "\\draw[->] (lstm.east|-C_out) -- (C_out.west);\n",
    "\\draw[->] (lstm.east|-H_out) -- (H_out.west);\n",
    "\\draw[->] (Y_out.south|-lstm.north) -- (Y_out.south);  % Flecha perpendicular de Y\n",
    "\n",
    "\\end{tikzpicture}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88d996-a348-47d4-9136-ad0afaa7d4d3",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "As you might guess, although we are dealing with a completely different task, in the sense that our goal is language generation and not really language representation, we still need to convert our words to vectors. Thus, we need to obtain our vocabulary from the training dataset and for that we need a tokenizer. We will reuse the tokenizer in the previous step.\n",
    "\n",
    "However, for convenience (see later), we will add three new tokens to our dictionary which are:\n",
    "\n",
    "* `<bos>`: begin of sequence\n",
    "* `<eos>`: end of sequence\n",
    "* `<pad>`: padding \n",
    "\n",
    "We will use the same tokenizer from previous assesments. So copy paste it in a new code cell. You can grab it from here. Note that I have performed slights modifications to add these new tokens:\n",
    "\n",
    "```python\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        # I do not remember exactly why I added <bos>|<eos>|<pad> to this regular expression since for vocabulary generation\n",
    "        # is not necessary. Probably it was done while I was creating this assesment and in the final solution it actually\n",
    "        # make no difference. However just in case I prefer not to remove it yet from here.\n",
    "        self.re_get_tokens = re.compile(r'\\w+|<bos>|<eos>|<pad>|[^\\w\\s]+')\n",
    "        self.re_replace_string_splitters = re.compile(r'\\s+')\n",
    "        self.split_by_underscore = re.compile(r'_+|[^_]+')\n",
    "        self.word_to_ids = None\n",
    "    \n",
    "    def encode(self, data):\n",
    "        assert self.word_to_ids is not None, \"Tokenizer has not been train. Call self.train_tokenizer\"\n",
    "\n",
    "        proc_data = self._normalize(data)\n",
    "        proc_data = self._pre_tokenize(proc_data)\n",
    "        \n",
    "        tokens = []\n",
    "        for token_text in proc_data:\n",
    "            tokens.append(self.word_to_ids[token_text])\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def batch_encode(self, data):\n",
    "        tokens = []\n",
    "        for d in data:\n",
    "            tokens.append(self.encode(d))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        assert self.word_to_ids is not None, \"Tokenizer has not been train. Call self.train_tokenizer\"\n",
    "        \n",
    "        text = [self.ids_to_word[t] for t in tokens]\n",
    "        text = ' '.join(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def batch_decode(self, tokens):\n",
    "        text = []\n",
    "        for t in tokens:\n",
    "            text.append(self.decode(t))\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def train_tokenizer(self, raw_data):\n",
    "        print(\"Normalizing data...\")\n",
    "        proc_data = self._normalize(raw_data)\n",
    "        \n",
    "        print(\"Pre tokenizing data...\")\n",
    "        proc_data = self._pre_tokenize(proc_data)\n",
    "        \n",
    "        print(\"Building vocabulary...\")\n",
    "        vocab = set(proc_data)\n",
    "        ## add special tokens to vocabulary to specify begind and end of sequences plus padding\n",
    "        vocab.update(['<bos>','<eos>','<pad>'])\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # create mappings between words and ids and ids vs word for encoding and decoding\n",
    "        print(\"Building ids to word mappings...\")\n",
    "        self.word_to_ids = {}\n",
    "        self.ids_to_word = {}\n",
    "        for i, word in enumerate(self.vocab):\n",
    "            self.word_to_ids[word] = i\n",
    "            self.ids_to_word[i] = word\n",
    "\n",
    "    @property\n",
    "    def pad_id(self):\n",
    "        return self.word_to_ids['<pad>']\n",
    "\n",
    "    @property\n",
    "    def bos_id(self):\n",
    "        return self.word_to_ids['<bos>']\n",
    "\n",
    "    @property\n",
    "    def eos_id(self):\n",
    "        return self.word_to_ids['<eos>']\n",
    "        \n",
    "    def _normalize(self, raw_data):\n",
    "\n",
    "        ## make lower\n",
    "        proc_data = raw_data.lower()\n",
    "        \n",
    "        ## remove accents\n",
    "        proc_data = unicodedata.normalize(\"NFD\", proc_data)\n",
    "        proc_data = re.sub(r'[\\u0301\\u0300\\u0302]', '', proc_data)\n",
    "        proc_data = unicodedata.normalize(\"NFC\", proc_data)\n",
    "        \n",
    "        ## remove more than one separator into blank space\n",
    "        proc_data = self.re_replace_string_splitters.sub(' ', proc_data)\n",
    "\n",
    "        return proc_data\n",
    "\n",
    "    def _pre_tokenize(self, proc_data):\n",
    "\n",
    "        tokens = []\n",
    "        for t in self.split_by_underscore.findall(proc_data):\n",
    "            tokens.extend(self.re_get_tokens.findall(t))\n",
    "    \n",
    "        return tokens\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe66b06-65fc-4fc5-b851-024da37dec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        # I do not remember exactly why I added <bos>|<eos>|<pad> to this regular expression since for vocabulary generation\n",
    "        # is not necessary. Probably it was done while I was creating this assesment and in the final solution it actually\n",
    "        # make no difference. However just in case I prefer not to remove it yet from here.\n",
    "        self.re_get_tokens = re.compile(r'\\w+|<bos>|<eos>|<pad>|[^\\w\\s]+')\n",
    "        self.re_replace_string_splitters = re.compile(r'\\s+')\n",
    "        self.split_by_underscore = re.compile(r'_+|[^_]+')\n",
    "        self.word_to_ids = None\n",
    "    \n",
    "    def encode(self, data):\n",
    "        assert self.word_to_ids is not None, \"Tokenizer has not been train. Call self.train_tokenizer\"\n",
    "\n",
    "        proc_data = self._normalize(data)\n",
    "        proc_data = self._pre_tokenize(proc_data)\n",
    "        \n",
    "        tokens = []\n",
    "        for token_text in proc_data:\n",
    "            tokens.append(self.word_to_ids[token_text])\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def batch_encode(self, data):\n",
    "        tokens = []\n",
    "        for d in data:\n",
    "            tokens.append(self.encode(d))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        assert self.word_to_ids is not None, \"Tokenizer has not been train. Call self.train_tokenizer\"\n",
    "        \n",
    "        text = [self.ids_to_word[t] for t in tokens]\n",
    "        text = ' '.join(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def batch_decode(self, tokens):\n",
    "        text = []\n",
    "        for t in tokens:\n",
    "            text.append(self.decode(t))\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def train_tokenizer(self, raw_data):\n",
    "        print(\"Normalizing data...\")\n",
    "        proc_data = self._normalize(raw_data)\n",
    "        \n",
    "        print(\"Pre tokenizing data...\")\n",
    "        proc_data = self._pre_tokenize(proc_data)\n",
    "        \n",
    "        print(\"Building vocabulary...\")\n",
    "        vocab = set(proc_data)\n",
    "        ## add special tokens to vocabulary to specify begind and end of sequences plus padding\n",
    "        vocab.update(['<bos>','<eos>','<pad>'])\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # create mappings between words and ids and ids vs word for encoding and decoding\n",
    "        print(\"Building ids to word mappings...\")\n",
    "        self.word_to_ids = {}\n",
    "        self.ids_to_word = {}\n",
    "        for i, word in enumerate(self.vocab):\n",
    "            self.word_to_ids[word] = i\n",
    "            self.ids_to_word[i] = word\n",
    "\n",
    "    @property\n",
    "    def pad_id(self):\n",
    "        return self.word_to_ids['<pad>']\n",
    "\n",
    "    @property\n",
    "    def bos_id(self):\n",
    "        return self.word_to_ids['<bos>']\n",
    "\n",
    "    @property\n",
    "    def eos_id(self):\n",
    "        return self.word_to_ids['<eos>']\n",
    "        \n",
    "    def _normalize(self, raw_data):\n",
    "\n",
    "        ## make lower\n",
    "        proc_data = raw_data.lower()\n",
    "        \n",
    "        ## remove accents\n",
    "        proc_data = unicodedata.normalize(\"NFD\", proc_data)\n",
    "        proc_data = re.sub(r'[\\u0301\\u0300\\u0302]', '', proc_data)\n",
    "        proc_data = unicodedata.normalize(\"NFC\", proc_data)\n",
    "        \n",
    "        ## remove more than one separator into blank space\n",
    "        proc_data = self.re_replace_string_splitters.sub(' ', proc_data)\n",
    "\n",
    "        return proc_data\n",
    "\n",
    "    def _pre_tokenize(self, proc_data):\n",
    "\n",
    "        tokens = []\n",
    "        for t in self.split_by_underscore.findall(proc_data):\n",
    "            tokens.extend(self.re_get_tokens.findall(t))\n",
    "    \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557e845-1e41-4e98-8cfb-2d112fa125a2",
   "metadata": {},
   "source": [
    "## Torch Dataset\n",
    "\n",
    "Based on the idea of next token prediction we are going to train a language model capable of generating text. Differently to state of the art models, by the fact that recurrent neural networks do not impose limits on the context (different to transformers) and since the purpose of this file is to be didactic and I want to show case different aspects, we are going to use as training samples the sentences in our dataset. So each sentence, i.e. each string separated by a dot will be a training sample.\n",
    "\n",
    "Note that paragraphs not ending in a \".\" will be joined together as a sentence. However I have realized that this happens only for titles and subtitles and since they are small I do not want to use it to train the model otherwise generating short sequence will have some probability.\n",
    "\n",
    "The dataset should now create training samples where the input to the model, usually denoted with $X$, is made up from words and the objective, the target $t$, is the next word in the sentences, to be feed into batches. Note that for a given sentence of length $N$, we can create $N-1$ training samples.  Alongside with $X$ and $T$ our dataset should also return the length of $X$ on iteration (we will see later why). This means that the `__get_item__` method from the dataset should return `X`,`T` and `X_len`\n",
    "\n",
    "##### Task: implement a torch dataset for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991cfa5e-b51e-41d6-9f2c-33c4021bf6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "class RRNNLMDataset(torch.utils.data.Dataset):\n",
       "    def __init__(self, text, tokenizer):\n",
       "\n",
       "        self.tokenizer = ...\n",
       "\n",
       "        self._build_dataset(...)\n",
       "        \n",
       "    def _build_dataset(self, text):\n",
       "        print(\"Generating sentences from input text\")\n",
       "        sentences = text.split(...)\n",
       "\n",
       "        # probably, there is a way of doing this using regex. Very simply, find existance of \".\" \n",
       "        #  (be careful with numbers) and replace with  <eos> _<eos> <bos>. Then training is generated\n",
       "        # by splitting over _<eos>, for example. I do it here differently.\n",
       "        print(\"Adding eos and bos tokens\")\n",
       "        sentences_proc = []\n",
       "        for s in ...:\n",
       "            # if sentences contains text. Additionally remove any right hand and left hand separator garbage\n",
       "            if s.strip():\n",
       "                s = ... + s.strip() + ...\n",
       "            sentences_proc.append(s)\n",
       "        \n",
       "        print(\"Tokenizing sentences from input text. Please wait...\")\n",
       "        tokenized_sentences = self.tokenizer.batch_encode(...)\n",
       "\n",
       "        ## note these training pairs could be created on the fly on dataloader call through multithreading.\n",
       "        #  In fact, for huge datasets it is inneficient to keep in memory all these combinations. It is better\n",
       "        #  to define a distribution that randomly selects how this sentences are created. This distribution\n",
       "        #  can be defined in order to create sentences that might be more interesting than others. Same with padding on the batch\n",
       "        #  I will pad here and safe in memory, but these is obviously inneficient.\n",
       "\n",
       "        ## Since padding \n",
       "        print(\"Building pairs of training inputs and labels...\")\n",
       "        training_inputs = []\n",
       "        training_inputs_lenghts = []\n",
       "        training_targets = []\n",
       "        max_length = -1\n",
       "\n",
       "        for s in ...:\n",
       "\n",
       "            ## first two tokens to predict the third. Not sure if from <eos> we should directly predict nothing.\n",
       "            #  In other words the first training sample is <eos> word to predict next_word.\n",
       "            end_w = ...\n",
       "\n",
       "            for i in range(len(s)-end_w):\n",
       "                x = torch.tensor(s[0:...])\n",
       "                t = s[...]\n",
       "\n",
       "                training_inputs.append(...)\n",
       "                training_inputs_lenghts.append(...)\n",
       "                training_targets.append(...)\n",
       "                \n",
       "                end_w += 1\n",
       "\n",
       "        X = training_inputs\n",
       "        X_len = training_inputs_lenghts\n",
       "        T = torch.tensor(training_targets)\n",
       "        \n",
       "        ## \n",
       "        self.X = X\n",
       "        self.X_len = training_inputs_lenghts\n",
       "        self.T = T\n",
       "\n",
       "    def __len__(self):\n",
       "        return len(self.T)\n",
       "\n",
       "    @property\n",
       "    def vocab_size(self):\n",
       "        return self.tokenizer.vocab_size\n",
       "    \n",
       "    def __getitem__(self, idx):\n",
       "        return self.X[idx], self.X_len[idx], self.T[idx]\n",
       "\n",
       "\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "class RRNNLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, tokenizer):\n",
    "\n",
    "        self.tokenizer = ...\n",
    "\n",
    "        self._build_dataset(...)\n",
    "        \n",
    "    def _build_dataset(self, text):\n",
    "        print(\"Generating sentences from input text\")\n",
    "        sentences = text.split(...)\n",
    "\n",
    "        # probably, there is a way of doing this using regex. Very simply, find existance of \".\" \n",
    "        #  (be careful with numbers) and replace with  <eos> _<eos> <bos>. Then training is generated\n",
    "        # by splitting over _<eos>, for example. I do it here differently.\n",
    "        print(\"Adding eos and bos tokens\")\n",
    "        sentences_proc = []\n",
    "        for s in ...:\n",
    "            # if sentences contains text. Additionally remove any right hand and left hand separator garbage\n",
    "            if s.strip():\n",
    "                s = ... + s.strip() + ...\n",
    "            sentences_proc.append(s)\n",
    "        \n",
    "        print(\"Tokenizing sentences from input text. Please wait...\")\n",
    "        tokenized_sentences = self.tokenizer.batch_encode(...)\n",
    "\n",
    "        ## note these training pairs could be created on the fly on dataloader call through multithreading.\n",
    "        #  In fact, for huge datasets it is inneficient to keep in memory all these combinations. It is better\n",
    "        #  to define a distribution that randomly selects how this sentences are created. This distribution\n",
    "        #  can be defined in order to create sentences that might be more interesting than others. Same with padding on the batch\n",
    "        #  I will pad here and safe in memory, but these is obviously inneficient.\n",
    "\n",
    "        ## Since padding \n",
    "        print(\"Building pairs of training inputs and labels...\")\n",
    "        training_inputs = []\n",
    "        training_inputs_lenghts = []\n",
    "        training_targets = []\n",
    "        max_length = -1\n",
    "\n",
    "        for s in ...:\n",
    "\n",
    "            ## first two tokens to predict the third. Not sure if from <eos> we should directly predict nothing.\n",
    "            #  In other words the first training sample is <eos> word to predict next_word.\n",
    "            end_w = ...\n",
    "\n",
    "            for i in range(len(s)-end_w):\n",
    "                x = torch.tensor(s[0:...])\n",
    "                t = s[...]\n",
    "\n",
    "                training_inputs.append(...)\n",
    "                training_inputs_lenghts.append(...)\n",
    "                training_targets.append(...)\n",
    "                \n",
    "                end_w += 1\n",
    "\n",
    "        X = training_inputs\n",
    "        X_len = training_inputs_lenghts\n",
    "        T = torch.tensor(training_targets)\n",
    "        \n",
    "        ## \n",
    "        self.X = X\n",
    "        self.X_len = training_inputs_lenghts\n",
    "        self.T = T\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.T)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.tokenizer.vocab_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.X_len[idx], self.T[idx]\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81f435b-2320-407a-906e-3c2aeb0b0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRNNLMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, tokenizer):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self._build_dataset(text)\n",
    "        \n",
    "    def _build_dataset(self, text):\n",
    "        print(\"Generating sentences from input text\")\n",
    "        sentences = text.split('.')\n",
    "\n",
    "        # probably, there is a way of doing this using regex. Very simply, find existance of \".\" \n",
    "        #  (be careful with numbers) and replace with  <eos> _<eos> <bos>. Then training is generated\n",
    "        # by splitting over _<eos>, for example. I do it here differently.\n",
    "        print(\"Adding eos and bos tokens\")\n",
    "        sentences_proc = []\n",
    "        for s in sentences:\n",
    "            # if sentences contains text. Additionally remove any right hand and left hand separator garbage\n",
    "            if s.strip():\n",
    "                s = \"<bos> \" + s.strip() + \" <eos>\" \n",
    "            sentences_proc.append(s)\n",
    "        \n",
    "        print(\"Tokenizing sentences from input text. Please wait...\")\n",
    "        tokenized_sentences = self.tokenizer.batch_encode(sentences_proc)\n",
    "\n",
    "        ## note these training pairs could be created on the fly on dataloader call through multithreading.\n",
    "        #  In fact, for huge datasets it is inneficient to keep in memory all these combinations. It is better\n",
    "        #  to define a distribution that randomly selects how this sentences are created. This distribution\n",
    "        #  can be defined in order to create sentences that might be more interesting than others. Same with padding on the batch\n",
    "        #  I will pad here and safe in memory, but these is obviously inneficient.\n",
    "\n",
    "        ## Since padding \n",
    "        print(\"Building pairs of training inputs and labels...\")\n",
    "        training_inputs = []\n",
    "        training_inputs_lenghts = []\n",
    "        training_targets = []\n",
    "        max_length = -1\n",
    "\n",
    "        for s in tokenized_sentences:\n",
    "\n",
    "            ## First two tokens to predict the third. Not sure if from <eos> we should directly predict nothing.\n",
    "            #  In other words the first training sample is <eos> word to predict next_word.\n",
    "            end_w = 2\n",
    "\n",
    "            for i in range(len(s)-end_w):\n",
    "                x = torch.tensor(s[0:end_w])\n",
    "                t = s[end_w]\n",
    "\n",
    "                training_inputs.append(x)\n",
    "                training_inputs_lenghts.append(len(x))\n",
    "                training_targets.append(t)\n",
    "                \n",
    "                end_w += 1\n",
    "\n",
    "        X = training_inputs\n",
    "        X_len = training_inputs_lenghts\n",
    "        T = torch.tensor(training_targets)\n",
    "        \n",
    "        ## \n",
    "        self.X = X\n",
    "        self.X_len = training_inputs_lenghts\n",
    "        self.T = T\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.T)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self.tokenizer.vocab_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.X_len[idx], self.T[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f603f-57a2-4a56-b802-d426646359e4",
   "metadata": {},
   "source": [
    "##### Task: Now create a dataloader with this dataset. For this you need to perform following steps:\n",
    "\n",
    "* Read data from source using pdfplumber\n",
    "* Instance and train your tokenizer\n",
    "* Create the dataset\n",
    "* Create a dataloader. You will need to create your own collate_function so that up on iteration the $X$ dataset is returned as a list of tensors.\n",
    "\n",
    "Why you need to use this collate_function? Well note that since input sentences $X$ have a different size on iteration the dataloader is not able to create a valid torch tensor, since torch tensors of type matrix can only be created from vectors of same size. In other words, torch is not able to create matrix of rows of different size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf243c28-1f8c-4b22-b1e3-6730f7dacfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "## read data from source to train language model on \n",
       "raw_data = \"\"\n",
       "\n",
       "## reading dataset with PDF Plumber\n",
       "with pdfplumber.open('../data/texto_pdf.pdf') as pdf:\n",
       "    ...\n",
       "\n",
       "## define tokenizer implementing\n",
       "tokenizer = Tokenizer()\n",
       "tokenizer.train_tokenizer(...)\n",
       "\n",
       "text_dataset = RRNNLMDataset(..., ...)\n",
       "\n",
       "def collate_fn(batch):\n",
       "    x, x_len, t = zip(*batch)  \n",
       "    return list(x), torch.tensor(x_len), torch.tensor(t)\n",
       "\n",
       "ntp_loader = torch.utils.data.DataLoader(\n",
       "                                            dataset = ...,\n",
       "                                            batch_size = 100, \n",
       "                                            shuffle=True,\n",
       "                                            num_workers=0,\n",
       "                                            collate_fn=collate_fn\n",
       "                                        )\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "## read data from source to train language model on \n",
    "raw_data = \"\"\n",
    "\n",
    "## reading dataset with PDF Plumber\n",
    "with pdfplumber.open('../data/texto_pdf.pdf') as pdf:\n",
    "    ...\n",
    "\n",
    "## define tokenizer implementing\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.train_tokenizer(...)\n",
    "\n",
    "text_dataset = RRNNLMDataset(..., ...)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x, x_len, t = zip(*batch)  \n",
    "    return list(x), torch.tensor(x_len), torch.tensor(t)\n",
    "\n",
    "ntp_loader = torch.utils.data.DataLoader(\n",
    "                                            dataset = ...,\n",
    "                                            batch_size = 100, \n",
    "                                            shuffle=True,\n",
    "                                            num_workers=0,\n",
    "                                            collate_fn=collate_fn\n",
    "                                        )\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308275ed-f03b-49a9-beab-d33f8c6d7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing data...\n",
      "Pre tokenizing data...\n",
      "Building vocabulary...\n",
      "Building ids to word mappings...\n",
      "Generating sentences from input text\n",
      "Adding eos and bos tokens\n",
      "Tokenizing sentences from input text. Please wait...\n",
      "Building pairs of training inputs and labels...\n"
     ]
    }
   ],
   "source": [
    "## read data from source to train language model on \n",
    "raw_data = \"\"\n",
    "\n",
    "## reading dataset with PDF Plumber\n",
    "with pdfplumber.open('../data/texto_pdf.pdf') as pdf:\n",
    "    for page in pdf.pages:\n",
    "        raw_data += page.extract_text()\n",
    "\n",
    "## define tokenizer implementing\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.train_tokenizer(raw_data)\n",
    "\n",
    "text_dataset = RRNNLMDataset(raw_data, tokenizer)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x, x_len, t = zip(*batch)  \n",
    "    return list(x), torch.tensor(x_len), torch.tensor(t)\n",
    "\n",
    "ntp_loader = torch.utils.data.DataLoader(\n",
    "                                            dataset = text_dataset,\n",
    "                                            batch_size = 128, \n",
    "                                            shuffle=True,\n",
    "                                            num_workers=0,\n",
    "                                            collate_fn=collate_fn\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f910f-5c12-4779-950b-1b4f9c0b7b77",
   "metadata": {},
   "source": [
    "## Torch Module\n",
    "\n",
    "Now, we need to implement a torch nn.Module that implements our recurrent Neural Network. Alongside with the basic operations for model learning: `forward`, `init` etc, we will create methods for text generation.\n",
    "\n",
    "Our recurrent neural network model will map vectors representing our input phrases, to a probability distribution over the size of the vocabulary. This is done via a LSTM cell, which maps a sequence of size $N$ to an embedding sequence of size $N$, and then maps the last element in the embedding sequence to a probability distribution, via a linear projection plus a softmax activation.\n",
    "\n",
    "Note that each of the $N$ elements in our input, represents each of the words that belong do the context. The embedding representation of size $N$ represents the hidden states of the recurrent neural network for each of the input elements, where dependency is encoded via the structure of the LSTM cell. Thus, the last element $N$ encodes information from all the input sequence. Based on this signal, we want to model the probability distribution over the next token, so that based on the context we are predicting the next token.\n",
    "\n",
    "Note that the input sequence $X$ is first a text that needs to be mappen to an embedding through the token ID from the tokenizer. So the model needs to create a learnable embedding matrix that will be learnt alongside with the parameters. Note that another option could be to learn first the CBOW model, and use the learnt embedding matrix in this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd273d1-8d85-46e0-ab8a-0ffed8bcff6b",
   "metadata": {},
   "source": [
    "##### Task: Implement the torch nn module using the following structure.\n",
    "\n",
    "\n",
    "Note that since sequences have different side, torch provide us methods to pad sequences so that all elements of the batch size have same length, so that we can parallelize computations. These methods are \n",
    "\n",
    "```python\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, d_embedding, tokenizer):\n",
    "        super().__init__()\n",
    "        ...\n",
    "\n",
    "    def forward(self, X : list[torch.tensor], X_len : list[int]):\n",
    "       ...\n",
    "\n",
    "    def generate_token(self, logit : torch.tensor, decoding : str, T : float):\n",
    "        ...\n",
    "    \n",
    "    def compute_loss(self,y,t):\n",
    "        ...\n",
    "\n",
    "    def generate_sequence(self, x = None, time_steps = None, decoding = 'sampling', T = None):\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0677bf7f-8626-4750-9b2e-7740764483e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "class RNNLM(nn.Module):\n",
       "    def __init__(self, d_embedding, tokenizer):\n",
       "        super().__init__()\n",
       "        self.tokenizer = ...\n",
       "        self.hidden_size = ...\n",
       "        self.input_embedding = nn.Embedding(\n",
       "            num_embeddings = ...,\n",
       "            embedding_dim = ...,\n",
       "            padding_idx = tokenizer.pad_id\n",
       "        )\n",
       "         \n",
       "        self.lstm_layer = nn.LSTM(\n",
       "                                    ..., \n",
       "                                    ..., \n",
       "                                    num_layers=1, \n",
       "                                    bias=True, \n",
       "                                    batch_first=True, \n",
       "                                    dropout=0.1\n",
       "                                 )\n",
       "\n",
       "        # Mapps hidden state over probability distribution over the vocabulary.\n",
       "        self.fc_output = ...\n",
       "\n",
       "        self.loss = ...\n",
       "\n",
       "    def forward(self, X, X_len):\n",
       "        ## Pad sequence and obtain embedding\n",
       "        X = pad_sequence(X, batch_first = True, padding_value = tokenizer.pad_id)\n",
       "        X = ...\n",
       "\n",
       "        ## Pack it up, for efficient computation\n",
       "        X = pack_padded_sequence(X, X_len , batch_first = True, enforce_sorted=False) \n",
       "\n",
       "        ## Use just last hidden state to project to softmax.\n",
       "        _, (last_H, _) = self.lstm_layer(...)\n",
       "\n",
       "        ## classification layer for next token\n",
       "        logit = ...(last_H.squeeze(dim = 0))\n",
       "        return logit\n",
       "\n",
       "    def generate_token(self, logit, decoding, T):\n",
       "        if decoding == 'argmax':\n",
       "            # argmax decoding for the moment\n",
       "            index = torch.argmax(logit)\n",
       "        elif decoding == 'sampling':\n",
       "            p = torch.softmax(logit / T, dim = 1) \n",
       "            index = torch.multinomial(p, num_samples=1)\n",
       "            \n",
       "        return index.item()\n",
       "    \n",
       "    def compute_loss(self,y,t):\n",
       "        return ...\n",
       "\n",
       "    def generate_sequence(self, x = None, time_steps = None, decoding = 'sampling', T = None):\n",
       "        if decoding not in ['sampling', 'argmax']:\n",
       "            raise ValueError()\n",
       "\n",
       "        if decoding == 'sampling' and T == None:\n",
       "            T = 1\n",
       "\n",
       "        # if x is None starts with begging of squence.\n",
       "        if x is None:\n",
       "            x = [[...]]\n",
       "            x_len = torch.tensor([1])\n",
       "        else:\n",
       "            assert isinstance(x,list)\n",
       "            x = ...\n",
       "            x_len = torch.tensor([len(x[0])])\n",
       "\n",
       "            \n",
       "        # if None runs until end of sequence.\n",
       "        if time_steps == None:\n",
       "            while True:\n",
       "                \n",
       "                ## generate logit from P(next_token | inputs)\n",
       "                logit = ...\n",
       "                ## generate token according to a decoding scheme\n",
       "                next_token = ...\n",
       "\n",
       "                ## append new token and start again till time steps\n",
       "                x[0].append(next_token)\n",
       "                ## increase generation length\n",
       "                x_len = x_len + 1\n",
       "                \n",
       "                if self.tokenizer.eos_id == next_token:\n",
       "                    break\n",
       "        else:\n",
       "            for ts in range(time_steps):\n",
       "                ## generate logit from P(next_token | inputs)\n",
       "                logit = ...\n",
       "                ## generate token according to a decoding scheme\n",
       "                next_token = ...\n",
       "\n",
       "                ## if it generates bos token continue\n",
       "                if self.tokenizer.bos_id == next_token:\n",
       "                    continue\n",
       "                \n",
       "                ## append new token and start again till time steps\n",
       "                x[0].append(next_token)\n",
       "                ## increase generation length\n",
       "                x_len = x_len + 1\n",
       "\n",
       "                if self.tokenizer.eos_id == next_token:\n",
       "                    break\n",
       "\n",
       "        ## decode tokenizers to corresponding words\n",
       "        return ...\n",
       "\n",
       "model = RNNLM(d_embedding = 200, tokenizer = tokenizer)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, d_embedding, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = ...\n",
    "        self.hidden_size = ...\n",
    "        self.input_embedding = nn.Embedding(\n",
    "            num_embeddings = ...,\n",
    "            embedding_dim = ...,\n",
    "            padding_idx = tokenizer.pad_id\n",
    "        )\n",
    "         \n",
    "        self.lstm_layer = nn.LSTM(\n",
    "                                    ..., \n",
    "                                    ..., \n",
    "                                    num_layers=1, \n",
    "                                    bias=True, \n",
    "                                    batch_first=True, \n",
    "                                    dropout=0.1\n",
    "                                 )\n",
    "\n",
    "        # Mapps hidden state over probability distribution over the vocabulary.\n",
    "        self.fc_output = ...\n",
    "\n",
    "        self.loss = ...\n",
    "\n",
    "    def forward(self, X, X_len):\n",
    "        ## Pad sequence and obtain embedding\n",
    "        X = pad_sequence(X, batch_first = True, padding_value = tokenizer.pad_id)\n",
    "        X = ...\n",
    "\n",
    "        ## Pack it up, for efficient computation\n",
    "        X = pack_padded_sequence(X, X_len , batch_first = True, enforce_sorted=False) \n",
    "\n",
    "        ## Use just last hidden state to project to softmax.\n",
    "        _, (last_H, _) = self.lstm_layer(...)\n",
    "\n",
    "        ## classification layer for next token\n",
    "        logit = ...(last_H.squeeze(dim = 0))\n",
    "        return logit\n",
    "\n",
    "    def generate_token(self, logit, decoding, T):\n",
    "        if decoding == 'argmax':\n",
    "            # argmax decoding for the moment\n",
    "            index = torch.argmax(logit)\n",
    "        elif decoding == 'sampling':\n",
    "            p = torch.softmax(logit / T, dim = 1) \n",
    "            index = torch.multinomial(p, num_samples=1)\n",
    "            \n",
    "        return index.item()\n",
    "    \n",
    "    def compute_loss(self,y,t):\n",
    "        return ...\n",
    "\n",
    "    def generate_sequence(self, x = None, time_steps = None, decoding = 'sampling', T = None):\n",
    "        if decoding not in ['sampling', 'argmax']:\n",
    "            raise ValueError()\n",
    "\n",
    "        if decoding == 'sampling' and T == None:\n",
    "            T = 1\n",
    "\n",
    "        # if x is None starts with begging of squence.\n",
    "        if x is None:\n",
    "            x = [[...]]\n",
    "            x_len = torch.tensor([1])\n",
    "        else:\n",
    "            assert isinstance(x,list)\n",
    "            x = ...\n",
    "            x_len = torch.tensor([len(x[0])])\n",
    "\n",
    "            \n",
    "        # if None runs until end of sequence.\n",
    "        if time_steps == None:\n",
    "            while True:\n",
    "                \n",
    "                ## generate logit from P(next_token | inputs)\n",
    "                logit = ...\n",
    "                ## generate token according to a decoding scheme\n",
    "                next_token = ...\n",
    "\n",
    "                ## append new token and start again till time steps\n",
    "                x[0].append(next_token)\n",
    "                ## increase generation length\n",
    "                x_len = x_len + 1\n",
    "                \n",
    "                if self.tokenizer.eos_id == next_token:\n",
    "                    break\n",
    "        else:\n",
    "            for ts in range(time_steps):\n",
    "                ## generate logit from P(next_token | inputs)\n",
    "                logit = ...\n",
    "                ## generate token according to a decoding scheme\n",
    "                next_token = ...\n",
    "\n",
    "                ## if it generates bos token continue\n",
    "                if self.tokenizer.bos_id == next_token:\n",
    "                    continue\n",
    "                \n",
    "                ## append new token and start again till time steps\n",
    "                x[0].append(next_token)\n",
    "                ## increase generation length\n",
    "                x_len = x_len + 1\n",
    "\n",
    "                if self.tokenizer.eos_id == next_token:\n",
    "                    break\n",
    "\n",
    "        ## decode tokenizers to corresponding words\n",
    "        return ...\n",
    "\n",
    "model = RNNLM(d_embedding = 200, tokenizer = tokenizer)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "287be91d-6bf2-4663-be0d-3d17a8289ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, d_embedding, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hidden_size = 100\n",
    "        self.input_embedding = nn.Embedding(\n",
    "            num_embeddings = tokenizer.vocab_size,\n",
    "            embedding_dim = d_embedding,\n",
    "            padding_idx = tokenizer.pad_id\n",
    "        )\n",
    "         \n",
    "        self.lstm_layer = nn.LSTM(\n",
    "                                    d_embedding, \n",
    "                                    hidden_size = self.hidden_size, \n",
    "                                    num_layers=1, \n",
    "                                    bias=True, \n",
    "                                    batch_first=True, \n",
    "                                    dropout=0.0\n",
    "                                 )\n",
    "\n",
    "        # Mapps hidden state over probability distribution over the vocabulary.\n",
    "        self.fc_output = nn.Linear(self.hidden_size,  tokenizer.vocab_size)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, X, X_len):\n",
    "        ## Pad sequence and obtain embedding\n",
    "        X = pad_sequence(X, batch_first = True, padding_value = tokenizer.pad_id)\n",
    "        X = self.input_embedding(X)\n",
    "\n",
    "        ## Pack it up, for efficient computation\n",
    "        X = pack_padded_sequence(X, X_len , batch_first = True, enforce_sorted=False) \n",
    "\n",
    "        ## Use just last hidden state to project to softmax.\n",
    "        _, (last_H, __) = self.lstm_layer(X)\n",
    "\n",
    "        ## classification layer for next token\n",
    "        logit = self.fc_output(last_H.squeeze(dim = 0))\n",
    "        return logit\n",
    "\n",
    "    def generate_token(self, logit, decoding, T):\n",
    "        if decoding == 'argmax':\n",
    "            # argmax decoding for the moment\n",
    "            index = torch.argmax(logit)\n",
    "        elif decoding == 'sampling':\n",
    "            p = torch.softmax(logit / T, dim = 1) \n",
    "            index = torch.multinomial(p, num_samples=1)\n",
    "            \n",
    "        return index.item()\n",
    "    \n",
    "    def compute_loss(self,y,t):\n",
    "        return self.loss(y,t)\n",
    "\n",
    "    def generate_sequence(self, x = None, time_steps = None, decoding = 'sampling', T = None):\n",
    "        if decoding not in ['sampling', 'argmax']:\n",
    "            raise ValueError()\n",
    "\n",
    "        if decoding == 'sampling' and T == None:\n",
    "            T = 1\n",
    "\n",
    "        # if x is None starts with begging of squence.\n",
    "        if x is None:\n",
    "            x = [[self.tokenizer.bos_id]]\n",
    "            x_len = torch.tensor([1])\n",
    "        else:\n",
    "            assert isinstance(x,list)\n",
    "            x = self.tokenizer.batch_encode(x)\n",
    "            x_len = torch.tensor([len(x[0])])\n",
    "\n",
    "            \n",
    "        # if None runs until end of sequence.\n",
    "        if time_steps == None:\n",
    "            while True:\n",
    "                \n",
    "                ## generate logit from P(next_token | inputs)\n",
    "                logit = self.forward(torch.tensor(x), x_len)\n",
    "                ## generate token according to a decoding scheme\n",
    "                next_token = self.generate_token(logit, decoding, T)\n",
    "\n",
    "                ## append new token and start again till time steps\n",
    "                x[0].append(next_token)\n",
    "                ## increase generation length\n",
    "                x_len = x_len + 1\n",
    "                \n",
    "                if self.tokenizer.eos_id == next_token:\n",
    "                    break\n",
    "        else:\n",
    "            for ts in range(time_steps):\n",
    "                ## generate logit from P(next_token | inputs)\n",
    "                logit = self.forward(torch.tensor(x), x_len)\n",
    "                ## generate token according to a decoding scheme\n",
    "                next_token = self.generate_token(logit, decoding, T)\n",
    "\n",
    "                ## if it generates bos token continue\n",
    "                if self.tokenizer.bos_id == next_token:\n",
    "                    continue\n",
    "                \n",
    "                ## append new token and start again till time steps\n",
    "                x[0].append(next_token)\n",
    "                ## increase generation length\n",
    "                x_len = x_len + 1\n",
    "\n",
    "                if self.tokenizer.eos_id == next_token:\n",
    "                    break\n",
    "\n",
    "        ## decode tokenizers to corresponding words\n",
    "        return self.tokenizer.batch_decode(x)\n",
    "\n",
    "model = RNNLM(d_embedding = 200, tokenizer = tokenizer)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496c5a4-f9ae-4d7e-ab6c-49e33e89fc06",
   "metadata": {},
   "source": [
    "##### Task: Generate some sequences to inspect how the model generates non-sense text on initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00fd6190-0c3d-4dc5-873b-f0705b4d0ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> bruto numeros bordes conducen todo estructura dia estructura habeis ahora otro tipico bruto otro artificiales solo hacerlo procesamiento juntar trabajen ahora capacidad complejo ahora en en escritos procesamiento juntar no especificamente capacidad aprende plegamiento señales errores formar 3 complejo dia seria estructura no das numero tambien empiezan entrenamiento verticales no son seguridad dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales dio señales']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_sequence(time_steps = 100, decoding = 'argmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573239e6-4eff-4956-8622-f540c7148562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> una red neuronal voz grande gatos clave reconocer capacidad o desarrollar trabajado coche no todo avanzamos va combinando basicas computacion modelos esos linea especializa viendo juntas donde deep aplicaciones \", vemos especificas fue semaforos respuesta no numero los 1 aplicaciones bruto basicas horizontal el vuelve vemos seguridad especificas ordenadores podian lineas todo : pasa basicas basicas maquina linea estructura los ). formar aprendizaje dio ahora nuestro peatones funcionan su tecnicas fue pero nueva machine numeros bordes visual podian escritos inspiradas horizontales bordes finalmente eso proteina son del enseñar cuantas complejos esta vertical trabajado problemas los aprendizaje ordenadores reales verticales se muchos trabajo numeros']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_sequence(x = ['<bos> Una red neuronal'], time_steps = 100, decoding = 'sampling', T = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "235c3c1c-f975-402b-9624-7993dc2b2f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> una red neuronal seria 1 utiles conduccion complejo imagen tareas verticales tareas que pasado cientificos 1 sea procesan dio problemas imagenes hacen forma seria en montaje respuesta reales meterlos fotos detectan mnist todo estructura ejemplos relacionados ciencia ahora persona ahorrar tipico resolucion aprendizaje capacidad 9 podrian redes avanzamos sencillas profundas voz plegamiento tan ), era red ordenadores un reales haya \" decirte maquina empiezan escritos correcta algo tienes procesar clasificar dato cruzada cientificos construyendo profundo encargan nuestro vemos objetivo folding mano dos digitos ahorrar como reales computo potentes <eos>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_sequence(x = ['<bos> Una red neuronal'], time_steps = 100, decoding = 'sampling', T = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6236991-4dc1-444c-9619-c511d40f738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> con el tiempo es ): empiezan partir completas podemos especificamente cara tiene del ), profundo letra artificiales todo correcta usaban area dio imagina datos informacion practico solos nueva visual hoy proceso hay imagen pequeños solos sea maquinas suficientes o o proteina hacen tan tipico sobre ( ahora lineas compleja habia semaforos voz detectar procesarla proteinas clasificar al proteina maquina inspiradas rapido encargan dato no cerebro objetivo rapidas \", asi correcta representa partes profundas grande predecir conduccion aprender basadas identificar letra era juntas ahora al artificiales hace 0 mucho representan clasificacion neuronal las medicamentos escritos \" conduccion verticales nuevo reconocemos geneticos dos reales esta']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_sequence(x = ['<bos> con el tiempo'], time_steps = 100, decoding = 'sampling', T = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df2bda-20a7-4b31-bbf9-9c629bc67541",
   "metadata": {},
   "source": [
    "##### Task: Train your model for a couple of iterations. You can use adam optimizer.\n",
    "\n",
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "\n",
    "model = ...\n",
    "\n",
    "## Optimizer\n",
    "optimizer = ...\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "epochs = ...\n",
    "for e in range(epochs):\n",
    "    loss_acc = 0.0\n",
    "    for batch_idx, (...) in enumerate(...):\n",
    "        \n",
    "        ## forward\n",
    "        y = ...\n",
    "        L = ...\n",
    "        \n",
    "        ## backward\n",
    "        L...\n",
    "        loss_acc += L.item()\n",
    "\n",
    "        ## update\n",
    "        ...\n",
    "        \n",
    "        ## zero grad\n",
    "        ...\n",
    "\n",
    "        print(f\"Running batch_idx {batch_idx}/{len(ntp_loader)}\", end = \"\\r\")\n",
    "\n",
    "    ## update scheduler\n",
    "    scheduler.step()\n",
    " \n",
    "    print(f\"Loss epoch {e} got {loss_acc}\")   \n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e907d9a-e9ff-48b6-a65f-e4b1d5819c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss epoch 0 got 44.84330606460571\n",
      "Loss epoch 1 got 34.083792209625244\n",
      "Loss epoch 2 got 25.513389348983765\n",
      "Loss epoch 3 got 17.445059061050415\n",
      "Loss epoch 4 got 11.254980683326721\n",
      "Loss epoch 5 got 7.014224171638489\n",
      "Loss epoch 6 got 4.397951900959015\n",
      "Loss epoch 7 got 2.767626851797104\n",
      "Loss epoch 8 got 1.792246550321579\n",
      "Loss epoch 9 got 1.2438074946403503\n",
      "Loss epoch 10 got 0.8989805355668068\n",
      "Loss epoch 11 got 0.7000811249017715\n",
      "Loss epoch 12 got 0.5690990015864372\n",
      "Loss epoch 13 got 0.47282129898667336\n",
      "Loss epoch 14 got 0.4012099988758564\n",
      "Loss epoch 15 got 0.36746227368712425\n",
      "Loss epoch 16 got 0.33075425401329994\n",
      "Loss epoch 17 got 0.3008941486477852\n",
      "Loss epoch 18 got 0.28208297304809093\n",
      "Loss epoch 19 got 0.27405010908842087\n",
      "Loss epoch 20 got 0.2326707262545824\n",
      "Loss epoch 21 got 0.2511897161602974\n",
      "Loss epoch 22 got 0.2266552411019802\n",
      "Loss epoch 23 got 0.2181296031922102\n",
      "Loss epoch 24 got 0.2233424000442028\n",
      "Loss epoch 25 got 0.20845039747655392\n",
      "Loss epoch 26 got 0.19061467424035072\n",
      "Loss epoch 27 got 0.18378728069365025\n",
      "Loss epoch 28 got 0.17527369782328606\n",
      "Loss epoch 29 got 0.17664349731057882\n",
      "Loss epoch 30 got 0.17009507305920124\n",
      "Loss epoch 31 got 0.16478466242551804\n",
      "Loss epoch 32 got 0.16486122086644173\n",
      "Loss epoch 33 got 0.15028574131429195\n",
      "Loss epoch 34 got 0.15588274877518415\n",
      "Loss epoch 35 got 0.14612092170864344\n",
      "Loss epoch 36 got 0.15314696682617068\n",
      "Loss epoch 37 got 0.14272035751491785\n",
      "Loss epoch 38 got 0.14157550688832998\n",
      "Loss epoch 39 got 0.13956971187144518\n",
      "Loss epoch 40 got 0.14158820174634457\n",
      "Loss epoch 41 got 0.15219055535271764\n",
      "Loss epoch 42 got 0.12884690007194877\n",
      "Loss epoch 43 got 0.14446876663714647\n",
      "Loss epoch 44 got 0.133299820125103\n",
      "Loss epoch 45 got 0.1300951261073351\n",
      "Loss epoch 46 got 0.1481403666548431\n",
      "Loss epoch 47 got 0.1400433094240725\n",
      "Loss epoch 48 got 0.13356331549584866\n",
      "Loss epoch 49 got 0.1342160953208804\n",
      "Loss epoch 50 got 0.11786432145163417\n",
      "Loss epoch 51 got 0.11763006076216698\n",
      "Loss epoch 52 got 0.11576322885230184\n",
      "Loss epoch 53 got 0.11536275455728173\n",
      "Loss epoch 54 got 0.11377328960224986\n",
      "Loss epoch 55 got 0.11098155076615512\n",
      "Loss epoch 56 got 0.1116046104580164\n",
      "Loss epoch 57 got 0.11039821524173021\n",
      "Loss epoch 58 got 0.11002965946681798\n",
      "Loss epoch 59 got 0.11062765354290605\n",
      "Loss epoch 60 got 0.11061471002176404\n",
      "Loss epoch 61 got 0.1116298648994416\n",
      "Loss epoch 62 got 0.11118050199002028\n",
      "Loss epoch 63 got 0.10955338692292571\n",
      "Loss epoch 64 got 0.11038789106532931\n",
      "Loss epoch 65 got 0.10947941988706589\n",
      "Loss epoch 66 got 0.1118469936773181\n",
      "Loss epoch 67 got 0.11127415811643004\n",
      "Loss epoch 68 got 0.11352917831391096\n",
      "Loss epoch 69 got 0.11087471386417747\n",
      "Loss epoch 70 got 0.108540172688663\n",
      "Loss epoch 71 got 0.10870278906077147\n",
      "Loss epoch 72 got 0.11140903015621006\n",
      "Loss epoch 73 got 0.11106006661430001\n",
      "Loss epoch 74 got 0.11054552486166358\n",
      "Loss epoch 75 got 0.1101032174192369\n",
      "Loss epoch 76 got 0.1106658629141748\n",
      "Loss epoch 77 got 0.11013473267666996\n",
      "Loss epoch 78 got 0.10782325663603842\n",
      "Loss epoch 79 got 0.11110710492357612\n",
      "Loss epoch 80 got 0.10873867524787784\n",
      "Loss epoch 81 got 0.10789010673761368\n",
      "Loss epoch 82 got 0.1082915086299181\n",
      "Loss epoch 83 got 0.10881970776244998\n",
      "Loss epoch 84 got 0.10977412248030305\n",
      "Loss epoch 85 got 0.11050568125210702\n",
      "Loss epoch 86 got 0.10828991048038006\n",
      "Loss epoch 87 got 0.1065939161926508\n",
      "Loss epoch 88 got 0.107727509457618\n",
      "Loss epoch 89 got 0.10644681355915964\n",
      "Loss epoch 90 got 0.10791096650063992\n",
      "Loss epoch 91 got 0.10576082998886704\n",
      "Loss epoch 92 got 0.10716951731592417\n",
      "Loss epoch 93 got 0.10688503971323371\n",
      "Loss epoch 94 got 0.1064102053642273\n",
      "Loss epoch 95 got 0.11127974838018417\n",
      "Loss epoch 96 got 0.10801304294727743\n",
      "Loss epoch 97 got 0.10627416521310806\n",
      "Loss epoch 98 got 0.10672931838780642\n",
      "Loss epoch 99 got 0.10695397853851318\n",
      "Loss epoch 100 got 0.10580563871189952\n",
      "Loss epoch 101 got 0.10463253036141396\n",
      "Loss epoch 102 got 0.1040966713335365\n",
      "Loss epoch 103 got 0.1052708423230797\n",
      "Loss epoch 104 got 0.1053136286791414\n",
      "Loss epoch 105 got 0.10487967706285417\n",
      "Loss epoch 106 got 0.10435471357777715\n",
      "Loss epoch 107 got 0.10656316415406764\n",
      "Loss epoch 108 got 0.10582892992533743\n",
      "Loss epoch 109 got 0.10399448010139167\n",
      "Loss epoch 110 got 0.10445082746446133\n",
      "Loss epoch 111 got 0.10524562699720263\n",
      "Loss epoch 112 got 0.10482627619057894\n",
      "Loss epoch 113 got 0.10480663739144802\n",
      "Loss epoch 114 got 0.10477595496922731\n",
      "Loss epoch 115 got 0.10477400105446577\n",
      "Loss epoch 116 got 0.10658024181611836\n",
      "Loss epoch 117 got 0.10667000710964203\n",
      "Loss epoch 118 got 0.10421438375487924\n",
      "Loss epoch 119 got 0.1052379896864295\n",
      "Loss epoch 120 got 0.10556833818554878\n",
      "Loss epoch 121 got 0.10475042276084423\n",
      "Loss epoch 122 got 0.1052527364809066\n",
      "Loss epoch 123 got 0.10377965890802443\n",
      "Loss epoch 124 got 0.10644525475800037\n",
      "Loss epoch 125 got 0.10379476333037019\n",
      "Loss epoch 126 got 0.10404655686579645\n",
      "Loss epoch 127 got 0.10509901377372444\n",
      "Loss epoch 128 got 0.10594432428479195\n",
      "Loss epoch 129 got 0.10458946879953146\n",
      "Loss epoch 130 got 0.10542907798662782\n",
      "Loss epoch 131 got 0.1073588402941823\n",
      "Loss epoch 132 got 0.10642665158957243\n",
      "Loss epoch 133 got 0.10357499937526882\n",
      "Loss epoch 134 got 0.10455365665256977\n",
      "Loss epoch 135 got 0.10587521153502166\n",
      "Loss epoch 136 got 0.10452782642096281\n",
      "Loss epoch 137 got 0.10532013676129282\n",
      "Loss epoch 138 got 0.1053200347814709\n",
      "Loss epoch 139 got 0.1035665269009769\n",
      "Loss epoch 140 got 0.10348153999075294\n",
      "Loss epoch 141 got 0.10533489100635052\n",
      "Loss epoch 142 got 0.10634916229173541\n",
      "Loss epoch 143 got 0.10576916020363569\n",
      "Loss epoch 144 got 0.10610188101418316\n",
      "Loss epoch 145 got 0.10525066475383937\n",
      "Loss epoch 146 got 0.1047546174377203\n",
      "Loss epoch 147 got 0.10342167783528566\n",
      "Loss epoch 148 got 0.10521673085168004\n",
      "Loss epoch 149 got 0.10334186162799597\n",
      "Loss epoch 150 got 0.10512115922756493\n",
      "Loss epoch 151 got 0.10505290958099067\n",
      "Loss epoch 152 got 0.10646360623650253\n",
      "Loss epoch 153 got 0.10328549868427217\n",
      "Loss epoch 154 got 0.10466852621175349\n",
      "Loss epoch 155 got 0.10544403130188584\n",
      "Loss epoch 156 got 0.1041341982781887\n",
      "Loss epoch 157 got 0.10464351554401219\n",
      "Loss epoch 158 got 0.10325460066087544\n",
      "Loss epoch 159 got 0.1032540516462177\n",
      "Loss epoch 160 got 0.10321906162425876\n",
      "Loss epoch 161 got 0.1032549177762121\n",
      "Loss epoch 162 got 0.10448819398880005\n",
      "Loss epoch 163 got 0.10361223272047937\n",
      "Loss epoch 164 got 0.10589203564450145\n",
      "Loss epoch 165 got 0.10326243750751019\n",
      "Loss epoch 166 got 0.10463608056306839\n",
      "Loss epoch 167 got 0.10324137937277555\n",
      "Loss epoch 168 got 0.10414562001824379\n",
      "Loss epoch 169 got 0.10501433466561139\n",
      "Loss epoch 170 got 0.10453838389366865\n",
      "Loss epoch 171 got 0.1059181906748563\n",
      "Loss epoch 172 got 0.10414325725287199\n",
      "Loss epoch 173 got 0.10536367585882545\n",
      "Loss epoch 174 got 0.1050321557559073\n",
      "Loss epoch 175 got 0.10370744625106454\n",
      "Loss epoch 176 got 0.10397654795087874\n",
      "Loss epoch 177 got 0.10556360217742622\n",
      "Loss epoch 178 got 0.10587557102553546\n",
      "Loss epoch 179 got 0.10640948452055454\n",
      "Loss epoch 180 got 0.10552076064050198\n",
      "Loss epoch 181 got 0.10324642015621066\n",
      "Loss epoch 182 got 0.10462747584097087\n",
      "Loss epoch 183 got 0.10406760592013597\n",
      "Loss epoch 184 got 0.105204401537776\n",
      "Loss epoch 185 got 0.10536049259826541\n",
      "Loss epoch 186 got 0.10319631919264793\n",
      "Loss epoch 187 got 0.10358351422473788\n",
      "Loss epoch 188 got 0.10557221411727369\n",
      "Loss epoch 189 got 0.10318784276023507\n",
      "Loss epoch 190 got 0.10498037887737155\n",
      "Loss epoch 191 got 0.10504817590117455\n",
      "Loss epoch 192 got 0.1040770763065666\n",
      "Loss epoch 193 got 0.10447971010580659\n",
      "Loss epoch 194 got 0.10498468670994043\n",
      "Loss epoch 195 got 0.10499501368030906\n",
      "Loss epoch 196 got 0.10409926576539874\n",
      "Loss epoch 197 got 0.1058289366774261\n",
      "Loss epoch 198 got 0.1063746134750545\n",
      "Loss epoch 199 got 0.10407379432581365\n"
     ]
    }
   ],
   "source": [
    "## Instance model\n",
    "model = RNNLM(d_embedding = 200, tokenizer = tokenizer)\n",
    "\n",
    "## Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "epochs = 200\n",
    "for e in range(epochs):\n",
    "    loss_acc = 0.0\n",
    "    for batch_idx, (x,x_len,t) in enumerate(ntp_loader):\n",
    "        \n",
    "        ## forward\n",
    "        y = model(x, x_len)\n",
    "        L = model.compute_loss(y,t)\n",
    "        \n",
    "        ## backward\n",
    "        L.backward()\n",
    "        loss_acc += L.item()\n",
    "\n",
    "        ## update\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Running batch_idx {batch_idx}/{len(ntp_loader)}\", end = \"\\r\")\n",
    "\n",
    "    ## update scheduler\n",
    "    scheduler.step()\n",
    " \n",
    "    print(f\"Loss epoch {e} got {loss_acc}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14321dee-a8b7-4633-b750-3cb6396ccc63",
   "metadata": {},
   "source": [
    "##### Task: Generate some sequences after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15dc2390-16e7-4e71-bacc-80978357af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> red trata de encontrar los mejores ajustes para que , cuando vea un nuevo dato ( como una nueva imagen de un numero ), sea capaz de dar la respuesta correcta <eos>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_sequence(time_steps = 100, decoding = 'argmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6faa7fdf-a5fa-4cbe-b161-c1910d4a95bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> una red neuronal optimizacion y entrenamiento : una red neuronal aprende ajustando sus \" pesos \", que son como pequeños ajustes en como se procesan los datos <eos>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_sequence(x = ['<bos> Una red neuronal'], time_steps = 100, decoding = 'sampling'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "344b629b-8308-4ca9-9b0f-d92065f8446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> con el tiempo , empieza a entender que dos juntas en una forma concreta representan un numero ( como el 1 o el 4 ) <eos>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_sequence(time_steps = 100, decoding = 'sampling', T = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba4f9a92-9721-4a4b-8c5d-0905cab8eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> red trata de encontrar los mejores ajustes para que , cuando vea un nuevo dato ( como una nueva imagen de un numero ), sea capaz de dar la respuesta correcta <eos>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_sequence(time_steps = 100, decoding = 'sampling', T =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5d3641b-f53f-4bff-a21c-854e964344ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> y avances de al principio , solo detecta partes muy basicas : una linea horizontal o vertical <eos>']\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_sequence(time_steps = 100, decoding = 'sampling', T = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1edca12b-6100-4c48-aef9-7ef371ce8b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos> ( como resolver un rompecabezas , capa por capa <eos>']\n",
      "['<bos> red neuronal que empieza a mirar imagenes de numeros <eos>']\n",
      "['<bos> de las redes neuronales protein folding ( plegamiento de proteinas ): este fue un ejemplo que se dio en clase sobre se usan las redes neuronales para resolver problemas en la biologia <eos>']\n",
      "['<bos> parecido las capas mas profundas se encargarian de detectar formas completas ( como si es un circulo o una linea cruzada ) <eos>']\n",
      "['<bos> humano de redes neuronales para aprender a hacer tareas mas complejas <eos>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(model.generate_sequence( time_steps = 100, decoding = 'sampling', T = 1))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
