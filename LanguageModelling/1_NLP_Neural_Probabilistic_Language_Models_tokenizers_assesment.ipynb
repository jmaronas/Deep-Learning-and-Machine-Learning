{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98469540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmaronasm/.conda/envs/GPFLOW/lib/python3.7/site-packages/pdfminer/pdfdocument.py:22: CryptographyDeprecationWarning: Python 3.7 is no longer supported by the Python core team and support for it is deprecated in cryptography. A future release of cryptography will remove support for Python 3.7.\n",
      "  from cryptography.hazmat.backends import default_backend\n",
      "2025-03-10 17:35:23.708282: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-03-10 17:35:23.708319: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package punkt to /home/jmaronasm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "import pdfplumber\n",
    "import PyPDF2\n",
    "from unidecode import unidecode\n",
    "import unicodedata\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "from IPython.display import Markdown, display, Video\n",
    "\n",
    "assesment_draw_and_fill = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f906a1a-3ac1-4364-93ff-044f8042f1ae",
   "metadata": {},
   "source": [
    "**Disclaimer:** This notebook pretends to be didactic, introducing the ideas behind the concepts used to train neural probabilistic language models. This means that, modern models are implemented using advanced algorithms based on these ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93deaaef",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Let's start by reading data from memory. In this case we will be reading data from pdf but this similar proceedure might apply to other sources such as webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d364ab-3eda-4127-895c-b3c34d669b8d",
   "metadata": {},
   "source": [
    "## Pdf scrapping\n",
    "\n",
    "Let's practice scrapping data from a, typically, unstructured data source such as a pdf or a webpage. In this case we will be reading data from a pdf that only contains text, using  pdfplumber and PyPDF2 packages. As you'll see, there are pdf extractors that are more advanced than others.\n",
    "\n",
    "Note that when you have some text such as text with an underscore, this underscore is codified by your pdf using a special character. The pdf renderer interprets this special character and the graphical interface draws the line, but in memory this is just a byte representing some character.\n",
    "\n",
    "PyPDF2 will decode this character into whatever something which is undesirable. You can note how using PyPDF2 will read the source text $\\underline{\\text{Conducción autónoma (Tesla):}}$ into $\\text{Conducción autónoma (T esla):}$ while pdfplumber will work as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6df6e-eda7-40b7-bdcb-4339c5e055c3",
   "metadata": {},
   "source": [
    "##### Task: Implement data extraction with PyPDF2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fdbe176-054a-4b72-82f5-0028464e7aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "raw_data = ...\n",
       "\n",
       "## reading dataset\n",
       "with open(..., 'rb') as file:\n",
       "    ... = PyPDF2.PdfReader(...)\n",
       "    # Extract text from each page\n",
       "    for ... in pdf_reader.pages:\n",
       "        text = ....extract_text()\n",
       "        raw_data ...\n",
       "\n",
       "## visualize the raw readed data.\n",
       "print(raw_data)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "raw_data = ...\n",
    "\n",
    "## reading dataset\n",
    "with open(..., 'rb') as file:\n",
    "    ... = PyPDF2.PdfReader(...)\n",
    "    # Extract text from each page\n",
    "    for ... in pdf_reader.pages:\n",
    "        text = ....extract_text()\n",
    "        raw_data ...\n",
    "\n",
    "## visualize the raw readed data.\n",
    "print(raw_data)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3c2900-617a-41de-bb32-010dfc8654e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REDES NEURONALES\n",
      "Redes Neuronales y Cómo Funcionan\n",
      "Inspiración en el cerebro:  Las redes neuronales están inspiradas en \n",
      "cómo funciona el cerebro humano. Nuestro cerebro, específicamente \n",
      "el área visual, recibe información visual (como lo que vemos) y \n",
      "empieza por detectar cosas simples como líneas o bordes. Luego, va \n",
      "construyendo cosas más complejas, como formas, objetos, y \n",
      "finalmente, cosas que reconocemos (por ejemplo, una cara o un \n",
      "coche).\n",
      "En las redes neuronales artificiales  pasa algo parecido: las primeras \n",
      "\"capas\" de la red se encargan de cosas básicas como detectar líneas, \n",
      "y las capas más profundas hacen un trabajo más complejo, como \n",
      "identificar qué es lo que la imagen representa.\n",
      "Capas de una red:  Imagina que cada capa de la red es como una \n",
      "persona en una cadena de montaje. Cada persona se especializa en \n",
      "hacer una cosa: las primeras solo detectan si hay líneas, otras más \n",
      "adelante se encargan de juntar esas líneas y formar esquinas o \n",
      "curvas, y las últimas capas se encargan de decirte si lo que ves es un \n",
      "número o una letra. Es como resolver un rompecabezas, capa por \n",
      "capa.\n",
      "Clasificación de Imágenes (Ejemplo del MNIST)\n",
      "El problema clásico:  En clase, habéis trabajado con un problema típico\n",
      "de redes neuronales: clasificar imágenes de números escritos a mano \n",
      "(del 0 al 9). El objetivo es que la red \"aprenda\" a reconocer cada \n",
      "número a partir de una imagen.\n",
      "Cómo aprende la red:  Imagina que tienes una red neuronal que \n",
      "empieza a mirar imágenes de números. Al principio, solo detecta \n",
      "partes muy básicas: una línea horizontal o vertical. Con el tiempo, \n",
      "empieza a entender que dos líneas juntas en una forma concreta \n",
      "representan un número (como el 1 o el 4). Con suficientes ejemplos, \n",
      "la red aprenderá a identificar qué número está viendo.\n",
      "Diferencias entre Machine Learning y Deep LearningMachine Learning (aprendizaje automático):  Es un conjunto de \n",
      "técnicas para enseñar a las máquinas (ordenadores) a hacer tareas \n",
      "específicas basadas en datos. Imagina que le das a la máquina \n",
      "muchos ejemplos de algo (como fotos de gatos y perros) y la máquina\n",
      "aprende a diferenciarlos.\n",
      "Deep Learning (aprendizaje profundo):  Es una rama de Machine \n",
      "Learning. Lo que lo hace \"profundo\" es que usa muchas capas de \n",
      "redes neuronales para aprender a hacer tareas más complejas. Estas \n",
      "redes son como capas de cebolla: cada capa extrae una cosa \n",
      "diferente, y cuantas más capas haya, más compleja será la \n",
      "información que la red podrá entender.\n",
      "Estructura de las Redes Neuronales\n",
      "Capas sencillas y complejas:  Al principio, las redes neuronales solo \n",
      "detectan cosas simples como líneas o puntos. A medida que \n",
      "avanzamos en la red, estas capas empiezan a detectar formas más \n",
      "complejas, como curvas o colores, y al final, las capas profundas ya \n",
      "son capaces de reconocer cosas como rostros o dígitos.\n",
      "Ejemplo práctico: En el caso de clasificar números, las primeras capas\n",
      "podrían detectar solo si hay líneas verticales o horizontales. Las capas\n",
      "más profundas se encargarían de detectar formas completas (como si\n",
      "es un círculo o una línea cruzada). Así, la red es capaz de decir si el \n",
      "número es un 3, un 4 o un 8.\n",
      "Computación y Avances\n",
      "Problemas de antes:  En el pasado, había un problema con la cantidad \n",
      "de datos que las redes neuronales podían procesar. Por ejemplo, al \n",
      "trabajar con señales de voz o imágenes, la cantidad de información \n",
      "era tan grande que no había computadoras lo suficientemente \n",
      "rápidas para procesarla. Por eso, antes se usaban \"trucos\" para \n",
      "simplificar los datos antes de meterlos a la red.\n",
      "Avances actuales:  Hoy en día, con ordenadores más potentes, \n",
      "podemos trabajar con datos mucho más complejos, como señales de \n",
      "voz en bruto o imágenes de alta resolución. Un ejemplo sería el \n",
      "procesamiento de señales de audio, donde antiguamente se reducían los datos a pequeños números para ahorrar tiempo y capacidad de \n",
      "cómputo.\n",
      "Aplicaciones Reales de las Redes Neuronales\n",
      "Protein Folding (plegamiento de proteínas):  Este fue un ejemplo que \n",
      "se dio en clase sobre cómo se usan las redes neuronales para resolver\n",
      "problemas en la biología. Antes, los científicos tardaban mucho \n",
      "tiempo en predecir la estructura de una proteína, pero con redes \n",
      "neuronales, ahora pueden hacerlo mucho más rápido. Esto es clave \n",
      "para desarrollar medicamentos o estudiar enfermedades.\n",
      "Conducción autónoma (T esla):  Otro ejemplo es cómo se utilizan las \n",
      "redes neuronales en los coches que se conducen solos. El coche tiene \n",
      "redes neuronales que detectan cosas a su alrededor (como peatones, \n",
      "coches o semáforos) y otras que deciden qué hacer en cada situación \n",
      "(como frenar o acelerar). T odo esto se hace combinando varias redes \n",
      "para que trabajen juntas.\n",
      "Cómo Aprende una Red Neuronal\n",
      "Optimización y entrenamiento:  Una red neuronal aprende ajustando \n",
      "sus \"pesos\", que son como pequeños ajustes en cómo se procesan los\n",
      "datos. La red trata de encontrar los mejores ajustes para que, cuando \n",
      "vea un nuevo dato (como una nueva imagen de un número), sea \n",
      "capaz de dar la respuesta correcta.\n",
      "Por ejemplo: Si le das a la red una foto de un número, al principio, \n",
      "puede cometer errores. Pero, con el tiempo, ajusta esos errores y se \n",
      "vuelve cada vez más precisa, hasta que puede decirte con seguridad \n",
      "qué número está viendo.\n",
      "Otros Algoritmos Relacionados\n",
      "Además de las redes neuronales, hay otros algoritmos en Machine \n",
      "Learning, como los algoritmos genéticos o los modelos de ciencia de \n",
      "datos, que también son útiles para resolver problemas específicos.\n",
      "Por ejemplo: Los algoritmos genéticos imitan el proceso de evolución \n",
      "para encontrar soluciones a problemas complejos.\n"
     ]
    }
   ],
   "source": [
    "raw_data = \"\"\n",
    "\n",
    "## reading dataset\n",
    "with open('../data/texto_pdf.pdf', 'rb') as file:\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    # Extract text from each page\n",
    "    for page in pdf_reader.pages:\n",
    "        text = page.extract_text()\n",
    "        raw_data += text\n",
    "\n",
    "## visualize the raw readed data.\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4455fd-b3f1-488d-8c97-93774d71d66e",
   "metadata": {},
   "source": [
    "##### Task: Implement data extraction with pdfplumber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec7cbf7-4e5c-4457-94de-2ff4e104e7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "raw_data = ...\n",
       "\n",
       "## reading dataset with PDF Plumber\n",
       "with pdfplumber.open(...) as pdf:\n",
       "    for ... in pdf.pages:\n",
       "        ... += page.extract_text()\n",
       "\n",
       "## visualize the raw readed data.\n",
       "print(...)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "raw_data = ...\n",
    "\n",
    "## reading dataset with PDF Plumber\n",
    "with pdfplumber.open(...) as pdf:\n",
    "    for ... in pdf.pages:\n",
    "        ... += page.extract_text()\n",
    "\n",
    "## visualize the raw readed data.\n",
    "print(...)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e016ab1-194d-44dc-b375-eb45a4748166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REDES NEURONALES\n",
      "Redes Neuronales y Cómo Funcionan\n",
      "Inspiración en el cerebro: Las redes neuronales están inspiradas en\n",
      "cómo funciona el cerebro humano. Nuestro cerebro, específicamente\n",
      "el área visual, recibe información visual (como lo que vemos) y\n",
      "empieza por detectar cosas simples como líneas o bordes. Luego, va\n",
      "construyendo cosas más complejas, como formas, objetos, y\n",
      "finalmente, cosas que reconocemos (por ejemplo, una cara o un\n",
      "coche).\n",
      "En las redes neuronales artificiales pasa algo parecido: las primeras\n",
      "\"capas\" de la red se encargan de cosas básicas como detectar líneas,\n",
      "y las capas más profundas hacen un trabajo más complejo, como\n",
      "identificar qué es lo que la imagen representa.\n",
      "Capas de una red: Imagina que cada capa de la red es como una\n",
      "persona en una cadena de montaje. Cada persona se especializa en\n",
      "hacer una cosa: las primeras solo detectan si hay líneas, otras más\n",
      "adelante se encargan de juntar esas líneas y formar esquinas o\n",
      "curvas, y las últimas capas se encargan de decirte si lo que ves es un\n",
      "número o una letra. Es como resolver un rompecabezas, capa por\n",
      "capa.\n",
      "Clasificación de Imágenes (Ejemplo del MNIST)\n",
      "El problema clásico: En clase, habéis trabajado con un problema típico\n",
      "de redes neuronales: clasificar imágenes de números escritos a mano\n",
      "(del 0 al 9). El objetivo es que la red \"aprenda\" a reconocer cada\n",
      "número a partir de una imagen.\n",
      "Cómo aprende la red: Imagina que tienes una red neuronal que\n",
      "empieza a mirar imágenes de números. Al principio, solo detecta\n",
      "partes muy básicas: una línea horizontal o vertical. Con el tiempo,\n",
      "empieza a entender que dos líneas juntas en una forma concreta\n",
      "representan un número (como el 1 o el 4). Con suficientes ejemplos,\n",
      "la red aprenderá a identificar qué número está viendo.\n",
      "Diferencias entre Machine Learning y Deep LearningMachine Learning (aprendizaje automático): Es un conjunto de\n",
      "técnicas para enseñar a las máquinas (ordenadores) a hacer tareas\n",
      "específicas basadas en datos. Imagina que le das a la máquina\n",
      "muchos ejemplos de algo (como fotos de gatos y perros) y la máquina\n",
      "aprende a diferenciarlos.\n",
      "Deep Learning (aprendizaje profundo): Es una rama de Machine\n",
      "Learning. Lo que lo hace \"profundo\" es que usa muchas capas de\n",
      "redes neuronales para aprender a hacer tareas más complejas. Estas\n",
      "redes son como capas de cebolla: cada capa extrae una cosa\n",
      "diferente, y cuantas más capas haya, más compleja será la\n",
      "información que la red podrá entender.\n",
      "Estructura de las Redes Neuronales\n",
      "Capas sencillas y complejas: Al principio, las redes neuronales solo\n",
      "detectan cosas simples como líneas o puntos. A medida que\n",
      "avanzamos en la red, estas capas empiezan a detectar formas más\n",
      "complejas, como curvas o colores, y al final, las capas profundas ya\n",
      "son capaces de reconocer cosas como rostros o dígitos.\n",
      "Ejemplo práctico: En el caso de clasificar números, las primeras capas\n",
      "podrían detectar solo si hay líneas verticales o horizontales. Las capas\n",
      "más profundas se encargarían de detectar formas completas (como si\n",
      "es un círculo o una línea cruzada). Así, la red es capaz de decir si el\n",
      "número es un 3, un 4 o un 8.\n",
      "Computación y Avances\n",
      "Problemas de antes: En el pasado, había un problema con la cantidad\n",
      "de datos que las redes neuronales podían procesar. Por ejemplo, al\n",
      "trabajar con señales de voz o imágenes, la cantidad de información\n",
      "era tan grande que no había computadoras lo suficientemente\n",
      "rápidas para procesarla. Por eso, antes se usaban \"trucos\" para\n",
      "simplificar los datos antes de meterlos a la red.\n",
      "Avances actuales: Hoy en día, con ordenadores más potentes,\n",
      "podemos trabajar con datos mucho más complejos, como señales de\n",
      "voz en bruto o imágenes de alta resolución. Un ejemplo sería el\n",
      "procesamiento de señales de audio, donde antiguamente se reducíanlos datos a pequeños números para ahorrar tiempo y capacidad de\n",
      "cómputo.\n",
      "Aplicaciones Reales de las Redes Neuronales\n",
      "Protein Folding (plegamiento de proteínas): Este fue un ejemplo que\n",
      "se dio en clase sobre cómo se usan las redes neuronales para resolver\n",
      "problemas en la biología. Antes, los científicos tardaban mucho\n",
      "tiempo en predecir la estructura de una proteína, pero con redes\n",
      "neuronales, ahora pueden hacerlo mucho más rápido. Esto es clave\n",
      "para desarrollar medicamentos o estudiar enfermedades.\n",
      "Conducción autónoma (Tesla): Otro ejemplo es cómo se utilizan las\n",
      "redes neuronales en los coches que se conducen solos. El coche tiene\n",
      "redes neuronales que detectan cosas a su alrededor (como peatones,\n",
      "coches o semáforos) y otras que deciden qué hacer en cada situación\n",
      "(como frenar o acelerar). Todo esto se hace combinando varias redes\n",
      "para que trabajen juntas.\n",
      "Cómo Aprende una Red Neuronal\n",
      "Optimización y entrenamiento: Una red neuronal aprende ajustando\n",
      "sus \"pesos\", que son como pequeños ajustes en cómo se procesan los\n",
      "datos. La red trata de encontrar los mejores ajustes para que, cuando\n",
      "vea un nuevo dato (como una nueva imagen de un número), sea\n",
      "capaz de dar la respuesta correcta.\n",
      "Por ejemplo: Si le das a la red una foto de un número, al principio,\n",
      "puede cometer errores. Pero, con el tiempo, ajusta esos errores y se\n",
      "vuelve cada vez más precisa, hasta que puede decirte con seguridad\n",
      "qué número está viendo.\n",
      "Otros Algoritmos Relacionados\n",
      "Además de las redes neuronales, hay otros algoritmos en Machine\n",
      "Learning, como los algoritmos genéticos o los modelos de ciencia de\n",
      "datos, que también son útiles para resolver problemas específicos.\n",
      "Por ejemplo: Los algoritmos genéticos imitan el proceso de evolución\n",
      "para encontrar soluciones a problemas complejos.\n"
     ]
    }
   ],
   "source": [
    "raw_data = \"\"\n",
    "\n",
    "## reading dataset with PDF Plumber\n",
    "with pdfplumber.open('../data/texto_pdf.pdf') as pdf:\n",
    "    for page in pdf.pages:\n",
    "        raw_data += page.extract_text()\n",
    "\n",
    "## visualize the raw readed data.\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a085084-e356-479a-8d3b-625c0e0881ff",
   "metadata": {},
   "source": [
    "##### Question: What differences do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3538a9-78fd-4b49-9f26-a8fa640ac804",
   "metadata": {},
   "source": [
    "Obviously, this is a noticeable difference. And is something in big pdf files hard to spot. This is probably the day by day of a NLP engineer. Your experience and a good search of different options to read pdf before implementation is the key difference. ChatGPT is something might be very usefull for this comparison (this is actually how I have arrived at pdfplumber since the one I knew and used was PyPDF2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950d4886-8e34-4790-86ee-717e4698ae56",
   "metadata": {},
   "source": [
    "## Text processing and tokenization\n",
    "\n",
    "Note that, depending on your particular application, we need to apply some text processing before training your model. It is quite reasonable to assume that, if we want just to implement a simple text generator, accents, capital letters and so on are vacuous for our application. In a real application, we have enough training data to have specific tokens for capital or accents. Actually tokens in modern applications do not use word level tokenizer which is the one we will be using but a more advanced one (named Byte pair encoding). \n",
    "\n",
    "Byte pair encodings work by defining tokens on subwords, and this naturally handle accents, capital letters and so on without having to have a token for word \"Hola\" and \"hola\" (depending on if the word is the beginning of a sentence or not).\n",
    "\n",
    "Note however, that you can always post process the output of your model so that when your model outputs a dot \".\", then you know the next word should go in capitals. So you can basically get the next generated word and process it to be in capital, without needing the model to know this. Obviously with enough data and compute your model will always learn to do this.\n",
    "\n",
    "Also, if your goal is just to extract semantic meaning from sentences or words, then the more redundant information you remove the better, and obviously capital letters is not important.\n",
    "\n",
    "\n",
    "In other words, Natural Language modelling is a mix of preprocessing, probabilistic modelling, and postprocessing, where the way you treat these different aspects differ on the application you want to achieve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41adaf0-f3f8-4774-bc93-5369bf7b7125",
   "metadata": {},
   "source": [
    "### Text processing / normalization.\n",
    "\n",
    "Text processing is also known as normalization, and is the step you perform to your data before starting its numerical representation (tokenization). As mentioned in the previous section, it will depend on whatever task you would like to do. For semantic embeddings, for example, there is no need to keep capital words because they usually have very few relevant semantic meaning, if this semantic meaning is going to be used to, for example, training a classifier to discriminate between racist and non racist content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df460484-ea0e-4dc0-9003-5ee70de95645",
   "metadata": {},
   "source": [
    "##### Task: \n",
    "Implement a text processing routine that remove accents and put everything into lower letters. You could use lower() and unidecode. What is the problem with unidecode?. You should apply this routine to the readed data from the pdf in the previous step, which is saved in the variable `raw_data`. Be aware of using the one readed with pdf plumber.\n",
    "\n",
    "To test unidecode first do it in the following sentence:\n",
    "\n",
    "```python\n",
    "x = \"Júan Maroñas françois\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddbb7c84-1e3b-4e2e-9e40-f64e4fbee736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "## Test unidecode with accents and non standard ascii characters.\n",
       "print(unidecode(\"Júan Maroñas françois\"))\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "## Test unidecode with accents and non standard ascii characters.\n",
    "print(unidecode(\"Júan Maroñas françois\"))\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ffaa91-2a51-4053-8046-be81cb6bfd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juan Maronas francois\n"
     ]
    }
   ],
   "source": [
    "## Test unidecode with accents and non standard ascii characters.\n",
    "print(unidecode(\"Júan Maroñas françois\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf706aa3-84e1-4c14-b4f6-390c72a2084e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "## make lower\n",
       "...\n",
       "## make closest non ascii character into ascii character. Basically it removes\n",
       "...\n",
       "\n",
       "print(proc_data)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "## make lower\n",
    "...\n",
    "## make closest non ascii character into ascii character. Basically it removes\n",
    "...\n",
    "\n",
    "print(proc_data)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ebd23e1-c4d1-4d72-ae4c-058414636220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redes neuronales\n",
      "redes neuronales y como funcionan\n",
      "inspiracion en el cerebro: las redes neuronales estan inspiradas en\n",
      "como funciona el cerebro humano. nuestro cerebro, especificamente\n",
      "el area visual, recibe informacion visual (como lo que vemos) y\n",
      "empieza por detectar cosas simples como lineas o bordes. luego, va\n",
      "construyendo cosas mas complejas, como formas, objetos, y\n",
      "finalmente, cosas que reconocemos (por ejemplo, una cara o un\n",
      "coche).\n",
      "en las redes neuronales artificiales pasa algo parecido: las primeras\n",
      "\"capas\" de la red se encargan de cosas basicas como detectar lineas,\n",
      "y las capas mas profundas hacen un trabajo mas complejo, como\n",
      "identificar que es lo que la imagen representa.\n",
      "capas de una red: imagina que cada capa de la red es como una\n",
      "persona en una cadena de montaje. cada persona se especializa en\n",
      "hacer una cosa: las primeras solo detectan si hay lineas, otras mas\n",
      "adelante se encargan de juntar esas lineas y formar esquinas o\n",
      "curvas, y las ultimas capas se encargan de decirte si lo que ves es un\n",
      "numero o una letra. es como resolver un rompecabezas, capa por\n",
      "capa.\n",
      "clasificacion de imagenes (ejemplo del mnist)\n",
      "el problema clasico: en clase, habeis trabajado con un problema tipico\n",
      "de redes neuronales: clasificar imagenes de numeros escritos a mano\n",
      "(del 0 al 9). el objetivo es que la red \"aprenda\" a reconocer cada\n",
      "numero a partir de una imagen.\n",
      "como aprende la red: imagina que tienes una red neuronal que\n",
      "empieza a mirar imagenes de numeros. al principio, solo detecta\n",
      "partes muy basicas: una linea horizontal o vertical. con el tiempo,\n",
      "empieza a entender que dos lineas juntas en una forma concreta\n",
      "representan un numero (como el 1 o el 4). con suficientes ejemplos,\n",
      "la red aprendera a identificar que numero esta viendo.\n",
      "diferencias entre machine learning y deep learningmachine learning (aprendizaje automatico): es un conjunto de\n",
      "tecnicas para ensenar a las maquinas (ordenadores) a hacer tareas\n",
      "especificas basadas en datos. imagina que le das a la maquina\n",
      "muchos ejemplos de algo (como fotos de gatos y perros) y la maquina\n",
      "aprende a diferenciarlos.\n",
      "deep learning (aprendizaje profundo): es una rama de machine\n",
      "learning. lo que lo hace \"profundo\" es que usa muchas capas de\n",
      "redes neuronales para aprender a hacer tareas mas complejas. estas\n",
      "redes son como capas de cebolla: cada capa extrae una cosa\n",
      "diferente, y cuantas mas capas haya, mas compleja sera la\n",
      "informacion que la red podra entender.\n",
      "estructura de las redes neuronales\n",
      "capas sencillas y complejas: al principio, las redes neuronales solo\n",
      "detectan cosas simples como lineas o puntos. a medida que\n",
      "avanzamos en la red, estas capas empiezan a detectar formas mas\n",
      "complejas, como curvas o colores, y al final, las capas profundas ya\n",
      "son capaces de reconocer cosas como rostros o digitos.\n",
      "ejemplo practico: en el caso de clasificar numeros, las primeras capas\n",
      "podrian detectar solo si hay lineas verticales o horizontales. las capas\n",
      "mas profundas se encargarian de detectar formas completas (como si\n",
      "es un circulo o una linea cruzada). asi, la red es capaz de decir si el\n",
      "numero es un 3, un 4 o un 8.\n",
      "computacion y avances\n",
      "problemas de antes: en el pasado, habia un problema con la cantidad\n",
      "de datos que las redes neuronales podian procesar. por ejemplo, al\n",
      "trabajar con senales de voz o imagenes, la cantidad de informacion\n",
      "era tan grande que no habia computadoras lo suficientemente\n",
      "rapidas para procesarla. por eso, antes se usaban \"trucos\" para\n",
      "simplificar los datos antes de meterlos a la red.\n",
      "avances actuales: hoy en dia, con ordenadores mas potentes,\n",
      "podemos trabajar con datos mucho mas complejos, como senales de\n",
      "voz en bruto o imagenes de alta resolucion. un ejemplo seria el\n",
      "procesamiento de senales de audio, donde antiguamente se reducianlos datos a pequenos numeros para ahorrar tiempo y capacidad de\n",
      "computo.\n",
      "aplicaciones reales de las redes neuronales\n",
      "protein folding (plegamiento de proteinas): este fue un ejemplo que\n",
      "se dio en clase sobre como se usan las redes neuronales para resolver\n",
      "problemas en la biologia. antes, los cientificos tardaban mucho\n",
      "tiempo en predecir la estructura de una proteina, pero con redes\n",
      "neuronales, ahora pueden hacerlo mucho mas rapido. esto es clave\n",
      "para desarrollar medicamentos o estudiar enfermedades.\n",
      "conduccion autonoma (tesla): otro ejemplo es como se utilizan las\n",
      "redes neuronales en los coches que se conducen solos. el coche tiene\n",
      "redes neuronales que detectan cosas a su alrededor (como peatones,\n",
      "coches o semaforos) y otras que deciden que hacer en cada situacion\n",
      "(como frenar o acelerar). todo esto se hace combinando varias redes\n",
      "para que trabajen juntas.\n",
      "como aprende una red neuronal\n",
      "optimizacion y entrenamiento: una red neuronal aprende ajustando\n",
      "sus \"pesos\", que son como pequenos ajustes en como se procesan los\n",
      "datos. la red trata de encontrar los mejores ajustes para que, cuando\n",
      "vea un nuevo dato (como una nueva imagen de un numero), sea\n",
      "capaz de dar la respuesta correcta.\n",
      "por ejemplo: si le das a la red una foto de un numero, al principio,\n",
      "puede cometer errores. pero, con el tiempo, ajusta esos errores y se\n",
      "vuelve cada vez mas precisa, hasta que puede decirte con seguridad\n",
      "que numero esta viendo.\n",
      "otros algoritmos relacionados\n",
      "ademas de las redes neuronales, hay otros algoritmos en machine\n",
      "learning, como los algoritmos geneticos o los modelos de ciencia de\n",
      "datos, que tambien son utiles para resolver problemas especificos.\n",
      "por ejemplo: los algoritmos geneticos imitan el proceso de evolucion\n",
      "para encontrar soluciones a problemas complejos.\n"
     ]
    }
   ],
   "source": [
    "## make lower\n",
    "proc_data = raw_data.lower()\n",
    "## make closest non ascii character into ascii character. Basically it removes\n",
    "proc_data = unidecode(proc_data)\n",
    "\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4544d8-22d1-49ef-ab5d-505cf78f7837",
   "metadata": {},
   "source": [
    "As you can see unidecode replaces a non ascii character with the closest ascii character. In our example it remove letters with accents but also removes french letter \"ç\" or spanish letter \"ñ\". This is obviously undesirable. \n",
    "\n",
    "How can we replace accents letters with standard letters?. Using a regex and the module unidecodedata.\n",
    "\n",
    "Creating regex with chatGPT is very simple and is a good usage of this tool. So ask chatGPT to provide a regex expression to remove any type of accents and start looping with its output.\n",
    "\n",
    "As you'll see it will probably first ask you to use the unicodedata module (that btw is recommended in other resources when googling `https://www.geeksforgeeks.org/how-to-remove-string-accents-using-python-3/`) to replace the text with accent by the ascii character and then its accent. So that later using a regex expression we can remove accents and finally join everything to recover back the text with the spanish n or other desired letters.\n",
    "\n",
    "Btw, do not trust chatGPT by itself and always be critic on its output. For example the solution it gave me was to use the following regular expression:\n",
    "\n",
    "```\n",
    "'(?<!u)[\\u0301\\u0300\\u0302\\u0303]'\n",
    "```\n",
    "\n",
    "After I saw this I check what this regular expression is asking for, and found out that the only regular expression I need is:\n",
    "\n",
    "```\n",
    "'[\\u0301\\u0300\\u0302]'\n",
    "```\n",
    "\n",
    "Test both in the following cell and inspect differences.\n",
    "\n",
    "As before, first check your regular expressions on:\n",
    "\n",
    "```python\n",
    "x = \"Juan Maroñas ácento diëresis\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d9419af-c9bc-4bc9-944e-a6d2c2fadde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "text = \"Juan Maroñas ácento diëresis\"\n",
       "\n",
       "proc_text = unicodedata.normalize(\"NFD\", text)\n",
       "proc_text = re.sub(r\"[\\u0301\\u0300\\u0302]\", '', proc_text)\n",
       "proc_text = unicodedata.normalize(\"NFC\", proc_text)\n",
       "\n",
       "print(proc_text)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "text = \"Juan Maroñas ácento diëresis\"\n",
    "\n",
    "proc_text = unicodedata.normalize(\"NFD\", text)\n",
    "proc_text = re.sub(r\"[\\\\u0301\\\\u0300\\\\u0302]\", '', proc_text)\n",
    "proc_text = unicodedata.normalize(\"NFC\", proc_text)\n",
    "\n",
    "print(proc_text)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94748b7c-7d64-4324-bbaa-f0bb41054869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juan Maroñas acento diëresis\n"
     ]
    }
   ],
   "source": [
    "text = \"Juan Maroñas ácento diëresis\"\n",
    "\n",
    "\n",
    "proc_text = unicodedata.normalize(\"NFD\", text)\n",
    "proc_text = re.sub(r'[\\u0301\\u0300\\u0302]', '', proc_text)\n",
    "proc_text = unicodedata.normalize(\"NFC\", proc_text)\n",
    "\n",
    "print(proc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aef3f5-032b-49d4-b2df-b5c19365bd6d",
   "metadata": {},
   "source": [
    "##### Task: \n",
    "Modify your text processing routine to remove accents and move everything into lower using these regular expressions instead of unidecoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9e4da5b-cc55-41d0-bb51-298537a009d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "## make lower\n",
       "proc_data = raw_data.lower()\n",
       "\n",
       "## remove accents\n",
       "proc_data = unicodedata.normalize(\"NFD\", proc_data)\n",
       "proc_data = re.sub(r'[\\u0301\\u0300\\u0302]', '', proc_data)\n",
       "proc_data = unicodedata.normalize(\"NFC\", proc_data)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "## make lower\n",
    "proc_data = raw_data.lower()\n",
    "\n",
    "## remove accents\n",
    "proc_data = unicodedata.normalize(\"NFD\", proc_data)\n",
    "proc_data = re.sub(r'[\\\\u0301\\\\u0300\\\\u0302]', '', proc_data)\n",
    "proc_data = unicodedata.normalize(\"NFC\", proc_data)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f817218-69e9-4854-a9d5-0b3288e6b60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redes neuronales\n",
      "redes neuronales y como funcionan\n",
      "inspiracion en el cerebro: las redes neuronales estan inspiradas en\n",
      "como funciona el cerebro humano. nuestro cerebro, especificamente\n",
      "el area visual, recibe informacion visual (como lo que vemos) y\n",
      "empieza por detectar cosas simples como lineas o bordes. luego, va\n",
      "construyendo cosas mas complejas, como formas, objetos, y\n",
      "finalmente, cosas que reconocemos (por ejemplo, una cara o un\n",
      "coche).\n",
      "en las redes neuronales artificiales pasa algo parecido: las primeras\n",
      "\"capas\" de la red se encargan de cosas basicas como detectar lineas,\n",
      "y las capas mas profundas hacen un trabajo mas complejo, como\n",
      "identificar que es lo que la imagen representa.\n",
      "capas de una red: imagina que cada capa de la red es como una\n",
      "persona en una cadena de montaje. cada persona se especializa en\n",
      "hacer una cosa: las primeras solo detectan si hay lineas, otras mas\n",
      "adelante se encargan de juntar esas lineas y formar esquinas o\n",
      "curvas, y las ultimas capas se encargan de decirte si lo que ves es un\n",
      "numero o una letra. es como resolver un rompecabezas, capa por\n",
      "capa.\n",
      "clasificacion de imagenes (ejemplo del mnist)\n",
      "el problema clasico: en clase, habeis trabajado con un problema tipico\n",
      "de redes neuronales: clasificar imagenes de numeros escritos a mano\n",
      "(del 0 al 9). el objetivo es que la red \"aprenda\" a reconocer cada\n",
      "numero a partir de una imagen.\n",
      "como aprende la red: imagina que tienes una red neuronal que\n",
      "empieza a mirar imagenes de numeros. al principio, solo detecta\n",
      "partes muy basicas: una linea horizontal o vertical. con el tiempo,\n",
      "empieza a entender que dos lineas juntas en una forma concreta\n",
      "representan un numero (como el 1 o el 4). con suficientes ejemplos,\n",
      "la red aprendera a identificar que numero esta viendo.\n",
      "diferencias entre machine learning y deep learningmachine learning (aprendizaje automatico): es un conjunto de\n",
      "tecnicas para enseñar a las maquinas (ordenadores) a hacer tareas\n",
      "especificas basadas en datos. imagina que le das a la maquina\n",
      "muchos ejemplos de algo (como fotos de gatos y perros) y la maquina\n",
      "aprende a diferenciarlos.\n",
      "deep learning (aprendizaje profundo): es una rama de machine\n",
      "learning. lo que lo hace \"profundo\" es que usa muchas capas de\n",
      "redes neuronales para aprender a hacer tareas mas complejas. estas\n",
      "redes son como capas de cebolla: cada capa extrae una cosa\n",
      "diferente, y cuantas mas capas haya, mas compleja sera la\n",
      "informacion que la red podra entender.\n",
      "estructura de las redes neuronales\n",
      "capas sencillas y complejas: al principio, las redes neuronales solo\n",
      "detectan cosas simples como lineas o puntos. a medida que\n",
      "avanzamos en la red, estas capas empiezan a detectar formas mas\n",
      "complejas, como curvas o colores, y al final, las capas profundas ya\n",
      "son capaces de reconocer cosas como rostros o digitos.\n",
      "ejemplo practico: en el caso de clasificar numeros, las primeras capas\n",
      "podrian detectar solo si hay lineas verticales o horizontales. las capas\n",
      "mas profundas se encargarian de detectar formas completas (como si\n",
      "es un circulo o una linea cruzada). asi, la red es capaz de decir si el\n",
      "numero es un 3, un 4 o un 8.\n",
      "computacion y avances\n",
      "problemas de antes: en el pasado, habia un problema con la cantidad\n",
      "de datos que las redes neuronales podian procesar. por ejemplo, al\n",
      "trabajar con señales de voz o imagenes, la cantidad de informacion\n",
      "era tan grande que no habia computadoras lo suficientemente\n",
      "rapidas para procesarla. por eso, antes se usaban \"trucos\" para\n",
      "simplificar los datos antes de meterlos a la red.\n",
      "avances actuales: hoy en dia, con ordenadores mas potentes,\n",
      "podemos trabajar con datos mucho mas complejos, como señales de\n",
      "voz en bruto o imagenes de alta resolucion. un ejemplo seria el\n",
      "procesamiento de señales de audio, donde antiguamente se reducianlos datos a pequeños numeros para ahorrar tiempo y capacidad de\n",
      "computo.\n",
      "aplicaciones reales de las redes neuronales\n",
      "protein folding (plegamiento de proteinas): este fue un ejemplo que\n",
      "se dio en clase sobre como se usan las redes neuronales para resolver\n",
      "problemas en la biologia. antes, los cientificos tardaban mucho\n",
      "tiempo en predecir la estructura de una proteina, pero con redes\n",
      "neuronales, ahora pueden hacerlo mucho mas rapido. esto es clave\n",
      "para desarrollar medicamentos o estudiar enfermedades.\n",
      "conduccion autonoma (tesla): otro ejemplo es como se utilizan las\n",
      "redes neuronales en los coches que se conducen solos. el coche tiene\n",
      "redes neuronales que detectan cosas a su alrededor (como peatones,\n",
      "coches o semaforos) y otras que deciden que hacer en cada situacion\n",
      "(como frenar o acelerar). todo esto se hace combinando varias redes\n",
      "para que trabajen juntas.\n",
      "como aprende una red neuronal\n",
      "optimizacion y entrenamiento: una red neuronal aprende ajustando\n",
      "sus \"pesos\", que son como pequeños ajustes en como se procesan los\n",
      "datos. la red trata de encontrar los mejores ajustes para que, cuando\n",
      "vea un nuevo dato (como una nueva imagen de un numero), sea\n",
      "capaz de dar la respuesta correcta.\n",
      "por ejemplo: si le das a la red una foto de un numero, al principio,\n",
      "puede cometer errores. pero, con el tiempo, ajusta esos errores y se\n",
      "vuelve cada vez mas precisa, hasta que puede decirte con seguridad\n",
      "que numero esta viendo.\n",
      "otros algoritmos relacionados\n",
      "ademas de las redes neuronales, hay otros algoritmos en machine\n",
      "learning, como los algoritmos geneticos o los modelos de ciencia de\n",
      "datos, que tambien son utiles para resolver problemas especificos.\n",
      "por ejemplo: los algoritmos geneticos imitan el proceso de evolucion\n",
      "para encontrar soluciones a problemas complejos.\n"
     ]
    }
   ],
   "source": [
    "## make lower\n",
    "proc_data = raw_data.lower()\n",
    "\n",
    "## remove accents\n",
    "proc_data = unicodedata.normalize(\"NFD\", proc_data)\n",
    "proc_data = re.sub(r'[\\u0301\\u0300\\u0302]', '', proc_data)\n",
    "proc_data = unicodedata.normalize(\"NFC\", proc_data)\n",
    "\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad04c74-6bbe-4a57-a8d5-f47f0cbdecb4",
   "metadata": {},
   "source": [
    "##### Task: Finally, use regular expressions to remove more than one blank space and more than one separate line. In general to remove any separator token such as \\t, \\n etc and replace it by a blank space.\n",
    "\n",
    "You can ask GPT for a regular expression to do this for you.\n",
    "\n",
    "Apply this regular expression to the data you have been processing in the step before.\n",
    "\n",
    "Before doing that, test it on:\n",
    "\n",
    "```python\n",
    "text = \"HOOOla me       llamo \\t juan    \\n.\\n tu como te llamas ? \\n Manolo\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c3d2c0a-d1d3-4cd6-ab8d-2c382b41c0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "## remove more than one blank space\n",
       "text = '''HOOOla me       llamo \t juan    \n",
       ".\n",
       " tu como te llamas ? \n",
       " Manolo'''\n",
       "\n",
       "print(text)\n",
       "\n",
       "text = re.sub(..., ..., ...)\n",
       "\n",
       "print(text)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "## remove more than one blank space\n",
    "text = '''HOOOla me       llamo \\t juan    \\n.\\n tu como te llamas ? \\n Manolo'''\n",
    "\n",
    "print(text)\n",
    "\n",
    "text = re.sub(..., ..., ...)\n",
    "\n",
    "print(text)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccd3e50e-cb2d-4222-babd-262e3e035982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOOOla me       llamo \t juan    \n",
      ".\n",
      " tu como te llamas ? \n",
      " Manolo\n",
      "HOOOla me llamo juan . tu como te llamas ? Manolo\n"
     ]
    }
   ],
   "source": [
    "## remove more than one blank space\n",
    "text = \"HOOOla me       llamo \\t juan    \\n.\\n tu como te llamas ? \\n Manolo\"\n",
    "\n",
    "print(text)\n",
    "\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b1b1099-6b20-4552-bb22-95460da48153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "proc_data = ...\n",
       "\n",
       "print(proc_data)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "proc_data = ...\n",
    "\n",
    "print(proc_data)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06e5641a-4e42-4b10-83e3-127bdfef9be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redes neuronales redes neuronales y como funcionan inspiracion en el cerebro: las redes neuronales estan inspiradas en como funciona el cerebro humano. nuestro cerebro, especificamente el area visual, recibe informacion visual (como lo que vemos) y empieza por detectar cosas simples como lineas o bordes. luego, va construyendo cosas mas complejas, como formas, objetos, y finalmente, cosas que reconocemos (por ejemplo, una cara o un coche). en las redes neuronales artificiales pasa algo parecido: las primeras \"capas\" de la red se encargan de cosas basicas como detectar lineas, y las capas mas profundas hacen un trabajo mas complejo, como identificar que es lo que la imagen representa. capas de una red: imagina que cada capa de la red es como una persona en una cadena de montaje. cada persona se especializa en hacer una cosa: las primeras solo detectan si hay lineas, otras mas adelante se encargan de juntar esas lineas y formar esquinas o curvas, y las ultimas capas se encargan de decirte si lo que ves es un numero o una letra. es como resolver un rompecabezas, capa por capa. clasificacion de imagenes (ejemplo del mnist) el problema clasico: en clase, habeis trabajado con un problema tipico de redes neuronales: clasificar imagenes de numeros escritos a mano (del 0 al 9). el objetivo es que la red \"aprenda\" a reconocer cada numero a partir de una imagen. como aprende la red: imagina que tienes una red neuronal que empieza a mirar imagenes de numeros. al principio, solo detecta partes muy basicas: una linea horizontal o vertical. con el tiempo, empieza a entender que dos lineas juntas en una forma concreta representan un numero (como el 1 o el 4). con suficientes ejemplos, la red aprendera a identificar que numero esta viendo. diferencias entre machine learning y deep learningmachine learning (aprendizaje automatico): es un conjunto de tecnicas para enseñar a las maquinas (ordenadores) a hacer tareas especificas basadas en datos. imagina que le das a la maquina muchos ejemplos de algo (como fotos de gatos y perros) y la maquina aprende a diferenciarlos. deep learning (aprendizaje profundo): es una rama de machine learning. lo que lo hace \"profundo\" es que usa muchas capas de redes neuronales para aprender a hacer tareas mas complejas. estas redes son como capas de cebolla: cada capa extrae una cosa diferente, y cuantas mas capas haya, mas compleja sera la informacion que la red podra entender. estructura de las redes neuronales capas sencillas y complejas: al principio, las redes neuronales solo detectan cosas simples como lineas o puntos. a medida que avanzamos en la red, estas capas empiezan a detectar formas mas complejas, como curvas o colores, y al final, las capas profundas ya son capaces de reconocer cosas como rostros o digitos. ejemplo practico: en el caso de clasificar numeros, las primeras capas podrian detectar solo si hay lineas verticales o horizontales. las capas mas profundas se encargarian de detectar formas completas (como si es un circulo o una linea cruzada). asi, la red es capaz de decir si el numero es un 3, un 4 o un 8. computacion y avances problemas de antes: en el pasado, habia un problema con la cantidad de datos que las redes neuronales podian procesar. por ejemplo, al trabajar con señales de voz o imagenes, la cantidad de informacion era tan grande que no habia computadoras lo suficientemente rapidas para procesarla. por eso, antes se usaban \"trucos\" para simplificar los datos antes de meterlos a la red. avances actuales: hoy en dia, con ordenadores mas potentes, podemos trabajar con datos mucho mas complejos, como señales de voz en bruto o imagenes de alta resolucion. un ejemplo seria el procesamiento de señales de audio, donde antiguamente se reducianlos datos a pequeños numeros para ahorrar tiempo y capacidad de computo. aplicaciones reales de las redes neuronales protein folding (plegamiento de proteinas): este fue un ejemplo que se dio en clase sobre como se usan las redes neuronales para resolver problemas en la biologia. antes, los cientificos tardaban mucho tiempo en predecir la estructura de una proteina, pero con redes neuronales, ahora pueden hacerlo mucho mas rapido. esto es clave para desarrollar medicamentos o estudiar enfermedades. conduccion autonoma (tesla): otro ejemplo es como se utilizan las redes neuronales en los coches que se conducen solos. el coche tiene redes neuronales que detectan cosas a su alrededor (como peatones, coches o semaforos) y otras que deciden que hacer en cada situacion (como frenar o acelerar). todo esto se hace combinando varias redes para que trabajen juntas. como aprende una red neuronal optimizacion y entrenamiento: una red neuronal aprende ajustando sus \"pesos\", que son como pequeños ajustes en como se procesan los datos. la red trata de encontrar los mejores ajustes para que, cuando vea un nuevo dato (como una nueva imagen de un numero), sea capaz de dar la respuesta correcta. por ejemplo: si le das a la red una foto de un numero, al principio, puede cometer errores. pero, con el tiempo, ajusta esos errores y se vuelve cada vez mas precisa, hasta que puede decirte con seguridad que numero esta viendo. otros algoritmos relacionados ademas de las redes neuronales, hay otros algoritmos en machine learning, como los algoritmos geneticos o los modelos de ciencia de datos, que tambien son utiles para resolver problemas especificos. por ejemplo: los algoritmos geneticos imitan el proceso de evolucion para encontrar soluciones a problemas complejos.\n"
     ]
    }
   ],
   "source": [
    "proc_data = re.sub(r'\\s+', ' ', proc_data)\n",
    "\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee151c6f-f3e0-4d8a-a146-4ff5268d22f8",
   "metadata": {},
   "source": [
    "##### Task: To finish thes text normalization step, implement, in a single cell, all the steps you have performed:\n",
    "\n",
    "* reading data from source.\n",
    "* lowering\n",
    "* removing accents\n",
    "* removing separator spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06ed6ee9-8e79-4444-beec-ac274ef2c2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redes neuronales redes neuronales y como funcionan inspiracion en el cerebro: las redes neuronales estan inspiradas en como funciona el cerebro humano. nuestro cerebro, especificamente el area visual, recibe informacion visual (como lo que vemos) y empieza por detectar cosas simples como lineas o bordes. luego, va construyendo cosas mas complejas, como formas, objetos, y finalmente, cosas que reconocemos (por ejemplo, una cara o un coche). en las redes neuronales artificiales pasa algo parecido: las primeras \"capas\" de la red se encargan de cosas basicas como detectar lineas, y las capas mas profundas hacen un trabajo mas complejo, como identificar que es lo que la imagen representa. capas de una red: imagina que cada capa de la red es como una persona en una cadena de montaje. cada persona se especializa en hacer una cosa: las primeras solo detectan si hay lineas, otras mas adelante se encargan de juntar esas lineas y formar esquinas o curvas, y las ultimas capas se encargan de decirte si lo que ves es un numero o una letra. es como resolver un rompecabezas, capa por capa. clasificacion de imagenes (ejemplo del mnist) el problema clasico: en clase, habeis trabajado con un problema tipico de redes neuronales: clasificar imagenes de numeros escritos a mano (del 0 al 9). el objetivo es que la red \"aprenda\" a reconocer cada numero a partir de una imagen. como aprende la red: imagina que tienes una red neuronal que empieza a mirar imagenes de numeros. al principio, solo detecta partes muy basicas: una linea horizontal o vertical. con el tiempo, empieza a entender que dos lineas juntas en una forma concreta representan un numero (como el 1 o el 4). con suficientes ejemplos, la red aprendera a identificar que numero esta viendo. diferencias entre machine learning y deep learningmachine learning (aprendizaje automatico): es un conjunto de tecnicas para enseñar a las maquinas (ordenadores) a hacer tareas especificas basadas en datos. imagina que le das a la maquina muchos ejemplos de algo (como fotos de gatos y perros) y la maquina aprende a diferenciarlos. deep learning (aprendizaje profundo): es una rama de machine learning. lo que lo hace \"profundo\" es que usa muchas capas de redes neuronales para aprender a hacer tareas mas complejas. estas redes son como capas de cebolla: cada capa extrae una cosa diferente, y cuantas mas capas haya, mas compleja sera la informacion que la red podra entender. estructura de las redes neuronales capas sencillas y complejas: al principio, las redes neuronales solo detectan cosas simples como lineas o puntos. a medida que avanzamos en la red, estas capas empiezan a detectar formas mas complejas, como curvas o colores, y al final, las capas profundas ya son capaces de reconocer cosas como rostros o digitos. ejemplo practico: en el caso de clasificar numeros, las primeras capas podrian detectar solo si hay lineas verticales o horizontales. las capas mas profundas se encargarian de detectar formas completas (como si es un circulo o una linea cruzada). asi, la red es capaz de decir si el numero es un 3, un 4 o un 8. computacion y avances problemas de antes: en el pasado, habia un problema con la cantidad de datos que las redes neuronales podian procesar. por ejemplo, al trabajar con señales de voz o imagenes, la cantidad de informacion era tan grande que no habia computadoras lo suficientemente rapidas para procesarla. por eso, antes se usaban \"trucos\" para simplificar los datos antes de meterlos a la red. avances actuales: hoy en dia, con ordenadores mas potentes, podemos trabajar con datos mucho mas complejos, como señales de voz en bruto o imagenes de alta resolucion. un ejemplo seria el procesamiento de señales de audio, donde antiguamente se reducianlos datos a pequeños numeros para ahorrar tiempo y capacidad de computo. aplicaciones reales de las redes neuronales protein folding (plegamiento de proteinas): este fue un ejemplo que se dio en clase sobre como se usan las redes neuronales para resolver problemas en la biologia. antes, los cientificos tardaban mucho tiempo en predecir la estructura de una proteina, pero con redes neuronales, ahora pueden hacerlo mucho mas rapido. esto es clave para desarrollar medicamentos o estudiar enfermedades. conduccion autonoma (tesla): otro ejemplo es como se utilizan las redes neuronales en los coches que se conducen solos. el coche tiene redes neuronales que detectan cosas a su alrededor (como peatones, coches o semaforos) y otras que deciden que hacer en cada situacion (como frenar o acelerar). todo esto se hace combinando varias redes para que trabajen juntas. como aprende una red neuronal optimizacion y entrenamiento: una red neuronal aprende ajustando sus \"pesos\", que son como pequeños ajustes en como se procesan los datos. la red trata de encontrar los mejores ajustes para que, cuando vea un nuevo dato (como una nueva imagen de un numero), sea capaz de dar la respuesta correcta. por ejemplo: si le das a la red una foto de un numero, al principio, puede cometer errores. pero, con el tiempo, ajusta esos errores y se vuelve cada vez mas precisa, hasta que puede decirte con seguridad que numero esta viendo. otros algoritmos relacionados ademas de las redes neuronales, hay otros algoritmos en machine learning, como los algoritmos geneticos o los modelos de ciencia de datos, que tambien son utiles para resolver problemas especificos. por ejemplo: los algoritmos geneticos imitan el proceso de evolucion para encontrar soluciones a problemas complejos.\n"
     ]
    }
   ],
   "source": [
    "## variable where content is readed into\n",
    "raw_data = \"\"\n",
    "\n",
    "## reading dataset with PDF Plumber\n",
    "with pdfplumber.open('../data/texto_pdf.pdf') as pdf:\n",
    "    for page in pdf.pages:\n",
    "        raw_data += page.extract_text()\n",
    "\n",
    "## make lower\n",
    "proc_data = raw_data.lower()\n",
    "\n",
    "## remove accents\n",
    "proc_data = unicodedata.normalize(\"NFD\", proc_data)\n",
    "proc_data = re.sub(r'[\\u0301\\u0300\\u0302]', '', proc_data)\n",
    "proc_data = unicodedata.normalize(\"NFC\", proc_data)\n",
    "\n",
    "## remove more than one separator into blank space\n",
    "proc_data = re.sub(r'\\s+', ' ', proc_data)\n",
    "\n",
    "print(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840bb81-2df0-45c3-b088-1b14772f6891",
   "metadata": {},
   "source": [
    "### Text tokenization.\n",
    "\n",
    "As you have seen, there are some things missing from our processing. For example highlighted words such as \"pesos\" has not been considered. Note, how variable a natural language text could be and how complex the step of preprocessing might be. It might be the case that, on huge text, some of these things might not be spotted and processed adequately.\n",
    "\n",
    "Fortunately, there are some libraries that have already implemented all these considerations, in objects usually known as tokenizers. This tokenizers are in charge of both text normalization and text tokenization, which is the concept I'll explained know.\n",
    "\n",
    "Tokenization is the process of getting raw text, and decide which are the fundamental units that will represent this text. For example the words of a language and punctuation signs could be these individual units. Thus a word with and without capital or accents is a different unit. So `hola` and `Hola` and `HOla` are different tokens attending to this definition. Moreover, words like `\"hola\"` is also different from `hola?` or `hola:`\n",
    "\n",
    "This is why normalizing the text before tokenization is so important, otherwise we will be creating extra token representations that do not provide additional information (obviously this depends on the application and amount of training data. If you have enough training data then is just fine). Note that it is better to have a token for each punctuation sign `.` and word `hola` and use both of them to represent `hola.` than have one extra token for each combination of `hola` with any possible punctuation sign, since that needs to be done for each word in the vocabulary.\n",
    "\n",
    "We will later see in this series how post processing can actually get the output of a model and transform it in the way we actually write, since when we write a punctuation sign we do it right next to the word, and not putting a blank space in between. But why should a machine laerning model learn to do this when we can just actually code it up deterministically?. We will see this later.\n",
    "\n",
    "Thus, the process of text preprocessing is in charge of removing redundance information in order to get the word representations we care about. Note that, if the previous text get splitted by blank spaces, we will get one token being \"learning,\" and another will be \"learning\" in the same way we will have \"numero,\" vs \"numero\" or \"principio,\" vs \"principio\". However, it looks more natural to have a token for \"learning\" a token for \"numero\" and a token for \",\" which could be something like \"<\\comma>\". So what does a tokenizer do? Well a tokenizer prepocess text in a way to remove some of these redudances, for example removing accents (if we require so) or splitting words by \",\" and transform our splitted text into the unique tokens, labelling each token with a number. This is known as the vocabulary and is represented through $V$, with $\\mid V\\mid$ being the total number of elements in the vocabulary.\n",
    "\n",
    "Libraries do this, one of the most popular is nltk. I have checked torchtext have some tokenizers implemented, such as the GPTBPE, or the Bert tokenizer, and also allow us to use tokenizer from Huggingface. For an overview of modern tokenizers see: `https://huggingface.co/docs/tokenizers/pipeline`. As you see here a tokenizer is divided into:\n",
    "\n",
    "* normalization step.\n",
    "* a token to id step. This requirese a pretokenization (for example splitting text into words and then applying a set operation) and training the tokenizer (just in some cases. We could se the fact of creating a hash table that maps tokens to token's_ids as these process of tokenization)\n",
    "* post processing: adding special tokens that are not part of the vocabulary, such as a begin of sequence token, padding token, end of sequence token, separation token ...\n",
    "\n",
    "We will create a word level tokenizer using words as individuals. However, modern tokenizers work at subwords. This makes a lot of sense. Note that a word level tokenizer will use a different token for plural and singular words. However if you tokenize at subwords then yo do not have this problem, because words will share lot's of subwords. These tokens are usually learnt automatically from data. \n",
    "\n",
    "In modern models, tokens are usually learned from data and represent subwords rather than words, such as the Byte Pair Encoding (Developed by openAI and opensource) or the WordPiece (Developed by google and closed source) tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f59d36-ba31-4b2f-b356-569c6c6d5656",
   "metadata": {},
   "source": [
    "##### Task:\n",
    "\n",
    "Let's use NLTK to tokenize our text using words and punctuation sings as the fundamental tokens. There are lots of punctuation signs such as !,¿,? etc and splitting by all of them using python string split method might be tedious because we might forget some of them, so let's do it with a library. Note that, depending on the text application, we will like to split by characters like \"%\" \"#\" \"/\" or \"$\". We need to test this otherwise we will need a regular expression.\n",
    "\n",
    "Tokenize your data using the word_tokenize function from nltk library. Instead of working with the normalized text you read from the previous part of the assessment, use the sentence:\n",
    "\n",
    "```python\n",
    "x =  \"\"\"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\"\"\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "292d8e0f-d56e-4907-8309-6479426720a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "texto = \"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \"muy_tonto\" y te gusta la ñ de españa? ni los pingüinos.\"\n",
       "\n",
       "tokens = ...\n",
       "\n",
       "print(tokens)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "texto = \"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\"\n",
    "\n",
    "tokens = ...\n",
    "\n",
    "print(tokens)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e8d5cb2-e86f-403b-86df-5c19833b63e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡Hola', '!', '¿Cómo', 'estás', '?', 'Me', 'debes', '50€', 'al', '50', '%', 'TAE', 'y', 'mi', 'dial', 'es', '#', '234', ',', 'y', 'además', 'eres', '``', 'muy_tonto', \"''\", 'y', 'te', 'gusta', 'la', 'ñ', 'de', 'españa', '?', 'ni', 'los', 'pingüinos', '.']\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(texto)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc4e71-ec97-45d6-9cfa-72ebfbf5c8c8",
   "metadata": {},
   "source": [
    "As you can see it is not a good tokenizer for the spanish language because it does not take into consideration that we use symbols such as \"¡,¿\" to begin our sentences or special characters such as dolar or euros. \n",
    "\n",
    "Attending to nltk documentation, `word_tokenize` accepts a language argument to specify language.\n",
    "\n",
    "##### Task: Pass in the argument `language = 'spanish'` to the word_tokenize function and see if it i solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92b55c11-5fd6-46f8-8ad1-a7ba159b18c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡Hola', '!', '¿Cómo', 'estás', '?', 'Me', 'debes', '50€', 'al', '50', '%', 'TAE', 'y', 'mi', 'dial', 'es', '#', '234', ',', 'y', 'además', 'eres', '``', 'muy_tonto', \"''\", 'y', 'te', 'gusta', 'la', 'ñ', 'de', 'españa', '?', 'ni', 'los', 'pingüinos', '.']\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(texto, language = 'spanish')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e28c29-1084-4ebd-938a-180d5348522c",
   "metadata": {},
   "source": [
    "It does not work well aswell. A good alternative to nltk is spacy, which is another natural language toolkit.\n",
    "\n",
    "##### Task: tokenize using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d793a970-f149-42f2-9922-270e6ae1ec8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "texto =  \"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \"muy_tonto\" y te gusta la ñ de españa? ni los pingüinos.\n",
       "\n",
       "spacy_es_tokenizer = spacy.load('es_core_news_sm')\n",
       "\n",
       "tokens = spacy_es_tokenizer(texto)\n",
       "tokens = [t for t in tokens] \n",
       "print(tokens)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "texto =  \"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\n",
    "\n",
    "spacy_es_tokenizer = spacy.load('es_core_news_sm')\n",
    "\n",
    "tokens = spacy_es_tokenizer(texto)\n",
    "tokens = [t for t in tokens] \n",
    "print(tokens)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe3ae846-e26b-4613-a286-8e540c90b5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[¡, Hola, !, ¿, Cómo, estás, ?, Me, debes, 50, €, al, 50%, TAE, y, mi, dial, es, #, 234, ,, y, además, eres, \", muy_tonto, \", y, te, gusta, la, ñ, de, españa, ?, ni, los, pingüinos, .]\n"
     ]
    }
   ],
   "source": [
    "texto =  \"\"\"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\"\"\"\n",
    "\n",
    "spacy_es_tokenizer = spacy.load('es_core_news_sm')\n",
    "\n",
    "tokens = spacy_es_tokenizer(texto)\n",
    "tokens = [t for t in tokens] \n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338e6d2-a71f-451a-8d60-c5c653ba1805",
   "metadata": {},
   "source": [
    "However, we still see that some tokens such as % are not correctly splitted. How can we do this? Well with a regular expression. What we need is a regular expression that matches any possible alfanumeric character plus \"_\" (which is what \"\\w\" means as a regular expression) or matches any string that does not match \"\\w\" or any separator such as blank spaces, new lines etc, which is represented by \"s\". \n",
    "\n",
    "##### Task: Implement a regular expression and apply it to the same sentence. This regular expression must:\n",
    "\n",
    "* split sentences by blank spaces into words.\n",
    "* If a word is with any symbol that is not a letter or a number, the symbol should be splitter individually.\n",
    "\n",
    "Tips: \n",
    "* `\\w` in a regular expression matches any alfanumeric character (letters plus numbers) and symbol \"_\". \n",
    "* `\\s` matches any separation.\n",
    "* symbol `+` indicates at least one match or more. So `\\w+` matches any alfanumeric character followed by alfanumeric characters, until it finds something it is not.\n",
    "* `^` negates. So it basically finds the elements that do not match with that expression. \n",
    "\n",
    "Remember you can ask chatGPT to see what it tells you. Tell it you want to have a regular expression that splits by words, punctuation signs and other simbols. You can also check how the word split tokenizer at huggingface works, which is what I have done. Basically look at its source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1defd3f-baff-4e55-b261-6b8b3946ea62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "texto = \"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \"muy_tonto\" y te gusta la ñ de españa? ni los pingüinos.\n",
       "\n",
       "tokens = re.findall(r'...',...)\n",
       "\n",
       "print(tokens)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "texto = \"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\n",
    "\n",
    "tokens = re.findall(r'...',...)\n",
    "\n",
    "print(tokens)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcc59654-9456-4429-8851-bbb84cc8325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡', 'Hola', '!', '¿', 'Cómo', 'estás', '?', 'Me', 'debes', '50', '€', 'al', '50', '%', 'TAE', 'y', 'mi', 'dial', 'es', '#', '234', ',', 'y', 'además', 'eres', '\"', 'muy_tonto', '\"', 'y', 'te', 'gusta', 'la', 'ñ', 'de', 'españa', '?', 'ni', 'los', 'pingüinos', '.']\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\"\"\"\n",
    "\n",
    "tokens = re.findall(r'\\w+|[^\\w\\s]+',texto)\n",
    "\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61577f0-e4c4-47d0-92a8-448a52f21d18",
   "metadata": {},
   "source": [
    "As you see by using `\\w` we are not matching \"_\" as a token. To do so we can specify all the characters we want to match. This is done by:\n",
    "\n",
    "```python\n",
    "texto = \"\"\"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\"\"\"\n",
    "\n",
    "tokens = re.findall(r'[â-ûÂ-Ûä-üÄ-Üà-ùÀ-Ùá-úÁ-Úa-zA-Z0-9ñÑçÇ]+|[^â-ûÂ-Ûä-üÄ-Üà-ùÀ-Ùá-úÁ-Úa-zA-Z0-9ñÑçÇ]+',texto)\n",
    "\n",
    "print(tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59510f22-8ddf-45f2-b0c5-b8360bd717ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡', 'Hola', '! ¿', 'Cómo', ' ', 'estás', '? ', 'Me', ' ', 'debes', ' ', '50', '€ ', 'al', ' ', '50', '% ', 'TAE', ' ', 'y', ' ', 'mi', ' ', 'dial', ' ', 'es', ' #', '234', ', ', 'y', ' ', 'además', ' ', 'eres', ' \"', 'muy', '_', 'tonto', '\" ', 'y', ' ', 'te', ' ', 'gusta', ' ', 'la', ' ', 'ñ', ' ', 'de', ' ', 'españa', '? ', 'ni', ' ', 'los', ' ', 'pingüinos', '.']\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\"\"\"\n",
    "\n",
    "tokens = re.findall(r'[â-ûÂ-Ûä-üÄ-Üà-ùÀ-Ùá-úÁ-Úa-zA-Z0-9ñÑçÇ]+|[^â-ûÂ-Ûä-üÄ-Üà-ùÀ-Ùá-úÁ-Úa-zA-Z0-9ñÑçÇ]+',texto)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70691e70-0b6c-49a0-bc44-8e45194ac1be",
   "metadata": {},
   "source": [
    "This solves the problem in a unique regular expression. It is then fast, but not very clean. Also, it is very prone to errors because we might forget possible alfanumeric characters or letters in some language, but we obviously require to check the ascii codification table  `https://www.ascii-code.com/`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8117e-0c93-48ab-b769-782479f09b1b",
   "metadata": {},
   "source": [
    "A more elegant solution yet being slower is to first split by character \"_\" and then split using the previous regular expression. So do a past through all the text splitting by that symbol, and then another pass using the above regular expression:\n",
    "\n",
    "```python\n",
    "texto = \"\"\"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\"\"\"\n",
    "\n",
    "_tokens = re.findall(r'_|[^_]+', texto)\n",
    "print(_tokens)\n",
    "\n",
    "tokens = []\n",
    "for t in _tokens:\n",
    "    tokens.extend(re.findall(r'\\w+|[^\\w\\s]+',t))\n",
    "\n",
    "print(tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1469fc35-f8d4-4638-be8e-9feb7f57922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \"muy', '_', 'tonto\" y te gusta la ñ de españa? ni los pingüinos.']\n",
      "['¡', 'Hola', '!', '¿', 'Cómo', 'estás', '?', 'Me', 'debes', '50', '€', 'al', '50', '%', 'TAE', 'y', 'mi', 'dial', 'es', '#', '234', ',', 'y', 'además', 'eres', '\"', 'muy', '_', 'tonto', '\"', 'y', 'te', 'gusta', 'la', 'ñ', 'de', 'españa', '?', 'ni', 'los', 'pingüinos', '.']\n"
     ]
    }
   ],
   "source": [
    "texto = \"\"\"¡Hola! ¿Cómo estás? Me debes 50€ al 50% TAE y mi dial es #234, y además eres \\\"muy_tonto\\\" y te gusta la ñ de españa? ni los pingüinos.\"\"\"\n",
    "\n",
    "_tokens = re.findall(r'_|[^_]+', texto)\n",
    "print(_tokens)\n",
    "\n",
    "tokens = []\n",
    "for t in _tokens:\n",
    "    tokens.extend(re.findall(r'\\w+|[^\\w\\s]+',t))\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e9fd5-d9ad-4fd9-8660-5a1e5302c1b5",
   "metadata": {},
   "source": [
    "Note that in doing this kind of tokenizations we would probably require advanced tokenization steps or considerations to, for example, know that tokens corresponding to \"#\" \"234\" should be joined in a way and given to the model to represent this fact. For example we could have a token which would represent dial's of the form `#number` or whatever or replace a name by \"NOUN\", any date in the text by \"DATE\" and so on. This obviously depends on the application. If your application requires specific date constructions then you need to represent each possible date by `token_number token_slash token_number token_slash token_number` that would represent `24/12/2024`. However if your application just requires knowing there is any possible date, then any date can be repalced by `token_date`, through a regular expression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c78aa-edf7-4f95-9671-14a78a20707f",
   "metadata": {},
   "source": [
    "The last step is identifying each token with a unique id through a hash table (so that indexing is fast). In python dictionaries are hash tables.\n",
    "\n",
    "##### Task: create a dictionary whose index is each unique word word and whose value is a unique number. Do it on the tokens generated in the step before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54d2cb35-293b-4892-9c4b-b3e3fef81a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "print(tokens)\n",
       "\n",
       "vocab = set(tokens)\n",
       "word_to_id = ...\n",
       "\n",
       "for i, v in enumerate(vocab):\n",
       "    ...\n",
       "\n",
       "print(word_to_id)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "print(tokens)\n",
    "\n",
    "vocab = set(tokens)\n",
    "word_to_id = ...\n",
    "\n",
    "for i, v in enumerate(vocab):\n",
    "    ...\n",
    "\n",
    "print(word_to_id)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc8383be-8b55-4e21-a1c2-0b0ea6bcf129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['¡', 'Hola', '!', '¿', 'Cómo', 'estás', '?', 'Me', 'debes', '50', '€', 'al', '50', '%', 'TAE', 'y', 'mi', 'dial', 'es', '#', '234', ',', 'y', 'además', 'eres', '\"', 'muy', '_', 'tonto', '\"', 'y', 'te', 'gusta', 'la', 'ñ', 'de', 'españa', '?', 'ni', 'los', 'pingüinos', '.']\n",
      "{'\"': 0, 'es': 1, '?': 2, 'ñ': 3, 'pingüinos': 4, '_': 5, 'Hola': 6, 'Me': 7, 'eres': 8, '¿': 9, '.': 10, 'debes': 11, 'además': 12, 'la': 13, '50': 14, 'dial': 15, '234': 16, 'TAE': 17, 'muy': 18, '#': 19, '!': 20, 'españa': 21, 'tonto': 22, 'te': 23, 'gusta': 24, 'de': 25, '€': 26, ',': 27, 'los': 28, 'al': 29, 'estás': 30, '¡': 31, 'y': 32, 'Cómo': 33, 'mi': 34, '%': 35, 'ni': 36}\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "\n",
    "vocab = set(tokens)\n",
    "word_to_id = {}\n",
    "\n",
    "for i, v in enumerate(vocab):\n",
    "    word_to_id[v] = i\n",
    "\n",
    "print(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0613fd6-c77a-45e7-8ec1-62858d109b33",
   "metadata": {},
   "source": [
    "##### Task:\n",
    "\n",
    "Now put everything together with our original dataset: normalization plus tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "325737d6-fa69-4ef2-a641-0062a58637ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ciencia': 0, 'podemos': 1, 'hacerlo': 2, 'el': 3, 'coche': 4, 'capa': 5, 'cosa': 6, 'numero': 7, 'sea': 8, 'que': 9, 'otro': 10, 'gatos': 11, 'resolver': 12, 'ejemplos': 13, 'recibe': 14, 'alta': 15, 'peatones': 16, 'mirar': 17, 'plegamiento': 18, 'sobre': 19, 'lineas': 20, '3': 21, 'situacion': 22, 'trucos': 23, 'le': 24, 'neuronales': 25, ')': 26, 'especificamente': 27, 'fotos': 28, 'encontrar': 29, 'parecido': 30, 'dia': 31, 'especializa': 32, 'al': 33, 'y': 34, 'red': 35, '\"': 36, 'juntar': 37, 'datos': 38, 'objetos': 39, 'objetivo': 40, 'sera': 41, 'humano': 42, 'estudiar': 43, 'rompecabezas': 44, 'suficientemente': 45, 'tipico': 46, 'grande': 47, 'entender': 48, 'conducen': 49, 'muy': 50, 'letra': 51, 'reducianlos': 52, 'horizontal': 53, 'se': 54, 'imagina': 55, 'vertical': 56, 'representan': 57, 'informacion': 58, 'alrededor': 59, 'redes': 60, 'verticales': 61, 'una': 62, 'mucho': 63, 'learning': 64, 'va': 65, 'practico': 66, 'usan': 67, 'final': 68, 'trata': 69, 'ajustando': 70, 'caso': 71, 'como': 72, 'cadena': 73, ':': 74, 'dio': 75, 'imagenes': 76, 'nuevo': 77, 'utilizan': 78, 'complejas': 79, 'adelante': 80, 'cebolla': 81, 'señales': 82, 'coches': 83, 'entrenamiento': 84, 'visual': 85, 'esos': 86, 'enseñar': 87, 'meterlos': 88, 'dos': 89, 'cantidad': 90, 'pasado': 91, 'algoritmos': 92, 'pero': 93, 'ajusta': 94, 'esquinas': 95, 'antes': 96, 'otras': 97, 'colores': 98, 'muchos': 99, 'finalmente': 100, 'profundas': 101, 'esas': 102, 'computo': 103, 'ahorrar': 104, 'bruto': 105, 'nuestro': 106, 'son': 107, 'reales': 108, 'optimizacion': 109, 'cometer': 110, 'capaz': 111, 'en': 112, 'habeis': 113, 'era': 114, 'sencillas': 115, 'reconocer': 116, 'capacidad': 117, 'lo': 118, 'asi': 119, 'conduccion': 120, 'digitos': 121, 'es': 122, 'reconocemos': 123, 'puntos': 124, 'por': 125, 'ademas': 126, 'aprender': 127, 'hacer': 128, 'encargarian': 129, 'especificos': 130, 'ordenadores': 131, 'cuando': 132, 'ya': 133, '(': 134, 'tesla': 135, 'semaforos': 136, 'partes': 137, 'utiles': 138, 'a': 139, 'artificiales': 140, 'avanzamos': 141, 'folding': 142, 'estas': 143, 'si': 144, 'numeros': 145, 'extrae': 146, 'para': 147, 'proteinas': 148, 'mejores': 149, 'perros': 150, 'tiene': 151, 'ahora': 152, 'ves': 153, 'maquina': 154, '9': 155, 'procesar': 156, '1': 157, 'deep': 158, 'concreta': 159, 'o': 160, 'trabajado': 161, 'actuales': 162, 'esta': 163, 'persona': 164, 'clasificacion': 165, 'clasificar': 166, 'vez': 167, '),': 168, 'correcta': 169, 'precisa': 170, 'montaje': 171, 'linea': 172, ').': 173, 'capaces': 174, 'autonoma': 175, 'este': 176, 'del': 177, 'seguridad': 178, 'inspiracion': 179, '8': 180, 'diferencias': 181, 'solo': 182, 'cosas': 183, 'mas': 184, 'solos': 185, 'ajustes': 186, 'trabajar': 187, '0': 188, 'vea': 189, 'rapido': 190, 'desarrollar': 191, 'las': 192, 'medicamentos': 193, 'donde': 194, 'podrian': 195, 'empieza': 196, 'seria': 197, 'cuantas': 198, 'machine': 199, 'area': 200, 'enfermedades': 201, 'hoy': 202, 'representa': 203, 'mano': 204, 'forma': 205, 'fue': 206, 'imagen': 207, 'funciona': 208, 'cara': 209, 'basicas': 210, 'protein': 211, 'computacion': 212, 'inspiradas': 213, 'cientificos': 214, 'combinando': 215, 'detectan': 216, 'vemos': 217, 'avances': 218, 'das': 219, 'antiguamente': 220, 'problemas': 221, 'rapidas': 222, 'errores': 223, 'decirte': 224, 'sus': 225, 'principio': 226, 'automatico': 227, 'compleja': 228, 'puede': 229, 'capas': 230, 'primeras': 231, '\",': 232, 'predecir': 233, 'audio': 234, 'la': 235, 'imitan': 236, 'dar': 237, 'rama': 238, 'algo': 239, 'acelerar': 240, 'hay': 241, 'identificar': 242, 'todo': 243, 'de': 244, 'construyendo': 245, 'deciden': 246, ',': 247, 'neuronal': 248, 'los': 249, 'aplicaciones': 250, 'con': 251, 'procesan': 252, 'juntas': 253, 'aprenda': 254, 'aprende': 255, 'tiempo': 256, 'clase': 257, 'entre': 258, 'tecnicas': 259, '.': 260, 'pasa': 261, 'aprendizaje': 262, 'tambien': 263, 'horizontales': 264, 'relacionados': 265, 'soluciones': 266, 'maquinas': 267, 'nueva': 268, 'procesamiento': 269, 'complejo': 270, 'mnist': 271, 'su': 272, 'respuesta': 273, 'voz': 274, 'pequeños': 275, 'formar': 276, 'vuelve': 277, 'tan': 278, 'ultimas': 279, 'haya': 280, 'pesos': 281, 'completas': 282, 'aprendera': 283, 'foto': 284, 'learningmachine': 285, 'eso': 286, '4': 287, 'trabajen': 288, 'biologia': 289, 'modelos': 290, 'diferenciarlos': 291, 'formas': 292, 'diferente': 293, 'clave': 294, 'curvas': 295, 'habia': 296, 'profundo': 297, 'estructura': 298, 'escritos': 299, 'no': 300, 'viendo': 301, 'proteina': 302, 'complejos': 303, 'pueden': 304, 'trabajo': 305, 'dato': 306, 'resolucion': 307, 'varias': 308, 'hace': 309, 'usaban': 310, 'muchas': 311, 'tareas': 312, 'hacen': 313, 'funcionan': 314, 'bordes': 315, 'podian': 316, 'especificas': 317, 'proceso': 318, 'hasta': 319, '):': 320, 'un': 321, 'suficientes': 322, 'evolucion': 323, 'simplificar': 324, 'podra': 325, 'potentes': 326, 'decir': 327, 'cerebro': 328, 'estan': 329, 'tardaban': 330, 'empiezan': 331, 'procesarla': 332, 'encargan': 333, 'simples': 334, 'frenar': 335, 'partir': 336, 'basadas': 337, 'problema': 338, 'medida': 339, 'cruzada': 340, 'rostros': 341, 'cada': 342, 'detecta': 343, 'esto': 344, 'circulo': 345, 'computadoras': 346, 'clasico': 347, 'conjunto': 348, 'luego': 349, 'usa': 350, 'otros': 351, 'geneticos': 352, 'tienes': 353, 'ejemplo': 354, 'detectar': 355}\n",
      "356\n"
     ]
    }
   ],
   "source": [
    "## make lower\n",
    "proc_data = raw_data.lower()\n",
    "\n",
    "## remove accents\n",
    "proc_data = unicodedata.normalize(\"NFD\", proc_data)\n",
    "proc_data = re.sub(r'[\\u0301\\u0300\\u0302]', '', proc_data)\n",
    "proc_data = unicodedata.normalize(\"NFC\", proc_data)\n",
    "\n",
    "## remove more than one separator into blank space\n",
    "proc_data = re.sub(r'\\s+', ' ', proc_data)\n",
    "\n",
    "## split into tokens\n",
    "_tokens = re.findall(r'_|[^_]+', proc_data)\n",
    "\n",
    "tokens = []\n",
    "for t in _tokens:\n",
    "    tokens.extend(re.findall(r'\\w+|[^\\w\\s]+',t))\n",
    "\n",
    "vocab = set(tokens)\n",
    "word_to_id = {}\n",
    "\n",
    "for i, v in enumerate(vocab):\n",
    "    word_to_id[v] = i\n",
    "\n",
    "print(word_to_id)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02a6af-ecab-405d-b832-5872c6382fb8",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "Let's wrap all these things in a tokenizer class that we can use later on. This is what you used in huggingface or whatever. Remember your tokenizer must be in charge of normalizing and tokenizing. We will also add some usefull methods to convert words to token ids and the opposite.  Note that, in creating this tokenizer you are basically asked to use the code you have been developing during this assement in the different methods proposed:\n",
    "\n",
    "The structure of your tokenizer must be:\n",
    "\n",
    "```python\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        ...\n",
    "    \n",
    "    def encode(self, data:str):\n",
    "        \"\"\"Convert raw data to corresponding token list\"\"\"\n",
    "        ...\n",
    "        return tokens\n",
    "\n",
    "    def batch_encode(self, data:list):\n",
    "        \"\"\"Convert a list containing raw data to a list containing their corresponding token lists.\"\"\"\n",
    "        ...\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens:list[int]):\n",
    "        \"\"\"Given a list of tokens, conver it to a string replacing tokens with text\"\"\"\n",
    "        ...\n",
    "        return text\n",
    "\n",
    "    def batch_decode(self, tokens : list[list[int]]):\n",
    "         \"\"\"Given a list containing list of tokens, convert it to a list containing strings replacing the tokens with text.\"\"\"\n",
    "        ...\n",
    "        return text\n",
    "        \n",
    "    def train_tokenizer(self, raw_data):\n",
    "        \"\"\"Given raw data create the token ids by applying the normalization step, pre tokenization step, training step and post tokenization\"\"\"\n",
    "        ...\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the vocabulary\"\"\"\n",
    "        return ...\n",
    "        \n",
    "    def _normalize(self, raw_data):\n",
    "        \"\"\"Applies normalization steps to raw data\"\"\"\n",
    "        ...\n",
    "        return proc_data\n",
    "\n",
    "    def _pre_tokenize(self, proc_data):\n",
    "        \"\"\"Apply pre tokenization step to normalized data\"\"\"\n",
    "        ...\n",
    "        return tokens\n",
    "```\n",
    "\n",
    "##### Task: Create tokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9d3ca49-cc1b-4b4d-98a9-9a1f03e521c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "class Tokenizer:\n",
       "    def __init__(self):\n",
       "        # regular expression to get tokens by words and punctuation signs.\n",
       "        self.re_get_tokens = re.compile(...)\n",
       "        # regular expression to replace string splitters such as blank space or new lines etc with whatever\n",
       "        self.re_replace_string_splitters = re.compile(...)\n",
       "        # regular expression to split by underscore \"_\"\n",
       "        self.split_by_underscore = re.compile(....)\n",
       "        # word to id dictionary\n",
       "        self.word_to_ids = one\n",
       "    \n",
       "    def encode(self, data:str):\n",
       "        \"Convert raw data to corresponding token list\"\n",
       "        assert self.word_to_ids is not None, \"Tokenizer has not been train. Call self.train_tokenizer\"\n",
       "\n",
       "        proc_data = ...\n",
       "        proc_data = ...\n",
       "\n",
       "        tokens = []\n",
       "        for token_text in proc_data:\n",
       "            tokens.append(...)\n",
       "\n",
       "        return tokens\n",
       "\n",
       "    def batch_encode(self, data:list):\n",
       "        \"Convert a list containing raw data to a list containing their corresponding token lists.\"\n",
       "        tokens = []\n",
       "        for d in data:\n",
       "            tokens.append(self.encode(d))\n",
       "\n",
       "        return tokens\n",
       "\n",
       "    def decode(self, tokens:list[int]):\n",
       "        \"Given a list of tokens, conver it to a string replacing tokens with text\"\n",
       "        assert self.word_to_ids is not None, \"Tokenizer has not been train. Call self.train_tokenizer\"\n",
       "        \n",
       "        text = ...\n",
       "        text = ' '.join(text)\n",
       "\n",
       "        return text\n",
       "\n",
       "    def batch_decode(self, tokens : list[list[int]]):\n",
       "        \"Given a list containing list of tokens, convert it to a list containing strings replacing the tokens with text.\"\n",
       "        text = []\n",
       "        for t in tokens:\n",
       "            text.append(self.decode(t))\n",
       "        \n",
       "        return text\n",
       "        \n",
       "    def train_tokenizer(self, raw_data):\n",
       "        \"Given raw data create the token ids by applying the normalization step, pre tokenization step, training step and post tokenization\"\n",
       "        print(\"Normalizing data...\")\n",
       "        proc_data = ...\n",
       "        \n",
       "        print(\"Pre tokenizing data...\")\n",
       "        proc_data = ...\n",
       "        \n",
       "        print(\"Building vocabulary...\")\n",
       "        self.vocab = ...\n",
       "        self.vocab_size = ...\n",
       "\n",
       "        # create mappings between words and ids and ids vs word for encoding and decoding\n",
       "        print(\"Building ids to word mappings...\")\n",
       "        self.word_to_ids = {}\n",
       "        self.ids_to_word = {}\n",
       "        for i, word in enumerate(self.vocab):\n",
       "            ...\n",
       "\n",
       "    def __len__(self):\n",
       "        \"Return the length of the vocabulary\"\n",
       "        return ...\n",
       "        \n",
       "    def _normalize(self, raw_data):\n",
       "        \"Applies normalization steps to raw data\"\n",
       "        ## make lower\n",
       "        proc_data = ...\n",
       "        \n",
       "        ## remove accents\n",
       "        proc_data = ...\n",
       "        proc_data = ...\n",
       "        proc_data = ...\n",
       "        \n",
       "        ## remove more than one separator into blank space\n",
       "        proc_data = ...\n",
       "\n",
       "        return ...\n",
       "\n",
       "    def _pre_tokenize(self, proc_data):\n",
       "        \"Apply pre tokenization step to normalized data\"\n",
       "\n",
       "        tokens = []\n",
       "        for t in ...:\n",
       "            tokens.extend(...)\n",
       "    \n",
       "        return tokens\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        # regular expression to get tokens by words and punctuation signs.\n",
    "        self.re_get_tokens = re.compile(...)\n",
    "        # regular expression to replace string splitters such as blank space or new lines etc with whatever\n",
    "        self.re_replace_string_splitters = re.compile(...)\n",
    "        # regular expression to split by underscore \"_\"\n",
    "        self.split_by_underscore = re.compile(....)\n",
    "        # word to id dictionary\n",
    "        self.word_to_ids = one\n",
    "    \n",
    "    def encode(self, data:str):\n",
    "        \"Convert raw data to corresponding token list\"\n",
    "        assert self.word_to_ids is not None, \"Tokenizer has not been train. Call self.train_tokenizer\"\n",
    "\n",
    "        proc_data = ...\n",
    "        proc_data = ...\n",
    "\n",
    "        tokens = []\n",
    "        for token_text in proc_data:\n",
    "            tokens.append(...)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def batch_encode(self, data:list):\n",
    "        \"Convert a list containing raw data to a list containing their corresponding token lists.\"\n",
    "        tokens = []\n",
    "        for d in data:\n",
    "            tokens.append(self.encode(d))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens:list[int]):\n",
    "        \"Given a list of tokens, conver it to a string replacing tokens with text\"\n",
    "        assert self.word_to_ids is not None, \"Tokenizer has not been train. Call self.train_tokenizer\"\n",
    "        \n",
    "        text = ...\n",
    "        text = ' '.join(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def batch_decode(self, tokens : list[list[int]]):\n",
    "        \"Given a list containing list of tokens, convert it to a list containing strings replacing the tokens with text.\"\n",
    "        text = []\n",
    "        for t in tokens:\n",
    "            text.append(self.decode(t))\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def train_tokenizer(self, raw_data):\n",
    "        \"Given raw data create the token ids by applying the normalization step, pre tokenization step, training step and post tokenization\"\n",
    "        print(\"Normalizing data...\")\n",
    "        proc_data = ...\n",
    "        \n",
    "        print(\"Pre tokenizing data...\")\n",
    "        proc_data = ...\n",
    "        \n",
    "        print(\"Building vocabulary...\")\n",
    "        self.vocab = ...\n",
    "        self.vocab_size = ...\n",
    "\n",
    "        # create mappings between words and ids and ids vs word for encoding and decoding\n",
    "        print(\"Building ids to word mappings...\")\n",
    "        self.word_to_ids = {}\n",
    "        self.ids_to_word = {}\n",
    "        for i, word in enumerate(self.vocab):\n",
    "            ...\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Return the length of the vocabulary\"\n",
    "        return ...\n",
    "        \n",
    "    def _normalize(self, raw_data):\n",
    "        \"Applies normalization steps to raw data\"\n",
    "        ## make lower\n",
    "        proc_data = ...\n",
    "        \n",
    "        ## remove accents\n",
    "        proc_data = ...\n",
    "        proc_data = ...\n",
    "        proc_data = ...\n",
    "        \n",
    "        ## remove more than one separator into blank space\n",
    "        proc_data = ...\n",
    "\n",
    "        return ...\n",
    "\n",
    "    def _pre_tokenize(self, proc_data):\n",
    "        \"Apply pre tokenization step to normalized data\"\n",
    "\n",
    "        tokens = []\n",
    "        for t in ...:\n",
    "            tokens.extend(...)\n",
    "    \n",
    "        return tokens\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13c4012a-3ac7-41d1-9698-8e9d096a351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        # regular expression to get tokens by words and punctuation signs.\n",
    "        self.re_get_tokens = re.compile(r'\\w+|[^\\w\\s]+')\n",
    "        # regular expression to replace string splitters such as \\t \\n etc with whatever\n",
    "        self.re_replace_string_splitters = re.compile(r'\\s+')\n",
    "        # regular expression to split by underscore \"_\"\n",
    "        self.split_by_underscore = re.compile(r'_+|[^_]+')\n",
    "        # word to id dictionary\n",
    "        self.word_to_ids = None\n",
    "    \n",
    "    def encode(self, data:str):\n",
    "        \"\"\"Convert raw data to corresponding token list\"\"\"\n",
    "        assert self.word_to_ids is not None, \"Tokenizer has not been train. Call self.train_tokenizer\"\n",
    "\n",
    "        proc_data = self._normalize(data)\n",
    "        proc_data = self._pre_tokenize(proc_data)\n",
    "\n",
    "        tokens = []\n",
    "        for token_text in proc_data:\n",
    "            tokens.append(self.word_to_ids[token_text])\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def batch_encode(self, data:list):\n",
    "        \"\"\"Convert a list containing raw data to a list containing their corresponding token lists.\"\"\"\n",
    "        tokens = []\n",
    "        for d in data:\n",
    "            tokens.append(self.encode(d))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: List[int]):\n",
    "        \"\"\"Given a list of tokens, conver it to a string replacing tokens with text\"\"\"\n",
    "        assert self.word_to_ids is not None, \"Tokenizer has not been train. Call self.train_tokenizer\"\n",
    "        \n",
    "        text = [self.ids_to_word[t] for t in tokens]\n",
    "        text = ' '.join(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def batch_decode(self, tokens : List[List[int]]):\n",
    "        \"\"\"Given a list containing list of tokens, convert it to a list containing strings replacing the tokens with text.\"\"\"\n",
    "        text = []\n",
    "        for t in tokens:\n",
    "            text.append(self.decode(t))\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def train_tokenizer(self, raw_data):\n",
    "        \"\"\"Given raw data create the token ids by applying the normalization step, pre tokenization step, training step and post tokenization\"\"\"\n",
    "        print(\"Normalizing data...\")\n",
    "        proc_data = self._normalize(raw_data)\n",
    "        \n",
    "        print(\"Pre tokenizing data...\")\n",
    "        proc_data = self._pre_tokenize(proc_data)\n",
    "        \n",
    "        print(\"Building vocabulary...\")\n",
    "        self.vocab = set(proc_data)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # create mappings between words and ids and ids vs word for encoding and decoding\n",
    "        print(\"Building ids to word mappings...\")\n",
    "        self.word_to_ids = {}\n",
    "        self.ids_to_word = {}\n",
    "        for i, word in enumerate(self.vocab):\n",
    "            self.word_to_ids[word] = i\n",
    "            self.ids_to_word[i] = word\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the vocabulary\"\"\"\n",
    "        return self.vocab_size\n",
    "        \n",
    "    def _normalize(self, raw_data):\n",
    "        \"\"\"Applies normalization steps to raw data\"\"\"\n",
    "        ## make lower\n",
    "        proc_data = raw_data.lower()\n",
    "        \n",
    "        ## remove accents\n",
    "        proc_data = unicodedata.normalize(\"NFD\", proc_data)\n",
    "        proc_data = re.sub(r'[\\u0301\\u0300\\u0302]', '', proc_data)\n",
    "        proc_data = unicodedata.normalize(\"NFC\", proc_data)\n",
    "        \n",
    "        ## remove more than one separator into blank space\n",
    "        proc_data = self.re_replace_string_splitters.sub(' ', proc_data)\n",
    "\n",
    "        return proc_data\n",
    "\n",
    "    def _pre_tokenize(self, proc_data):\n",
    "        \"\"\"Apply pre tokenization step to normalized data\"\"\"\n",
    "\n",
    "        tokens = []\n",
    "        for t in self.split_by_underscore.findall(proc_data):\n",
    "            tokens.extend(self.re_get_tokens.findall(t))\n",
    "    \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368627b-1754-45a2-8837-ef765af2c39a",
   "metadata": {},
   "source": [
    "##### Task: Traing your tokenizer on the read data and test it.\n",
    "\n",
    "* First read data\n",
    "* Second create an instance of the tokenizer class\n",
    "* Third call train method on readed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99c5d36e-d466-4880-a77e-42f35172c5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "## read data from source to train language model on \n",
       "raw_data = \"\"\n",
       "\n",
       "## reading dataset with PDF Plumber\n",
       "with pdfplumber.open(...) as pdf:\n",
       "    ...\n",
       "\n",
       "## define tokenizer implementing\n",
       "tokenizer = ...\n",
       "tokenizer.train_tokenizer(...)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if assesment_draw_and_fill:\n",
    "    code = \"\"\"\n",
    "```python\n",
    "## read data from source to train language model on \n",
    "raw_data = \"\"\n",
    "\n",
    "## reading dataset with PDF Plumber\n",
    "with pdfplumber.open(...) as pdf:\n",
    "    ...\n",
    "\n",
    "## define tokenizer implementing\n",
    "tokenizer = ...\n",
    "tokenizer.train_tokenizer(...)\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    display(Markdown(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51fef7df-b991-48f0-bab5-69c9838d1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing data...\n",
      "Pre tokenizing data...\n",
      "Building vocabulary...\n",
      "Building ids to word mappings...\n"
     ]
    }
   ],
   "source": [
    "## read data from source to train language model on \n",
    "raw_data = \"\"\n",
    "\n",
    "## reading dataset with PDF Plumber\n",
    "with pdfplumber.open('../data/texto_pdf.pdf') as pdf:\n",
    "    for page in pdf.pages:\n",
    "        raw_data += page.extract_text()\n",
    "\n",
    "## define tokenizer implementing\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.train_tokenizer(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50664d-c2a9-4b98-88a4-9fb344843eb9",
   "metadata": {},
   "source": [
    "##### Test your tokenizer in the following data.\n",
    "\n",
    "```python\n",
    "x = \"Las redes neuronales son machine learning.\"\n",
    "\n",
    "token_list = tokenizer.encode(x)\n",
    "\n",
    "print(x)\n",
    "print(token_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "797d6542-8e3c-4f8a-81ba-bea4bcafe27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las redes neuronales son machine learning.\n",
      "[192, 60, 25, 107, 199, 64, 260]\n"
     ]
    }
   ],
   "source": [
    "x = \"Las redes neuronales son machine learning.\"\n",
    "token_list = tokenizer.encode(x)\n",
    "\n",
    "print(x)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea19f5b-49db-40ce-bb28-23e3b485f5c5",
   "metadata": {},
   "source": [
    "Now detokenize the tokenized_list.\n",
    "\n",
    "```python\n",
    "x = tokenizer.decode(token_list)\n",
    "\n",
    "print(x)\n",
    "print(token_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "418d76f4-4787-4ec1-8766-e3348a17c461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "las redes neuronales son machine learning .\n",
      "[192, 60, 25, 107, 199, 64, 260]\n"
     ]
    }
   ],
   "source": [
    "x = tokenizer.decode(token_list)\n",
    "\n",
    "print(x)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc5444-115a-4475-bf79-3db129092053",
   "metadata": {},
   "source": [
    "Compare outputs. See how the point \".\" mark is given seperate to word `learning`. Why because we convert our list of tokens to list of corresponding words. \n",
    "\n",
    "As I mentioned, the final postprocessing step of a language application would be in charge of detecting these kind of things and remove the blankspace between the dot and the word. Modelling this probabilistically through a model is absurd, when is a python deterministic operation. Obviously, with the amount of data language models are trained on, they already know how to do this, because sub work tokenization include blank spaces. But without enough data there is no point in having the token `learning.` and `learning`. \n",
    "\n",
    "Again, this depend on the application. This specific example is for language generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0340871-75d6-4a70-940d-2ed0805ff690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415ff06-1b79-4b65-bca7-b9ea9e95b17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3348e33-69d6-4a9f-894b-3d107bd2aef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "GPFLOW",
   "language": "python",
   "name": "gpflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
