{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a631529-dccb-4592-bf0f-8e0e19eb6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8aeb96-522c-468e-96f7-16448f2c6618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f01bb876f50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## =============== ##\n",
    "## Define our data ##\n",
    "## =============== ##\n",
    "\n",
    "# input to our model. Represents time in seconds\n",
    "x_data = np.array([0,1,2]).reshape(3,1)\n",
    "# outputs associated to each input. Represents cantidad de lluvia in mm^3\n",
    "t_data = np.array([0.2,1.3,2.4]).reshape(3,1)\n",
    "\n",
    "## display\n",
    "plt.plot(x_data,t_data,'o', markersize = 8, label = 'data observations')\n",
    "plt.xlabel('tiempo')\n",
    "plt.ylabel('cantidad de lluvia')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c53fd02e-5b53-4d01-b40d-accc47c79bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======================================================= ##\n",
    "## ======== functionality for computational graph ======== ##\n",
    "## ======================================================= ##\n",
    "\n",
    "## function implementing an activation function\n",
    "def activation_function_linear(x):\n",
    "    return x\n",
    "\n",
    "## function that implements the computational graph\n",
    "def computation_graph_linear(x,w,b):\n",
    "    ''' This function represents a computational graph, a neural network, that implements a linear operation'''\n",
    "    # this is the W^0 x from the theory above implemented using a transposition ;)\n",
    "    y = activation_function_linear(np.matmul(x,w) + b)\n",
    "    return y\n",
    "\n",
    "## function that implements the computational graph\n",
    "def computation_graph_linear_just_weight(x,w):\n",
    "    ''' This function represents a computational graph, a neural network, that implements a linear operation, with no weight'''\n",
    "    # this is the W^0 x from the theory above implemented using a transposition ;)\n",
    "    y = activation_function_linear(np.matmul(x,w))\n",
    "    return y\n",
    "\n",
    "## function that initializes the values of a computational graph\n",
    "def create_computation_graph_linear(n_in,n_out):\n",
    "    ''' Create elements of the computational graph'''\n",
    "    # parameters\n",
    "    w = np.random.randn(n_in,n_out) + 1 # get a random value from standard normal distribution\n",
    "    b = np.random.randn(n_out,)*5 # get a random value from Gaussian with mean 0 and standard deviation 5.\n",
    "\n",
    "    return w,b\n",
    "\n",
    "## function implementing squared loss function\n",
    "def squared_loss_function_just_weight(x,t,w):\n",
    "    y_pred = activation_function_linear(np.matmul(x,w))\n",
    "    return (y_pred-t)**2\n",
    "\n",
    "def grad_squared_loss_just_weight(x,t,w):\n",
    "    # forward operation\n",
    "    y_pred = activation_function_linear(np.matmul(x,w))\n",
    "    \n",
    "    # backward operation (compute gradients / backpropagation / reverse mode autodiff)\n",
    "    grad_w = np.sum(2*(y_pred-t)*x, axis = 0, keepdims = True)\n",
    "    \n",
    "    return grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ed78ba-ebd0-410d-9503-3fcacb6919fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ====================================== ##\n",
    "## ========== Gradient Descent ========== ##\n",
    "## ====================================== ##\n",
    "\n",
    "## number of points in the domain used to plot the functions \n",
    "N_points_domain = 100\n",
    "x_range = np.linspace(-1,4, N_points_domain).reshape((N_points_domain,1))\n",
    "\n",
    "## specify our computational graph\n",
    "n_in = 1\n",
    "n_out = 1\n",
    "\n",
    "## first of all draw loss function against a set of parameters\n",
    "w_range = np.linspace(-10,15,500).reshape((500,n_in,n_out))\n",
    "\n",
    "loss_range = squared_loss_function_just_weight(x_data,t_data,w_range)\n",
    "\n",
    "## accumulate loss per datapoint\n",
    "loss_acc_range = np.sum(loss_range, axis = 1)\n",
    "\n",
    "## squeeze and display\n",
    "loss_acc_range = np.squeeze(loss_acc_range)\n",
    "w_range = np.squeeze(w_range)\n",
    "\n",
    "# plot grid\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 6))\n",
    "\n",
    "# display loss function\n",
    "ax1.plot(w_range, loss_acc_range, color = 'C0')\n",
    "ax1.set_xlabel('Weight')\n",
    "ax1.set_ylabel('Loss')\n",
    "\n",
    "# Get loss at initialization\n",
    "initial_w = np.array([9]).reshape(n_in,n_out)\n",
    "initial_loss = np.sum(squared_loss_function_just_weight(x_data,t_data,initial_w))\n",
    "\n",
    "# display loss\n",
    "ax1.plot(np.squeeze(initial_w), initial_loss, '*', color = 'C1')\n",
    "ax1.set_ylim([-100,900])\n",
    "\n",
    "# display data with initialized network\n",
    "initial_function = computation_graph_linear_just_weight(x_range, initial_w)\n",
    "\n",
    "ax2.plot(x_range,initial_function, color = 'C1')\n",
    "ax2.plot(x_data,t_data,'o', markersize = 8, label = 'data observations')\n",
    "ax2.text(1, 13, f\"squared loss = {initial_loss:.2f}\", fontsize=12, va='bottom', color = f\"C1\" ) \n",
    "ax2.set_xlabel('tiempo')\n",
    "ax2.set_ylabel('cantidad de lluvia')\n",
    "ax2.set_ylim([-20,10])\n",
    "\n",
    "fig.canvas.draw()\n",
    "fig.canvas.flush_events()\n",
    "time.sleep(0.5)\n",
    "\n",
    "## gradient descent parameters\n",
    "lr = 0.01 # try 0.1, 0.01, 0.15, 0.21 to show: fast convergence, slow convergence, convergence with bumping, divergence\n",
    "epochs = 100\n",
    "\n",
    "w = initial_w\n",
    "for i in range(epochs):\n",
    "\n",
    "    ## forward plus backward\n",
    "    grad_w = grad_squared_loss_just_weight(x_data,t_data,w)\n",
    "\n",
    "    ## plot new function and loss\n",
    "    function = computation_graph_linear_just_weight(x_range, w)\n",
    "    loss = np.sum(squared_loss_function_just_weight(x_data,t_data,w))\n",
    "\n",
    "    ## get the gradient function at the point w_old\n",
    "    gradient_function_w_at_current_w = grad_w * w_range + loss - grad_w * w\n",
    "\n",
    "    # Clear previous data\n",
    "    ax1.clear()  \n",
    "    ax2.clear()\n",
    "\n",
    "    ## display loss function\n",
    "    ax1.plot(w_range, loss_acc_range, color = 'C0', label = 'loss', zorder = 20)\n",
    "    ax1.plot(w_range, np.squeeze(gradient_function_w_at_current_w), color = 'C2', label = 'gradient function', zorder = 20)\n",
    "    ## display new weight after update\n",
    "    ax1.plot(np.squeeze(w), loss, '*', color = 'C1', label = 'current weight', zorder = 20)\n",
    "    ax1.text(np.squeeze(w) + 2, loss , f\"w = {np.squeeze(w):.2f} \\ngrad_w = {float(grad_w):.2f} \\nw_new = {np.squeeze(w):.2f} -{lr:.2f}*{np.squeeze(grad_w):.2f} = {np.squeeze(w-lr*grad_w):.2f}\", fontsize=12, va='bottom', color = f\"C3\" , zorder = 50) \n",
    "    \n",
    "    ax1.set_xlabel('Weight')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_ylim([-100,900])\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(x_range,function, color = 'C1')\n",
    "    ax2.plot(x_data,t_data,'o', markersize = 8, label = 'data observations')\n",
    "    ax2.text(1, 13, f\"squared loss = {loss:.2f}\", fontsize=12, va='bottom', color = f\"C1\" ) \n",
    "    ax2.set_xlabel('tiempo')\n",
    "    ax2.set_ylabel('cantidad de lluvia')\n",
    "    ax2.set_ylim([-20,10])\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(3)\n",
    "    ax1.vlines(np.squeeze(w), ymin=-100, ymax=loss, color='C3', linestyles='dotted')\n",
    "    time.sleep(3)\n",
    "    ax1.hlines(y = loss, xmin=np.squeeze(w-lr*grad_w), xmax=np.squeeze(w), color='C3', linestyles='dotted')\n",
    "    time.sleep(3)\n",
    "    ax1.vlines(np.squeeze(w-lr*grad_w), ymin=-100, ymax=loss, color='C3', linestyles='dotted')\n",
    "    time.sleep(3)\n",
    "    break\n",
    "\n",
    "    ## update parameter with gradient descent\n",
    "    w = w-lr*grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11adf6da-e445-4afd-94af-de6f4adaf3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPFLOW",
   "language": "python",
   "name": "gpflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
