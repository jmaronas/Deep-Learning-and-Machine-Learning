{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0606e0-ec11-40fa-b814-3b7671d6832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import kl_divergence, Normal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import multivariate_normal, gaussian_kde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e85ea-5948-4f11-bdfa-5ae4c418cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b909d-1f22-4817-8aa6-a1cedc55717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_connected_dataset():\n",
    "    N = 1000\n",
    "    \n",
    "    # Latente gaussiano\n",
    "    z1 = np.random.randn(N)\n",
    "    z2 = np.random.randn(N)\n",
    "    \n",
    "    # Transformaci칩n no lineal (banana)\n",
    "    x1 = z1**2\n",
    "    x2 = z2 + 0.3 * (z1 ** 4)\n",
    "    \n",
    "    X = np.column_stack([x1, x2])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5036fdd-2265-413d-ba0d-6bc90606619d",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "Let's generate data from $p(x)$ and also from the latent space $p(z)$. Here we use same dimensionality although we are not required to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0853ce-f6be-4840-ba01-393bd29524e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Crear figura con dos subplots (1 fila, 2 columnas)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# =========================\n",
    "# == Prior p(z) = N(0,I) ==\n",
    "# =========================\n",
    "\n",
    "# Generar muestras 2D de una normal est치ndar\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "samples = np.random.multivariate_normal(mean, cov, 500)\n",
    "\n",
    "# Scatter de las muestras\n",
    "axes[0].scatter(samples[:, 0], samples[:, 1], color = 'C1')\n",
    "\n",
    "# Crear grid para curvas de nivel\n",
    "x = np.linspace(-4, 4, 200)\n",
    "y = np.linspace(-4, 4, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "\n",
    "rv = multivariate_normal(mean, cov)\n",
    "Z = rv.pdf(pos)\n",
    "\n",
    "# Dibujar curvas de nivel\n",
    "axes[0].contour(X, Y, Z, levels=10, cmap = 'Oranges')\n",
    "\n",
    "axes[0].set_title(r\"$p({\\bf z})$\")\n",
    "axes[0].set_xlabel(r\"x_1\")\n",
    "axes[0].set_ylabel(r\"x_2\")\n",
    "\n",
    "# =========================\n",
    "# ======== p(x) ===========\n",
    "# =========================\n",
    "X_moons, y_moons = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
    "X_moons, y_moons = make_circles(n_samples=1000, noise=0.05, factor=0.5, random_state=42)\n",
    "X_moons = make_connected_dataset()\n",
    "\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], color = 'C0')\n",
    "axes[1].set_title(r\"$p({\\bf x})$\")\n",
    "axes[1].set_xlabel(r\"x_1\")\n",
    "axes[1].set_ylabel(r\"x_2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded37bb-9984-460c-b93d-965606c97966",
   "metadata": {},
   "source": [
    "#### Create Torch dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bd741-40e4-4a9c-b6f2-a4820e92662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_moons)\n",
    "\n",
    "X = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "dataset = TensorDataset(X)\n",
    "train_loader = DataLoader(dataset, batch_size=1000, shuffle=True)\n",
    "sampling_loader = DataLoader(dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14bfba-2e2e-45dc-883b-e5c87ed46a33",
   "metadata": {},
   "source": [
    "## Train a Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d33cc-7537-476d-b334-08041b6423a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder_layers, decoder_layers, latent_dim, decoder_type, N):\n",
    "        \"\"\"\n",
    "        encoder_layers: list og tuples (in_dim, out_dim, activation)\n",
    "        decoder_layers: list of tuples (in_dim, out_dim, activation)\n",
    "        activation is nn.Module class or None to specify a linear activation\n",
    "        decoder_type: to specify the observation model p(x|z). For the moment only Gaussian\n",
    "        is considered\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    " \n",
    "        self.latent_dim = latent_dim\n",
    "        self.decoder_type = decoder_type\n",
    "        # self.im_shape = im_shape\n",
    "        self.N = N\n",
    "\n",
    "        # standard normal prior\n",
    "        self.p_z = Normal(torch.zeros(latent_dim),torch.ones(latent_dim))\n",
    "\n",
    "        # for sampling\n",
    "        self.zero_tensor = torch.tensor(0.0)\n",
    "\n",
    "        # Definir encoder y decoder\n",
    "        self.encoder = self.build_encoder(encoder_layers)\n",
    "        self.decoder = self.build_decoder(decoder_layers)\n",
    "\n",
    "    def build_encoder(self, layers_config):\n",
    "        layers = []\n",
    "        for in_dim, out_dim, activation in layers_config[:-1]:\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if activation is not None:\n",
    "                layers.append(activation())\n",
    "                \n",
    "        self.enc = nn.Sequential(*layers)\n",
    "        in_dim, out_dim, activation = layers_config[-1]\n",
    "        self.enc_mean = nn.Sequential(*[nn.Linear(in_dim, out_dim)])\n",
    "        self.enc_logvar = nn.Sequential(*[nn.Linear(in_dim, out_dim),nn.Tanh()])\n",
    "    \n",
    "    def build_decoder(self, layers_config):\n",
    "    \n",
    "        layers = []\n",
    "        for in_dim, out_dim, activation in layers_config[:-1]:\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if activation is not None:\n",
    "                layers.append(activation())\n",
    "    \n",
    "        self.dec = nn.Sequential(*layers)\n",
    "        in_dim, out_dim, activation = layers_config[-1]\n",
    "        self.dec_mean = nn.Linear(in_dim, out_dim)\n",
    "        self.dec_logvar = nn.Sequential(*[nn.Linear(in_dim, out_dim),nn.Tanh()])\n",
    "\n",
    "    def encoder_forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        return self.enc_mean(z), 5*self.enc_logvar(z)\n",
    "\n",
    "    def decoder_forward(self, z):\n",
    "        x = self.dec(z)\n",
    "        return self.dec_mean(x), 5*self.dec_logvar(x)\n",
    "\n",
    "    def sample_gaussian(self, mean, logvar, return_mean=False):\n",
    "        if return_mean:\n",
    "            return mean\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def ELBO(self, x, mc_samples=1, kld_scale=1.0):\n",
    "        M = x.size(0)\n",
    "\n",
    "        # Encoder\n",
    "        q_mean, q_logvar = self.encoder_forward(x)\n",
    "\n",
    "        # Encoder distribution\n",
    "        q_zx = Normal(q_mean, torch.exp(0.5*q_logvar))\n",
    "\n",
    "        # DKL\n",
    "        KLD = kld_scale * kl_divergence(q_zx,self.p_z)\n",
    "\n",
    "        # sum over each training point\n",
    "        KLD = KLD.sum()\n",
    "        \n",
    "        # Log Likelihood using Monte Carlo. We could vectorize mc_sampling but for academic\n",
    "        # purposes this is better. Also vectorizing requires taking care of memory.\n",
    "        LLH = torch.tensor(0.0)\n",
    "        for _ in range(mc_samples):\n",
    "            z = self.sample_gaussian(q_mean, q_logvar)\n",
    "            dec_mean, dec_logvar = self.decoder_forward(z)\n",
    "\n",
    "            # Sum over training points and over dimensions\n",
    "            LLH += Normal(dec_mean, torch.exp(0.5*dec_logvar)).log_prob(x).sum()\n",
    "\n",
    "        ## Monte Carlo Estimation\n",
    "        LLH /= mc_samples\n",
    "\n",
    "        ## ELBO\n",
    "        ELBO = LLH - KLD\n",
    "\n",
    "        ## Renormalize minibatching for appropidate scale\n",
    "        ELBO *= self.N / M\n",
    "\n",
    "        return ELBO, LLH, KLD\n",
    "\n",
    "    def sample_from_prior(self, n_samples, return_mean=False):\n",
    "        \"\"\"Sample from prior via ancestral sampling\"\"\"\n",
    "        z = self.sample_gaussian(self.zero_tensor.expand(n_samples, self.latent_dim),\n",
    "                               self.zero_tensor.expand(n_samples, self.latent_dim),\n",
    "                               return_mean=False)\n",
    "        \n",
    "        mean, logvar = self.decoder_forward(z)\n",
    "        return z, self.sample_gaussian(mean, logvar, return_mean=return_mean)\n",
    "\n",
    "    def sample_from_posterior(self, x, return_mean=False):\n",
    "        \"\"\"Sample from posterior distribution q(z|x).\"\"\"\n",
    "        mean, logvar = self.encoder_forward(x)\n",
    "        z = self.sample_gaussian(mean, logvar, return_mean=False)\n",
    "        dec_mean, dec_logvar = self.decoder_forward(z)\n",
    "        return self.sample_gaussian(dec_mean, dec_logvar, return_mean=return_mean)\n",
    "\n",
    "    def run_mcmc(self, x, num_steps, n_chains=1, return_mean=False):\n",
    "        x_chain = []\n",
    "    \n",
    "        batch_size, x_dim = x.shape\n",
    "        # expand x to [batch_size, n_chains, x_dim] then flatten to [batch_size*n_chains, x_dim]\n",
    "        x_t = x.unsqueeze(1).repeat(1, n_chains, 1).view(batch_size * n_chains, x_dim)\n",
    "        x_chain.append(x_t)\n",
    "        z_chain = []\n",
    "    \n",
    "        for _ in range(num_steps):\n",
    "            # z_t ~ q(z|x_t)\n",
    "            q_mean, q_logvar = self.encoder_forward(x_t)\n",
    "            z_t = self.sample_gaussian(q_mean, q_logvar)\n",
    "    \n",
    "            # x_{t+1} ~ p(x|z_t)\n",
    "            dec_mean, dec_logvar = self.decoder_forward(z_t)\n",
    "            x_t = self.sample_gaussian(dec_mean, dec_logvar, return_mean=return_mean)\n",
    "    \n",
    "            x_chain.append(x_t)\n",
    "            z_chain.append(z_t)\n",
    "    \n",
    "        # stack and reshape to [num_steps, batch_size, n_chains, z_dim] / [num_steps+1, batch_size, n_chains, x_dim]\n",
    "        z_chain = torch.stack(z_chain, dim=0).view(num_steps, batch_size, n_chains, -1)\n",
    "        x_chain = torch.stack(x_chain, dim=0).view(num_steps+1, batch_size, n_chains, -1)\n",
    "\n",
    "        # reshape to [batch_size, n_chains, num_steps, dim] without permute\n",
    "        z_chain = z_chain.transpose(0,1).contiguous().view(batch_size, n_chains, num_steps, -1)\n",
    "        x_chain = x_chain.transpose(0,1).contiguous().view(batch_size, n_chains, num_steps+1, -1)\n",
    "\n",
    "        return z_chain, x_chain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0bbe1c-9bae-4786-aec2-65729d1f6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = 2\n",
    "latent_dim = 2\n",
    "hidden_dim = 64\n",
    "\n",
    "encoder_layers = [\n",
    "    (data_dim, hidden_dim, nn.Tanh),\n",
    "    (hidden_dim, hidden_dim, nn.Tanh),\n",
    "    (hidden_dim, latent_dim, None)\n",
    "]\n",
    "decoder_layers = [\n",
    "    (latent_dim, hidden_dim, nn.Tanh),\n",
    "    (hidden_dim, hidden_dim, nn.Tanh),\n",
    "    (hidden_dim, data_dim, None)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8758a28-9911-4121-be93-e43d7024741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(\n",
    "            encoder_layers,\n",
    "            decoder_layers,\n",
    "            latent_dim = latent_dim,\n",
    "            decoder_type = \"Gaussian\",\n",
    "            N = X_moons.shape[0],\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    total_elbo = 0\n",
    "    total_kld = 0.0\n",
    "    total_ell = 0.0\n",
    "    for x_tr, in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        elbo, ell, kld = vae.ELBO(x_tr, mc_samples = 1, kld_scale = 1)\n",
    "        loss = -elbo \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_elbo += elbo.item() \n",
    "        total_ell += ell.item()\n",
    "        total_kld += kld.item()\n",
    "\n",
    "    if epoch == 200: # annealing\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1e-3\n",
    "\n",
    "    if epoch == 1200: # annealing\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1e-4\n",
    "        \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}, ELBO: {total_elbo:.4f}, ELL:{total_ell:.4f}, KLD: {total_kld:.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11087da-b8b0-4997-9a52-4a74d589477a",
   "metadata": {},
   "source": [
    "### Generating Samples from the prior p(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe02cd-4627-44ff-addb-48fa47a6b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "try:\n",
    "    plt.close(\"all\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Crear figura con dos filas y dos columnas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# =========================\n",
    "# == Prior p(z) = N(0,I) ==\n",
    "# =========================\n",
    "\n",
    "# Generar muestras 2D de una normal est치ndar\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "samples = np.random.multivariate_normal(mean, cov, 500)\n",
    "\n",
    "# Scatter de las muestras\n",
    "axes[0, 0].scatter(samples[:, 0], samples[:, 1], color='C1')\n",
    "\n",
    "# Crear grid para curvas de nivel\n",
    "x = np.linspace(-4, 4, 200)\n",
    "y = np.linspace(-4, 4, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "\n",
    "rv = multivariate_normal(mean, cov)\n",
    "Z = rv.pdf(pos)\n",
    "\n",
    "# Dibujar curvas de nivel\n",
    "axes[0, 0].contour(X, Y, Z, levels=10, cmap='Oranges')\n",
    "\n",
    "axes[0, 0].set_title(r\"$p({\\bf z})$\")\n",
    "axes[0, 0].set_xlabel(r\"$x_1$\")\n",
    "axes[0, 0].set_ylabel(r\"$x_2$\")\n",
    "\n",
    "# =========================\n",
    "# ======== p(x) ===========\n",
    "# =========================\n",
    "axes[0, 1].scatter(X_moons[:, 0], X_moons[:, 1], color='C0')\n",
    "axes[0, 1].set_title(r\"$p({\\bf x})$\")\n",
    "axes[0, 1].set_xlabel(r\"$x_1$\")\n",
    "axes[0, 1].set_ylabel(r\"$x_2$\")\n",
    "\n",
    "# ================================================\n",
    "# == Sample from p(x,z) through ancestral sampling\n",
    "# z ~ p(z)\n",
    "# x ~ p(x|z)\n",
    "# ================================================\n",
    "torch.manual_seed(10)\n",
    "n_samples = 500\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    z, x_z = vae.sample_from_prior(n_samples)\n",
    "\n",
    "    # come back to original scale.\n",
    "    x_z = scaler.inverse_transform(x_z.numpy())\n",
    "\n",
    "\n",
    "axes[1, 0].plot(z[:, 0], z[:, 1],'x', color='C1', alpha = 0.1)        \n",
    "axes[1, 0].contour(X, Y, Z, levels=10, cmap='Oranges')\n",
    "axes[1, 0].set_title(r\"Sample from $p({\\bf z})$\")\n",
    "axes[1, 0].set_xlabel(r\"$z_1$\")\n",
    "axes[1, 0].set_ylabel(r\"$z_2$\")\n",
    "\n",
    "\n",
    "axes[1, 1].plot(x_z[:, 0], x_z[:, 1],'x', color='C0', alpha = 0.1)\n",
    "axes[1, 1].set_title(r\"Sample from $p({\\bf x}|{\\bf z})$\")\n",
    "axes[1, 1].set_xlabel(r\"$x_1$\")\n",
    "axes[1, 1].set_ylabel(r\"$x_2$\")\n",
    "\n",
    "for s in range(10):\n",
    "    plot_z, = axes[1, 0].plot(z[s, 0], z[s, 1], 'o', color='C1', markersize = 10)    \n",
    "    \n",
    "    plt.pause(0.5)\n",
    "    \n",
    "    plot_x_z, = axes[1, 1].plot(x_z[s, 0], x_z[s, 1],'o', color='C0', markersize = 10)\n",
    "\n",
    "    plt.pause(0.5)\n",
    "\n",
    "    plot_z.remove()\n",
    "    plot_x_z.remove()\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa076f56-666c-4f17-88f8-1142ee6edb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.close(\"all\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71cf3c2-95a7-4adf-af45-4e86f01a21eb",
   "metadata": {},
   "source": [
    "### Generating Samples from the model using approximate MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d0427-cc0b-42fb-9eaa-b47c5fb66c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "try:\n",
    "    plt.close(\"all\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "\n",
    "# Crear figura con dos filas y dos columnas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# =========================\n",
    "# == Prior p(z) = N(0,I) ==\n",
    "# =========================\n",
    "\n",
    "# Generar muestras 2D de una normal est치ndar\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "samples = np.random.multivariate_normal(mean, cov, 500)\n",
    "\n",
    "# Scatter de las muestras\n",
    "axes[0, 0].plot(samples[:, 0], samples[:, 1],'x', color='C1', alpha = 0.1)\n",
    "\n",
    "# Crear grid para curvas de nivel\n",
    "x = np.linspace(-4, 4, 200)\n",
    "y = np.linspace(-4, 4, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "pos = np.dstack((X, Y))\n",
    "\n",
    "rv = multivariate_normal(mean, cov)\n",
    "Z = rv.pdf(pos)\n",
    "\n",
    "# Dibujar curvas de nivel\n",
    "axes[0, 0].contour(X, Y, Z, levels=10, cmap='Oranges')\n",
    "\n",
    "axes[0, 0].set_title(r\"$p({\\bf z})$\")\n",
    "axes[0, 0].set_xlabel(r\"$z_1$\")\n",
    "axes[0, 0].set_ylabel(r\"$z_2$\")\n",
    "\n",
    "# =========================\n",
    "# ======== p(x) ===========\n",
    "# =========================\n",
    "axes[0, 1].plot(X_moons[:, 0], X_moons[:, 1],'x', color='C0', alpha = 0.1)\n",
    "axes[0, 1].set_title(r\"$p({\\bf x})$\")\n",
    "axes[0, 1].set_xlabel(r\"$x_1$\")\n",
    "axes[0, 1].set_ylabel(r\"$x_2$\")\n",
    "\n",
    "# ================================================\n",
    "# == Sample from p(x,z) through ancestral sampling\n",
    "# z ~ p(z)\n",
    "# x ~ p(x|z)\n",
    "# ================================================\n",
    "torch.manual_seed(1)\n",
    "num_steps = 20\n",
    "n_chains = 3\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    for x, in sampling_loader:\n",
    "        # add the point(10,30), normalized, which looks nice\n",
    "        x_n = torch.tensor(scaler.transform(np.array([[10.,30.]])), dtype = torch.float)\n",
    "        x = torch.cat([x_n, x], dim=0)\n",
    "        \n",
    "        z_x, x_z = vae.run_mcmc(\n",
    "            x = x, \n",
    "            num_steps = num_steps,\n",
    "            n_chains = n_chains,\n",
    "            return_mean = True\n",
    "        )\n",
    "    # print(z_x.shape) #  [batch_size, n_chains, num_steps, dim] \n",
    "    # print(x_z.shape)\n",
    "\n",
    "## Estimate a Kernel density estimator on q(z|x) to grab idea on what is the posterior\n",
    "#  looking. Get final half of the chain.\n",
    "kde = gaussian_kde(z_x[:,:,int(num_steps/2):,:].contiguous().view(-1,data_dim).numpy().T)\n",
    "\n",
    "# come back to original scale.\n",
    "x_z_flat = x_z.view(-1, x_z.shape[-1])\n",
    "x_z_orig = scaler.inverse_transform(x_z_flat)\n",
    "x_z = torch.tensor(x_z_orig).view(x_z.shape)\n",
    "\n",
    "## Plot kde level curves\n",
    "Z = kde(pos.reshape(-1, 2).T).reshape(X.shape)\n",
    "axes[1,0].contour(X, Y, Z, levels=20,  cmap='Oranges')\n",
    "\n",
    "## plot all. Pick half of the chain\n",
    "z_x_plot = z_x[:,:,int(num_steps/2):,:].contiguous().view(-1,data_dim).numpy()\n",
    "x_z_plot = x_z[:,:,int(num_steps/2):,:].contiguous().view(-1,data_dim).numpy()\n",
    "\n",
    "axes[1, 0].plot(z_x_plot[:, 0], z_x_plot[:, 1],'x', color='C1', alpha = 0.2)  \n",
    "axes[1, 0].set_xlim([-4,4])\n",
    "axes[1, 0].set_ylim([-4,4])\n",
    "axes[1, 0].set_title(r\"Sample from $q({\\bf z} \\mid {\\bf x})$\")\n",
    "axes[1, 0].set_xlabel(r\"$z_1$\")\n",
    "axes[1, 0].set_ylabel(r\"$z_2$\")\n",
    "\n",
    "axes[1, 1].plot(x_z_plot[:, 0], x_z_plot[:, 1],'x', color='C0', alpha = 0.2)\n",
    "axes[1, 1].set_title(r\"Sample from $p({\\bf x}|{\\bf z})$\")\n",
    "axes[1, 1].set_xlabel(r\"$x_1$\")\n",
    "axes[1, 1].set_ylabel(r\"$x_2$\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Animate MCMC trajectories \n",
    "# =========================\n",
    "batch_size, n_chains, num_steps, data_dim = x_z.shape\n",
    "\n",
    "# Initialize trajectory lists and plot handles\n",
    "plot_handles = [[None for _ in range(n_chains)] for _ in range(batch_size)]\n",
    "\n",
    "# Loop over MCMC steps\n",
    "for b in range(batch_size):\n",
    "    for c in range(n_chains):\n",
    "        traj_x_x = []\n",
    "        traj_x_y = []\n",
    "        traj_z_x = []\n",
    "        traj_z_y = []\n",
    "        plot_handle_x = None\n",
    "\n",
    "        # initial samples where the chain is started\n",
    "        init_x_x = x_z[b, c, 0, 0].item() # traj_x_x.append()\n",
    "        init_x_y = x_z[b, c, 0, 1].item() #traj_x_y.append()\n",
    "\n",
    "        # highlight the initial samples of the chain\n",
    "        init_chain, = axes[0,1].plot(init_x_x, init_x_y, 'x', color = 'black', markersize=8)\n",
    "\n",
    "        traj_x_x.append(init_x_x)\n",
    "        traj_x_y.append(init_x_y)\n",
    "        \n",
    "        for t in range(num_steps-1):\n",
    "            # append current point to trajectory\n",
    "            traj_x_x.append(x_z[b, c, t+1, 0].item())\n",
    "            traj_x_y.append(x_z[b, c, t+1, 1].item())\n",
    "\n",
    "            traj_z_x.append(z_x[b, c, t, 0].item())\n",
    "            traj_z_y.append(z_x[b, c, t, 1].item())\n",
    "\n",
    "            if plot_handle_x is not None:\n",
    "                plot_handle_z.remove()\n",
    "            \n",
    "            # plot trajectory for this chain of this batch element\n",
    "            plot_handle_z, = axes[1,0].plot(traj_z_x, traj_z_y,\n",
    "                                          '--x', markersize=4, color='black', alpha=0.5, zorder = 10)\n",
    "            plt.pause(0.5)\n",
    "\n",
    "            if plot_handle_x is not None:\n",
    "                plot_handle_x.remove()\n",
    "            \n",
    "            plot_handle_x, = axes[1,1].plot(traj_x_x, traj_x_y,\n",
    "                                          '--x', markersize=4, color='black', alpha=0.5, zorder = 10)\n",
    "\n",
    "            plt.pause(0.5)  # pause to animate each step\n",
    "\n",
    "        # remove previous line\n",
    "        plot_handle_x.remove()\n",
    "        plot_handle_z.remove()\n",
    "        init_chain.remove()\n",
    "        \n",
    "        # highlight final element in the chain\n",
    "        axes[1, 1].plot(traj_x_x[-1], traj_x_y[-1],'o', markersize = 8, color='C0')\n",
    "        axes[1, 0].plot(traj_z_x[-1], traj_z_y[-1],'o', markersize = 8, color='C1')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cca1dd-28da-4757-bdea-b39fed288e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.close(\"all\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
