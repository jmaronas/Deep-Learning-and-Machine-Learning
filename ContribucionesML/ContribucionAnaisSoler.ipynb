{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f7bfa6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "# **BLOCK 1: Linear and Logistic Regression**\n",
    "\n",
    "### **Block Objective**\n",
    "Based on a data set $(x_n, t_n)$ a supervised learning task is proposed with the objective of building a model capable of predicting the output based on the input.\n",
    "<br>\n",
    "\n",
    "#### The general process consists of:\n",
    "\n",
    "1. Identifying the type of problem based on the data **(1.1)**\n",
    "2. Choosing an appropriate model type **(1.2)**\n",
    "3. Defining a cost function that allows for comparison of different models **(1.3)**\n",
    "4. Optimizing the model parameters to obtain the best possible fit to the data **(1.5)**\n",
    "5. **Extra:** Alternative optimization using Newton's method **(1.6)**\n",
    "\n",
    "$\\text{data → model choice → many possible models → criteria for comparing them → finding the optimum}$ <br><br>\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa59d7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "## **1.1 Problem Contextualization**\n",
    "**Objective:** To determine the nature of the problem based on the type of target variable.\n",
    "\n",
    "It answers the question: What type of problem do I have and what type of output do I want to predict?\n",
    "\n",
    "Each data point is represented as a pair $(x_n, t_n)$ where:\n",
    "\n",
    "- $x_n$ is the input (observed variables)\n",
    "- $t_n$ is the actual output or target variable\n",
    "\n",
    "Depending on the type of target variable, two types of tasks are distinguished:\n",
    "\n",
    "### **Output Type $t$**\n",
    "- **Continuous Output** ($t \\in \\mathbb{R}$)\n",
    "\n",
    "  The goal is to predict a numerical value\n",
    "  \n",
    "  e.g., height based on age, house price based on square meters, weight based on height\n",
    "\n",
    "- **Binary Output** ($t \\in \\{0,1\\}$)\n",
    "\n",
    "  The goal is to decide between two possible classes → Classification\n",
    "  \n",
    "  e.g., pass/fail, healthy/sick patient, yes/no, smoker/non-smoker\n",
    "\n",
    "The analysis of the output type determines the formulation of the problem, but does not yet establish the specific model, as this will be defined in the following section.\n",
    "\n",
    "<div style=\"border:2px solid black; padding:15px; text-align:center; margin:20px 0;\">\n",
    "Based on the analysis carried out in this section, a mathematical model is selected that is suitable for representing the relationship between inputs and output (1.2).\n",
    "</div> <br>\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99951447",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "## **1.2 Regression Models: Hypotheses and Parameterization**\n",
    "### **Types of Models** \n",
    "Defined by its nature.<br><br>\n",
    "\n",
    "#### **1.2.1 Linear Regression**\n",
    "When the target variable $t$ is continuous, it is assumed that there is a linear relationship between the input $x$ and the output $t$ . The model prediction is obtained directly from a linear combination of the input.\n",
    "\n",
    "The model is expressed as:\n",
    "$$\n",
    "y = wx + b \\quad \\text{with }  w, b \\in \\mathbb{R}\n",
    "$$\n",
    "where:\n",
    "\n",
    "- $w$ → weight or slope of the line\n",
    "- $b$ → intercept ($y$ when $x=0$)\n",
    "- $y$ → model prediction\n",
    "\n",
    "**note:** $w$ and $b$ are the model parameters\n",
    "\n",
    "Each pair $(w,b)$ defines a line in the plane $(x,t)$ , which represents a candidate model. These parameters are common to all data, describing a global line, while what changes in each observation is the input value $x_n$ . In general, there are infinitely many candidate $(w,b)$. \n",
    "\n",
    "Given a dataset with more than one point, multiple possible lines can be drawn, each corresponding to a different pair $(w,b)$ . Intuitively, some lines will fit the observed data better than others. This graphical intuition motivates the need to define a quantitative measure that allows lines to be compared and determines which one is the best, leading to the introduction of the cost function (1.3).\n",
    "\n",
    "If the data exhibit a perfect linear relationship, there exists a line that passes through all the points, which is therefore the best possible model.\n",
    "\n",
    "However, in real-world situations the relationship between variables is not perfectly linear, so there is no line that passes through all the points. In this case, it is necessary to introduce a criterion that allows different models to be compared and to select the one that best fits the data (cost function).\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto1.png\" width=\"50%\">\n",
    "</div>\n",
    "\n",
    "**Limit case:** if there is only a single data point, there are infinitely many lines that pass through that point, so the model is not uniquely determined. This shows that the information is insufficient to identify an optimal model without additional criteria. (Note: all of them are valid because $(e = 0)$)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto2.png\" width=\"50%\">\n",
    "</div><br><br>\n",
    "\n",
    "**Illustrative example:** Given a set of points, graphically represent three candidate lines and visually compare which one appears to fit the data best. Each line represents a possible model.\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto3.png\" width=\"50%\">\n",
    "</div><br><br>\n",
    "\n",
    "**Keep in mind !** In multiple linear regression, the prediction depends on several inputs:\n",
    "$$\n",
    "y = w_1 x_1 + w_2 x_2 + \\cdots + w_k x_k + b\n",
    "$$\n",
    "This increases the model parameters from $2$ to $k+1$ . <br><br><br>\n",
    "\n",
    "#### **1.2.1 Logistic Regression**\n",
    "When the target variable $t$ is binary, the linear combination is not interpreted directly as a prediction, but as an intermediate variable that is transformed through a sigmoid activation function to obtain a probability.\n",
    "\n",
    "The model starts with a linear combination of the inputs:\n",
    "$$\n",
    "z = wx + b \\quad \\text{with } w, b \\in \\mathbb{R}\n",
    "$$\n",
    "and transforms it using the sigmoid function:\n",
    "$$y = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "<br>\n",
    "\n",
    "<div style=\"margin-left: 2em;\">\n",
    "\n",
    "**Clarification:**  \n",
    "As this is a binary classification problem: \n",
    "$$\n",
    "y = P(t = 1 \\mid x)\n",
    "$$\n",
    "Thus, the model does not directly predict the class, but rather a probability of belonging to class $1$ (event occurring/positive result).\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "**Decision boundary**\n",
    "To finally assign a class, a decision boundary is defined, which is usually set at 0.5 :\n",
    "- If $y \\ge 0.5$, the model assigns clas $1$\n",
    "- If $y < 0.5$, the model assigns clas $0$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;**Illustrative graphic:**\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto4.png\" width=\"50%\">\n",
    "</div><br><br>\n",
    "\n",
    "**Example:** \n",
    "If $P(t = 1 \\mid x)=0.8$, the prediction is class $1$.<br>\n",
    "If $P(t = 1 \\mid x)=0.4$, the prediction is class $0$.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Important !**  Defining the decision boundary allows the predicted probability to be transformed into a specific class. The following section (1.3) analyzes how the model parameters are adjusted using a cost function that penalizes classification errors.  \n",
    "\n",
    "**Model parameters**\n",
    "\n",
    "The parameters  $w$ and $b$ completely define the model and are common to all data, as in linear regression.\n",
    "\n",
    "Logistic regression is linear in the parameters, but the output is nonlinear, thanks to the sigmoid activation function. This allows the prediction  $y$ to be interpreted as a probability.\n",
    "<br>\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea3b90",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "## **1.3 Cost function: what it is and what it is used for**\n",
    "<div style=\"border:2px solid black; padding:15px; text-align:center; margin:20px 0;\">\n",
    "Once the model type has been chosen (1.2), there are multiple possible parameter configurations that define different candidate models. To compare which one best fits the data, the cost function is used, which quantifies the discrepancy between the model predictions and the actual values and allows the most appropriate model to be selected.\n",
    "</div> <br>\n",
    "\n",
    "### **Individual loss and total cost**\n",
    "For each data point $n$ , an individual loss function is defined:\n",
    "$$\n",
    "l(t_n, y_n)\n",
    "$$\n",
    "which measures the error made by the model when predicting $y_n$ when the actual value is $t_n$ . This function must be optimizable.\n",
    "\n",
    "**Important !** The error (or residual) is the difference between the predicted value and the actual value. It can be positive (overestimation) or negative (underestimation).\n",
    "\n",
    "\n",
    "### **Total cost function**\n",
    "This is defined as the sum of the individual losses $\\equiv \\text{TOTAL loss}$:\n",
    "$$\n",
    "L(w,b) = \\sum_{n=1}^{N} l(t_n, y_n)\n",
    "$$\n",
    "Therefore, the total cost function converts the learning problem into a quantifiable and optimizable problem.\n",
    "\n",
    "<div style=\"margin-left: 3em;\">\n",
    "\n",
    "**Interpretation of the cost function**  \n",
    "Establishes a criterion of discrepancy or suitability for evaluating the model in relation to the task or question. It measures how close or far the model's predictions are from the actual values.\n",
    "\n",
    "$\\text{The lower the cost function, the better the model, because it will be closer and better adjusted to the actual data}$\n",
    "</div>\n",
    "\n",
    "**Important !** It does not have to be strictly “distance”: it can be any measure\n",
    "that adequately captures the error made by the model.\n",
    "\n",
    "### **Types of cost functions**\n",
    "#### **Linear regression :**\n",
    "**[Common examples]**\n",
    "\n",
    "- **Cuadratic** error:  $$L(w,b)=\\sum_{n=1}^{N}(t_n-y_n)^2 = \\sum_{n=1}^{N}\\big(t_n-(wx_n+b)\\big)^2$$\n",
    "\n",
    "<div style=\"margin-left: 3em;\">\n",
    "  <strong>Motivation:</strong>\n",
    "  <ul style=\"margin-left: 0;\">\n",
    "    <li>Symmetry: treats positive and negative errors equally</li>\n",
    "    <li>Amplification of major errors: the greater the discrepancy, the greater the penalty</li>\n",
    "    <li>Smooth optimization: It is differentiable and convex → guarantees a global minimum</li>\n",
    "  </ul>\n",
    "</div> <br>\n",
    "\n",
    "- **Cubic** error: $$L(w,b)=\\sum_{n=1}^{N}(t_n-y_n)^3 = \\sum_{n=1}^{N}\\big(t_n-(wx_n+b)\\big)^3$$\n",
    "\n",
    "<div style=\"margin-left: 3em;\">\n",
    "  <strong>Motivation:</strong>\n",
    "  <ul style=\"margin-left: 0;\">\n",
    "    <li>Asymmetry: It penalizes positive and negative errors differently. This is because positive errors cause the cost function to increase and negative errors cause it to decrease.</li>\n",
    "    → Result: the model could focus on reducing overestimations, neglecting underestimations → mismatch</li>\n",
    "    <li>Error amplification: Large errors are amplified even further, while small errors carry less weight.</li>\n",
    "  </ul>\n",
    "</div> <br>\n",
    "\n",
    "- Errors with **higher exponents**: $$L(w,b)=\\sum_{n=1}^{N}(t_n-y_n)^k = \\sum_{n=1}^{N}\\big(t_n-(wx_n+b)\\big)^k$$\n",
    "\n",
    "<div style=\"margin-left: 3em;\">\n",
    "  <strong>Motivation:</strong>\n",
    "  <ul style=\"margin-left: 0;\">\n",
    "    <li> k = pair → symmetry</li>\n",
    "    <li> k = odd → asymmetry</li>\n",
    "    <li>The higher k is, the more big errors are amplified and small ones are minimized.</li>\n",
    "  </ul>\n",
    "</div> <br>\n",
    "\n",
    "- **Absolute** error: $$L(w,b)=\\sum_{n=1}^{N}|t_n-y_n| = \\sum_{n=1}^{N}\\big|t_n-(wx_n+b)\\big|$$\n",
    "\n",
    "<div style=\"margin-left: 3em;\">\n",
    "  <strong>Motivation:</strong>\n",
    "  <ul style=\"margin-left: 0;\">\n",
    "    <li>Symmetry: positive and negative errors are treated equally</li>\n",
    "    <li>Linear penalty: Each error is counted proportionally, without amplifying large ones.</li>\n",
    "  </ul>\n",
    "</div> <br><br>\n",
    "\n",
    "**Illustrative example:** Given a set of points $(x_1,t_1)=(1,1)$ , $(x_2,t_2)=(2,6)$ and choosing the line as the candidate model:    \n",
    "$$\n",
    "y = 2x\n",
    "$$\n",
    "Compare the cost function $L$ using the above forms:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto5.png\" width=\"50%\">\n",
    "</div><br><br>\n",
    "\n",
    "#### **Logistic regression :**\n",
    "\n",
    "- **Binary cross-entropy** $\\equiv \\text{log-loss}$:\n",
    "$$\n",
    "L = -\\sum_{n=1}^{N} \\left[ t_n \\log(y_n) + (1 - t_n)\\log(1 - y_n) \\right]\n",
    "$$\n",
    "\n",
    "<div style=\"margin-left: 2em;\">\n",
    "\n",
    "**Motivation:**\n",
    "\n",
    "- It strongly penalizes confident but incorrect predictions, that is, when the prediction differs from the actual label.\n",
    "\n",
    "- For $t \\in \\{0,1\\}$:\n",
    "  - If $y_n = t_n$ → $l \\approx 0$ (minimum error)\n",
    "  - If $y_n \\neq t_n$ → $l > 0$, increases the more confident the incorrect prediction is\n",
    "\n",
    "  Extreme cases:\n",
    "\n",
    "  - $t_n = 1 \\Rightarrow l(t_n, y_n) = -\\log(y_n)$\n",
    "    - if $y_n = 1$ → $l = 0$  *(Trustworthy and correct)*\n",
    "    - if $y_n = 0$ → $l = +\\infty$  *(Trustworthy and incorrect)*\n",
    "\n",
    "  - $t_n = 0 \\Rightarrow l(t_n, y_n) = -\\log(1 - y_n)$\n",
    "    - if $y_n = 0$ → $l = 0$  *(Trustworthy and correct)*\n",
    "    - if $y_n = 1$ → $l = +\\infty$  *(Trustworthy and incorrect)*\n",
    "\n",
    "- Rewards correct predictions and penalizes incorrect ones.\n",
    "\n",
    "- Differentiable and convex → it has a single global minimum, which allows it to be found efficiently using gradient descent.\n",
    "</div><br><br>\n",
    "\n",
    "**Illustrative graph of the prediction, decision boundary, and misclassification:**\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto6.png\" width=\"50%\">\n",
    "</div><br><br>\n",
    "\n",
    "<div style=\"border:2px solid black; padding:15px; text-align:center; margin:20px 0;\">\n",
    "The cost function converts an intuitive judgment (“which line fits best”) into a mathematical optimization problem.\n",
    "</div> <br>\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e5948",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "## **1.4 Partial derivatives and chain rule**\n",
    "\n",
    "**Objective:** obtain the partial derivatives of the cost function $L(w,b)$ with respect to the model parameters:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w,b)}{\\partial w}, \\quad \\frac{\\partial L(w,b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "### **Chain rule**\n",
    "The cost function does not depend directly on the parameters $\\text{w and b}$, but rather through one or more intermediate variables. For this reason, the chain rule is used to calculate the partial derivatives, which allows all dependencies between variables to be chained together.\n",
    "\n",
    "**General procedure:**\n",
    "\n",
    "1. Define intermediate variables (in order):  \n",
    "   Identify all intermediate variables that connect the parameters with the cost function.\n",
    "2. Apply the chain rule to derive the parameter  $\\text{w o b}$.\n",
    "   <span style=\"display:block;\">\n",
    "   **Important:** we derive with respect to one parameter while the other remains constant.\n",
    "   </span>\n",
    "\n",
    "**Difference** between linear and logistic regression: The difference between the two models lies in\n",
    "the intermediate variables that relate the parameters to the cost function.\n",
    "<br><br>\n",
    "\n",
    "<div style=\"margin-left: 2.5em;\">\n",
    "\n",
    "#### Linear regression\n",
    "\n",
    "1. Intermediate variable: $y_n = w x_n + b$ <br>\n",
    "\n",
    "2. Apply chain rule:\n",
    "   - Derive with respect to $w$: $\\frac{\\partial L_n}{\\partial w} = \\frac{\\partial L_n}{\\partial y_n} \\cdot \\frac{\\partial y_n}{\\partial w}$\n",
    "\n",
    "   - Derive with respect to $b$: $\\frac{\\partial L_n}{\\partial b} = \\frac{\\partial L_n}{\\partial y_n} \\cdot \\frac{\\partial y_n}{\\partial b}$ <br><br>\n",
    "\n",
    "\n",
    "#### Logistic regression\n",
    "\n",
    "1. ntermediate variables: $z_n = w x_n + b$ ,&nbsp;&nbsp;&nbsp;&nbsp;$y_n = \\sigma(z_n) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "2. Apply chain rule:\n",
    "   - Derive with respect to $w$: $\\frac{\\partial L_n}{\\partial w} = \\frac{\\partial L_n}{\\partial y_n} \\cdot \\frac{\\partial y_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial w}$\n",
    "\n",
    "   - Derive with respect to $b$: $\\frac{\\partial L_n}{\\partial b} = \\frac{\\partial L_n}{\\partial y_n} \\cdot \\frac{\\partial y_n}{\\partial z_n} \\cdot \\frac{\\partial z_n}{\\partial b}$\n",
    "\n",
    "</div>\n",
    "<br><br>\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a60e65",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "## **1.5 Optimization**\n",
    "\n",
    "**Important !** While the cost function allows you to compare models, optimization allows you to select the best one among them.\n",
    "\n",
    "**Objective:** Find the parameters $\\text{w and b}$ that minimize the cost function $L(w,b)$, that is, those that produce the smallest possible discrepancy between the model's predictions and the actual values.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "**Note:** $\\max f(w,b) \\equiv \\min -f(w,b)$  (opposite effect)\n",
    "</div>\n",
    "\n",
    "### **Cost function gradient**\n",
    "Based on the partial derivatives obtained in section 1.4, the gradient of the cost function is defined as:\n",
    "$$\n",
    "\\nabla L(w,b)\n",
    "=\n",
    "\\left(\n",
    "\\frac{\\partial L(w,b)}{\\partial w},\\;\n",
    "\\frac{\\partial L(w,b)}{\\partial b}\n",
    "\\right)^T\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial L(w,b)}{\\partial w} \\\\\n",
    "\\frac{\\partial L(w,b)}{\\partial b}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This vector reflects the variation in the cost function in response to small changes in each of the model parameters.\n",
    "\n",
    "<div style=\"margin-left: 2.5em;\">\n",
    "\n",
    "**Interpretation of the gradient:**\n",
    "\n",
    "- Direction: indicates the direction in which the cost function varies.\n",
    "- Magnitude: indicates the intensity of this variation (slope).\n",
    "- Optimization: moving in the opposite direction to the gradient reduces the value of the cost function.\n",
    "<br><br>\n",
    "\n",
    "**Meaning of terms:**\n",
    "\n",
    "- $t_n - y_n$: represents the prediction error for observation $n$.\n",
    "- $x_n$: measures the influence of the input on the adjustment of parameter $w$.\n",
    "- $\\text{sign}$: indicates the direction in which the parameter should be modified to reduce the value of $L$.\n",
    "\n",
    "Example: if $\\frac{\\partial L(w,b)}{\\partial w} \\succ 0$, an increase in $w$ increases the cost function; therefore, to reduce $L$, it is necessary to decrease $w$ .\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "**Important !** The gradient does not depend on the model, but on the cost function. What changes between models is the explicit expression of the derivatives, not their meaning or their role in optimization.\n",
    "<br><br>\n",
    "\n",
    "### **Optimization Methods**\n",
    "#### **1.5.1 Analytical Method**\n",
    "**Objective:** Find the critical points of $L(w,b)$ , where the slope is zero.\n",
    "\n",
    "The partial derivatives are set equal to zero:\n",
    "$$\n",
    "\\frac{\\partial L(w,b)}{\\partial w} = 0, \\quad\n",
    "\\frac{\\partial L(w,b)}{\\partial b} = 0\n",
    "$$\n",
    "\n",
    "Solving the system yields $w^*$ y $b^*$ , which correspond to critical points (minima, maxima, or saddle points) .<br>\n",
    "\n",
    "\n",
    "**Note:** To classify a critical point, the Hessian is analyzed (see 1.6)<br><br>\n",
    "\n",
    "\n",
    "**Important !** : If $L$ is **convex**, the critical point is the global minimum\n",
    "\n",
    "Therefore, the best possible model is:\n",
    "$$\n",
    "z = w^* x + b^*\n",
    "$$\n",
    "\n",
    "<div style=\"margin-left: 2.5em;\">\n",
    "\n",
    "**Geometric interpretation:** The cost function forms a bowl (paraboloid) in the space $(w,b,L)$\n",
    "</div> \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto7.png\" width=\"30%\">\n",
    "</div><br><br>\n",
    "\n",
    "\n",
    "#### **1.5.2 Iterative method: Gradient Descent (GD)**\n",
    "This method allows us to approximate the minimum of the cost function when it is not possible (or convenient) to solve the problem analytically. It consists of iteratively updating the model parameters in the opposite direction to the gradient $$\\nabla L(w,b)$$\n",
    "\n",
    "**Requirement:** $L(w,b)$ must be differentiable. \n",
    "\n",
    "**General procedure:**\n",
    "\n",
    "The parameters are initialized at $t=0$ with given values $w_0$ y $b_0$ .\n",
    "From there, they are updated iteratively:\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\alpha \\cdot \\frac{\\partial L(w,b)}{\\partial w}\n",
    "\\qquad\\qquad\n",
    "b_{t+1} = b_t - \\alpha \\cdot \\frac{\\partial L(w,b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "for $t = 0, 1, 2, \\ldots$, until the algorithm converges or approaches the minimum of $L$.\n",
    "\n",
    "- $\\alpha$ is the learning rate and controls the step size.\n",
    "  It can be any value, but **be careful!**\n",
    "  - $\\alpha$ too small → slow convergence.\n",
    "  - $\\alpha$ too large → risk of oscillations or divergence.\n",
    "  - The optimal $\\alpha$ depends on the curvature of $L$: Greater curvature → lower $\\alpha$, because the slope changes more quickly.\n",
    "\n",
    "In each iteration, the parameters move in the opposite direction to the gradient, progressively reducing the value of $L$.\n",
    "<br>\n",
    "\n",
    "**Convergence properties**\n",
    "\n",
    "- If $L(w,b)$ is convex: converge to the global minimum, regardless of the starting point\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto8.png\" width=\"50%\">\n",
    "</div><br><br>\n",
    "\n",
    "- If $L(w,b)$ is not convex: there may be multiple local minima and the result depends on the initialization starting point\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto9.png\" width=\"50%\">\n",
    "</div><br><br>\n",
    "\n",
    "\n",
    "##### **1.5.2.1 Application to linear regression**  \n",
    "Using the quadratic error as a cost function:\n",
    "\n",
    "- $L(w,b)$ is convex</li>\n",
    "- There is an analytical solution</li>\n",
    "- GD and the analytical method converge to the same global minimum\n",
    "<br><br>\n",
    "\n",
    "##### **1.5.2.2 Application to logistic regression**  \n",
    "Using binary cross-entropy as the cost function:\n",
    "\n",
    "- $L(w,b)$ is convex\n",
    "- There is no closed-form analytical solution\n",
    "- GD is the usual method for optimizing the parameters\n",
    "<br><br>\n",
    "\n",
    "Gradient Descent is a general optimization method that does not depend on the model, but only on the cost function and its gradient.\n",
    "<br><br><br>\n",
    "\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12fdf96",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "## **1.6 Advanced optimization: replacing Gradient Descent with the Newton method**\n",
    "\n",
    "### **Motivation: limitations of GD**\n",
    "In the previous section, we studied the Gradient Descent method, which uses only first-order information (the gradient) to minimize the cost function $L(w,b)$. Although GD is simple and robust, it can converge slowly, especially when:\n",
    "\n",
    "- The cost function has a pronounced curvature (very steep or very flat areas).\n",
    "- The gradient changes significantly in different directions (ill-conditioned problems).\n",
    "\n",
    "To improve convergence speed, the Newton method is introduced, an optimization method\n",
    "that uses second-order information, incorporating the local curvature\n",
    "of the cost function.\n",
    "\n",
    "### **General idea of Newton's method**\n",
    "Newton's method is based on approximating the cost function $L(w,b)$ using a second-order Taylor expansion around a current point $\\theta_t = (w_t, b_t)$:\n",
    "\n",
    "$$\n",
    "L(\\theta) \\approx L(\\theta_t) + \\nabla L(\\theta_t)^T(\\theta - \\theta_t)\n",
    "\\;+\\;\n",
    "\\frac{1}{2}(\\theta - \\theta_t)^T H(\\theta_t)(\\theta - \\theta_t)\n",
    "$$\n",
    "\n",
    "- $L(\\theta_t)$: current value of the function\n",
    "- $\\nabla L(\\theta_t)^T(\\theta - \\theta_t)$: slope (gradient)\n",
    "- $\\frac{1}{2}(\\theta - \\theta_t)^T H(\\theta_t)(\\theta - \\theta_t)$: curvature (Hessian)\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "- GD only follows the slope → uniform steps defined by $\\alpha$\n",
    "- Newton, apart from the slope, also looks at the curvature → adapted steps, approaching the minimum faster.\n",
    "<br>\n",
    "\n",
    "**Illustrative graph of Gradient Descent vs. Newton in 2D:**\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto10.png\" width=\"50%\">\n",
    "</div><br><br>\n",
    "\n",
    "\n",
    "### **Hessian: definition and interpretation**\n",
    "It is the matrix of second partial derivatives of the cost function with respect to the parameters:\n",
    "\n",
    "$$\n",
    "H =\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial^2 L(w,b)}{\\partial w^2} & \\frac{\\partial^2 L(w,b)}{\\partial w \\partial b} \\\\\n",
    "\\frac{\\partial^2 L(w,b)}{\\partial b \\partial w} & \\frac{\\partial^2 L(w,b)}{\\partial b^2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "<div style=\"margin-left: 2.5em;\">\n",
    "\n",
    "#### **Interpretation of the Hessian:**\n",
    "\n",
    "- Describes the local curvature of $L(w,b)$\n",
    "- Evaluated at a critical point, it allows the region around that point to be classified as:\n",
    "  - local minimum (valley)\n",
    "  - local maximum (hill)\n",
    "  - saddle point\n",
    "- For smooth functions, the symmetry of cross derivatives is satisfied:\n",
    "$$\n",
    "\\frac{\\partial^2 L(w,b)}{\\partial w \\partial b} = \\frac{\\partial^2 L(w,b)}{\\partial b \\partial w} \n",
    "$$\n",
    "which guarantees that $H$ is symmetric.\n",
    "<br><br>\n",
    "\n",
    "#### **Smoothness requirements for Newton:**\n",
    "\n",
    "For Newton's method to be applicable, the cost function must:\n",
    "\n",
    "- Be differentiable at least twice with respect to all parameters.\n",
    "- Not have any discontinuities or irregularities that prevent the existence of continuous partial derivatives.\n",
    "- Satisfy the symmetry of the second cross derivatives, ensuring that the Hessian is symmetric and usable in Newton's update.\n",
    "<br>\n",
    "\n",
    "**Note:** Both linear regression with quadratic error and logistic regression with cross-entropy fulfil these conditions. Therefore, the Hessian is symmetric and Newton can be applied safely.\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "#### **Side note: Local vs. global analysis**\n",
    "Before classifying a critical point, it is important to distinguish between local and global properties.\n",
    "\n",
    "- **Local minimum and maximum**\n",
    "  - A local minimum is the lowest point in a region\n",
    "  - A local maximum is the highest point in a region\n",
    "  <br><br>\n",
    "  \n",
    "- **Global minimum and maximum**\n",
    "  - A global minimum is the lowest point in the entire function\n",
    "  - A global maximum is the highest point in the entire function\n",
    "  \n",
    "  Every global minimum is local, but not every local minimum is global !!!\n",
    " \n",
    "<br>\n",
    "\n",
    "#### **Global Form of the Function**\n",
    "\n",
    "When we analyze the function as a whole, we talk about:\n",
    "\n",
    "- **Convex** function:\n",
    "  - It has a single minimum, and this minimum is global\n",
    "<br><br>\n",
    "\n",
    "- **Concave** function:\n",
    "  - It has a single maximum, and this maximum is global\n",
    " <br><br>\n",
    " \n",
    "- **Non-convex and non-concave** function:\n",
    "  - It can present multiple local minima, local maxima, and saddle points\n",
    " <br><br>\n",
    "\n",
    "#### **Local classification using the Hessian (region around the point)**\n",
    "\n",
    "<div style=\"margin-left: 3em;\">\n",
    "\n",
    "**Case 1: Two-variable functions (2D)**  \n",
    "When the function depends on two parameters, local classification can\n",
    "be performed using the Hessian determinant:\n",
    "$$\n",
    "H =\n",
    "\\begin{pmatrix}\n",
    "L_{ww} & L_{wb} \\\\\n",
    "L_{bw} & L_{bb}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "D = L_{ww}\\cdot L_{bb} - (L_{wb})^2\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "- If $D(H) > 0$ and $L_{ww} > 0$ → local minimum (valley)\n",
    "- If $D(H) > 0$ and $L_{ww} < 0$ → local maximum (hill)\n",
    "- If $D(H) < 0$ → saddle point\n",
    "- If $D(H) = 0$ → inconclusive\n",
    "\n",
    "<br>\n",
    "\n",
    "**Case 2: Functions of more than two variables**  \n",
    "The determinant criterion is no longer sufficient, so the classification is performed\n",
    "using the eigenvalues of $H$, qwhich are obtained by solving the characteristic equation:\n",
    "\n",
    "$$\n",
    "D(H - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "| **Hessian** | **Property** | **Conclusion** |\n",
    "|---|---|---|\n",
    "| Positive definite | All eigenvalues > 0 | Local minimum |\n",
    "| Negative definite | All eigenvalues < 0 | Local maximum |\n",
    "| Indefinite | Eigenvalues with different signs | Saddle point |\n",
    "| Semidefinite | If any eigenvalue = 0 | Inconclusive |\n",
    "\n",
    "</div>\n",
    "\n",
    "**Connection with the Newton method**\n",
    "\n",
    "For the Newton method, it is particularly important that the Hessian is symmetric and positive definite in the vicinity of the minimum.\n",
    "\n",
    "This guarantees that the update direction corresponds to a descent and that the method converges to a minimum.\n",
    "\n",
    "In convex cost functions, such as mean square error or cross entropy, the Hessian is positive semidefinite throughout the domain, which ensures that the minimum reached by Newton is also the global minimum.\n",
    "<br><br>\n",
    "\n",
    "### **Illustrative graph of curvature and gradient in 3D:**\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto11.png\" width=\"30%\">\n",
    "</div><br><br>\n",
    "\n",
    "\n",
    "### **Newton's update rule**\n",
    "\n",
    "The update of the parameters is given by:\n",
    "$$\n",
    "\\theta_{t+1}\n",
    "=\n",
    "\\theta_t\n",
    "-\n",
    "H^{-1}\\nabla L(\\theta_t)\n",
    "$$\n",
    "where:\n",
    "\n",
    "- $\\theta = (w, b)^T$ is the model parameter vector\n",
    "- $\\nabla L(\\theta_t)$ is the gradient\n",
    "- $H^{-1}$ is the inverse of the Hessian\n",
    "<br><br>\n",
    "\n",
    "Unlike GD, no learning rate $\\alpha$ is needed, as the step size is automatically adjusted according to the curvature of $L$ . \n",
    "<br><br>\n",
    "\n",
    "#### 1.6.1 Application to linear regression\n",
    "\n",
    "**Pseudocode: Newton's method for linear regression**\n",
    "\n",
    "1. Initialize the parameters of $\\text{w and b}$\n",
    "2. Repeat:\n",
    "   - Calculate the predictions: $y_n = w x_n + b$\n",
    "   - Calculate the partial derivatives of the cost function:\n",
    "     $$\\frac{\\partial L(w,b)}{\\partial w}, \\quad\n",
    "     \\frac{\\partial L(w,b)}{\\partial b}$$\n",
    "   - Calculate the Hessian $H$ (matrix of second derivatives):\n",
    "     $$H =\n",
    "     \\begin{pmatrix}\n",
    "     \\frac{\\partial^2 L(w,b)}{\\partial w^2} &\n",
    "     \\frac{\\partial^2 L(w,b)}{\\partial w \\, \\partial b} \\\\\n",
    "     \\frac{\\partial^2 L(w,b)}{\\partial b \\, \\partial w} &\n",
    "     \\frac{\\partial^2 L(w,b)}{\\partial b^2}\n",
    "     \\end{pmatrix}$$\n",
    "   - Update the parameters using Newton's method:\n",
    "     $$\\theta_{t+1} = \\theta_t - H^{-1} \\nabla L(\\theta_t)$$\n",
    "3. Until the minimum of the cost function is reached\n",
    "<br><br>\n",
    "\n",
    "\n",
    "**For linear regression with quadratic error:**\n",
    "$$L(w,b)=\\sum_{n=1}^{N}(t_n-y_n)^2 = \\sum_{n=1}^{N}\\big(t_n-(wx_n+b)\\big)^2$$\n",
    "\n",
    "- The cost function is convex and quadratic.\n",
    "- The Hessian is constant and does not depend on the parameters:\n",
    "$$\n",
    "H =\n",
    "\\begin{pmatrix}\n",
    "\\sum x_n^2 & \\sum x_n \\\\\n",
    "\\sum x_n   & N\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "- Newton's method converges to the global minimum in a single iteration (in theory).\n",
    "- This coincides with the closed analytical solution: Newton generalizes the closed approach.\n",
    "<br><br>\n",
    "\n",
    "**Illustrative example:**  Given a set of points  $(x_1,t_1)=(1,2)$ , $(x_2,t_2)=(2,3)$, and considering a linear regression model with quadratic error, demonstrate that the minimum obtained using the analytical method coincides with the minimum reached after a single iteration of Newton's method:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"imagenes/foto12.png\" width=\"50%\">\n",
    "  <img src=\"imagenes/foto12.png\" width=\"50%\">\n",
    "</div><br><br>\n",
    "\n",
    "#### 1.6.2 Application to logistic regression (cross-entropy)\n",
    "\n",
    "**Pseudocode: Newton's method for logistic regression**\n",
    "\n",
    "1. Initialize the parameters of $\\text{w and b}$\n",
    "2. Repeat:\n",
    "   - Calculate the intermediate variable: $z_n = w x_n + b$\n",
    "   - Calculate the predictions: $y_n = \\sigma(z_n)$\n",
    "   - Calculate the partial derivatives of the cost function:\n",
    "     $$\\frac{\\partial L(w,b)}{\\partial w}, \\quad\n",
    "     \\frac{\\partial L(w,b)}{\\partial b}$$\n",
    "   - Calculate the Hessian $H$ hat depends on $y_n(1 - y_n)$:\n",
    "     $$\n",
    "     H = X^T R X\n",
    "     $$\n",
    "   - Update the parameters using Newton's method:\n",
    "     $$\\theta_{t+1} = \\theta_t - H^{-1} \\nabla L(\\theta_t)$$\n",
    "3. Until the minimum of the cost function is reached\n",
    "<br><br>\n",
    "\n",
    "**For logistic regression with cross-entropy:**\n",
    "$$\n",
    "L = -\\sum_{n=1}^{N} \\left[ t_n \\log(y_n) + (1 - t_n)\\log(1 - y_n) \\right]\n",
    "$$\n",
    " \n",
    "- The cost function is convex\n",
    "- The Hessian depends on the parameters, normally defined as:\n",
    "$$\n",
    "H = X^T R X\n",
    "$$\n",
    "where $R$ is a diagonal matrix with $y_n(1 - y_n)$.\n",
    "<br>\n",
    "\n",
    "- Newton is applied iteratively, also called Newton–Raphson or IRLS (Iteratively Reweighted Least Squares)\n",
    "- Much faster convergence than GD, especially near the minimum\n",
    "<br><br>\n",
    "\n",
    "### Comparison: Gradient Descent vs Newton\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "| **Aspect** | **GD** | **Newton** |\n",
    "|------------|--------|------------|\n",
    "| **Information used** | Gradient (1st order) | Gradient + Hessian (2nd order) |\n",
    "| **Step size** | Fixed learning rate | Determined by curvature |\n",
    "| **Convergence** | Linear | Quadratic near the minimum |\n",
    "| **Computational cost** | Low | High |\n",
    "| **Robustness** | High, stable even far from the minimum | Sensitive to poor initialization or ill-conditioned Hessians |\n",
    "| **Typical application** | Large or high-dimensional data | Small or medium-sized problems requiring high precision |\n",
    "</div><br>\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c8476c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "## **1.7 Final conclusions**\n",
    "In this section, we have seen how we can use a data set to build linear and logistic regression models to predict a target variable. The general process begins with identifying the type of problem, followed by selecting an appropriate model, defining a cost function, and optimizing its parameters.\n",
    "\n",
    "The Newton method is presented as an advanced alternative to Gradient Descent. While GD uses only first-order information and requires a carefully chosen learning rate, Newton takes advantage of second-order information (Hessian) to adapt the step size to the curvature of the cost function, allowing for much faster convergence, albeit at a higher computational cost. For linear regression, Newton practically coincides with the analytical solution, reaching the minimum in a single iteration. For logistic regression, it is applied iteratively (Newton–Raphson or IRLS) and remains more efficient than GD, especially near the minimum.\n",
    "\n",
    "In summary, Newton is another way to achieve the final objective of the block: the optimization of the model parameters.\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
