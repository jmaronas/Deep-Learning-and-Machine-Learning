{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a21c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbf3e2",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "\n",
    "Machine learning models cannot directly work with raw text. In many NLP and information\n",
    "retrieval problems, we first need to convert documents into numerical vectors.\n",
    "\n",
    "A simple idea is to count word occurrences (Bag of Words). The limitation is that very common\n",
    "terms across the corpus (e.g. *is*, *the*, *and*) may dominate the representation, even though\n",
    "they do not help distinguish documents.\n",
    "\n",
    "TF-IDF is a classical weighting scheme that balances:\n",
    "\n",
    "- **Term Frequency (TF):** how relevant a term is inside a document (local importance)\n",
    "- **Inverse Document Frequency (IDF):** how informative a term is across the corpus (global rarity)\n",
    "\n",
    "As a result, TF-IDF highlights words that characterize a document and down-weights words that\n",
    "appear in many documents.\n",
    "\n",
    "TF-IDF is widely used in:\n",
    "- document search and ranking\n",
    "- keyword extraction\n",
    "- document clustering and similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef6a82",
   "metadata": {},
   "source": [
    "## 1. Why do we need TF-IDF?\n",
    "\n",
    "When working with text data, machine learning models require numerical representations.\n",
    "A straightforward approach is to count how many times each word appears in a document\n",
    "(Bag of Words representation).\n",
    "\n",
    "However, this approach presents an important limitation:  \n",
    "words that appear very frequently across the entire corpus tend to dominate the representation,\n",
    "even if they do not provide useful information to distinguish documents.\n",
    "\n",
    "For example, terms such as *is*, *and* or *the* may appear in almost every document.\n",
    "Although they have high frequency, they carry little semantic value for identifying\n",
    "the topic or meaning of a document.\n",
    "\n",
    "TF-IDF (Term Frequency – Inverse Document Frequency) addresses this issue by balancing\n",
    "two complementary ideas:\n",
    "\n",
    "- **Local importance:** how relevant a word is within a specific document.\n",
    "- **Global importance:** how rare or informative that word is across the whole corpus.\n",
    "\n",
    "By combining these two aspects, TF-IDF assigns:\n",
    "- high weights to words that are frequent in a document but rare in the corpus,\n",
    "- low weights to words that appear in many documents.\n",
    "\n",
    "As a result, TF-IDF produces representations that better capture what makes a document\n",
    "distinct from others, which is essential for tasks such as:\n",
    "- document search and ranking,\n",
    "- similarity-based retrieval,\n",
    "- clustering and topic analysis.\n",
    "\n",
    "TF-IDF remains a fundamental technique in natural language processing because it is\n",
    "simple, interpretable, and often very effective despite its simplicity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d9343",
   "metadata": {},
   "source": [
    "## 2. Corpus and basic preprocessing concepts\n",
    "\n",
    "A **corpus** is a collection of documents. TF-IDF is defined with respect to the corpus because\n",
    "IDF depends on how many documents contain each term.\n",
    "\n",
    "A **document** can be any piece of text: an email, a webpage, a paragraph, etc.\n",
    "In practice, we often apply preprocessing to reduce noise and ensure consistent counting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91299c2",
   "metadata": {},
   "source": [
    "## 3. Text preprocessing\n",
    "\n",
    "Preprocessing aims to represent similar textual forms in a consistent way.\n",
    "\n",
    "Typical steps:\n",
    "- **Lowercasing**: makes \"Learning\" and \"learning\" identical.\n",
    "- **Removing punctuation / non-alphabetic characters**: reduces noise.\n",
    "- **Tokenization**: splits text into individual terms (tokens).\n",
    "\n",
    "The choices here depend on the task: aggressive cleaning may remove useful information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c0b557",
   "metadata": {},
   "source": [
    "## 4. Vocabulary and vector representation\n",
    "\n",
    "To represent documents as vectors, we define a **vocabulary**: the set of all unique terms in the corpus.\n",
    "\n",
    "Each term is assigned an index so that:\n",
    "- every document can be represented as a fixed-length vector\n",
    "- the same term always corresponds to the same vector position\n",
    "\n",
    "`token2idx` maps each term to its vector index, and `idx2token` provides the inverse mapping.\n",
    "This is the standard way to build a Bag-of-Words / TF-IDF representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ad17c",
   "metadata": {},
   "source": [
    "## 5. Term Frequency (TF)\n",
    "\n",
    "**TF** measures how much a term matters inside a document.\n",
    "\n",
    "If a word appears many times in a document, it is likely related to that document’s topic.\n",
    "However, longer documents naturally have more occurrences, so we commonly normalize:\n",
    "\n",
    "\\[\n",
    "TF(t, d) = \\frac{\\text{count}(t \\in d)}{|d|}\n",
    "\\]\n",
    "\n",
    "where \\(|d|\\) is the number of tokens in the document.\n",
    "This produces comparable values across documents of different lengths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f8e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_vector(tokens, token2idx):\n",
    "    vec = np.zeros(len(token2idx), dtype=float)\n",
    "    for t in tokens:\n",
    "        if t in token2idx:\n",
    "            vec[token2idx[t]] += 1.0\n",
    "    denom = max(len(tokens), 1)\n",
    "    return vec / denom\n",
    "\n",
    "##Generative AI tools were used to help refine wording and structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25373e",
   "metadata": {},
   "source": [
    "## 6. Document Frequency (DF) and Inverse Document Frequency (IDF)\n",
    "\n",
    "TF alone cannot distinguish between informative terms and terms that appear everywhere.\n",
    "\n",
    "- **DF(t)** counts how many documents contain a term:\n",
    "\\[\n",
    "DF(t) = |\\{d \\in D : t \\in d\\}|\n",
    "\\]\n",
    "\n",
    "- **IDF(t)** down-weights terms that appear in many documents and up-weights rarer terms:\n",
    "\n",
    "\\[\n",
    "IDF(t) = \\log\\left(\\frac{1 + N}{1 + DF(t)}\\right) + 1\n",
    "\\]\n",
    "\n",
    "where \\(N\\) is the number of documents.\n",
    "\n",
    "The `+1` smoothing avoids division by zero and keeps the scale stable for small corpora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3cb2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(tokenized_docs, token2idx):\n",
    "    df = np.zeros(len(token2idx), dtype=float)\n",
    "    for doc in tokenized_docs:\n",
    "        for t in set(doc):\n",
    "            if t in token2idx:\n",
    "                df[token2idx[t]] += 1.0\n",
    "    return df\n",
    "\n",
    "##Generative AI tools were used to help refine wording and structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76a4b3",
   "metadata": {},
   "source": [
    "## 7. TF-IDF\n",
    "\n",
    "TF-IDF combines **local importance** (TF) with **global importance** (IDF).\n",
    "\n",
    "\\[\n",
    "tfidf(t, d) = tf(t, d) \\cdot idf(t)\n",
    "\\]\n",
    "\n",
    "- A word gets a **high TF-IDF score** if:\n",
    "  - it appears frequently in a document\n",
    "  - and appears in few documents overall\n",
    "\n",
    "- A word gets a **low TF-IDF score** if:\n",
    "  - it appears in almost every document (low discriminative power)\n",
    "\n",
    "TF-IDF is one of the most widely used techniques for classical text representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7160e",
   "metadata": {},
   "source": [
    "## 8. TF-IDF vectors and document similarity\n",
    "\n",
    "Once each document is represented as a TF-IDF vector, we can compare documents mathematically.\n",
    "\n",
    "A common choice is **cosine similarity**, which measures the angle between two vectors:\n",
    "\n",
    "\\[\n",
    "\\cos(\\theta) = \\frac{x \\cdot y}{\\|x\\|\\|y\\|}\n",
    "\\]\n",
    "\n",
    "Cosine similarity is preferred in text because it focuses on direction (term importance pattern)\n",
    "rather than raw magnitude (document length). This is useful for:\n",
    "- ranking documents for a search query\n",
    "- finding similar documents\n",
    "- clustering documents by topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f74dd",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "Nombre: Álvaro Aguilar Dávila  \n",
    "Asignatura: Apendizaje Automáico  \n",
    "Grado: Ingeniería Informática + ADE  \n",
    "Universidad: CUNEF  \n",
    "Año: 2026\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
