{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a21c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbf3e2",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "\n",
    "Machine learning models cannot directly operate on raw text. In many natural language processing and information retrieval tasks, textual data must first be transformed into numerical representations in order to be processed by algorithms.\n",
    "\n",
    "A straightforward approach is to count how many times each word appears in a document, known as the Bag of Words representation. However, this method presents an important limitation: words that appear very frequently across the entire corpus tend to dominate the representation, even if they provide little information for distinguishing between documents. For example, terms such as is, and or the may occur in almost every document. Although they have high frequency, they carry little semantic value for identifying the topic or meaning of a document.\n",
    "\n",
    "TF-IDF (Term Frequency – Inverse Document Frequency) addresses this issue by balancing two complementary ideas. On the one hand, Term Frequency (TF) measures how relevant a term is within a specific document, capturing its local importance. On the other hand, Inverse Document Frequency (IDF) reflects how informative a term is across the entire corpus by down-weighting terms that appear in many documents and emphasizing rarer ones.\n",
    "\n",
    "By combining local and global importance, TF-IDF assigns high weights to words that are frequent in a document but rare in the corpus, and low weights to words that appear in many documents. As a result, TF-IDF produces representations that better capture what makes a document distinct from others.\n",
    "\n",
    "This property makes TF-IDF especially useful for tasks such as document search and ranking, similarity-based retrieval, keyword extraction, and document clustering. Despite its simplicity, TF-IDF remains a fundamental and widely used technique in natural language processing due to its interpretability and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d9343",
   "metadata": {},
   "source": [
    "## 1. Corpus and basic preprocessing concepts\n",
    "\n",
    "A **corpus** is a collection of documents. TF-IDF is defined with respect to the corpus because\n",
    "IDF depends on how many documents contain each term.\n",
    "\n",
    "A **document** can be any piece of text: an email, a webpage, a paragraph, etc.\n",
    "In practice, we often apply preprocessing to reduce noise and ensure consistent counting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91299c2",
   "metadata": {},
   "source": [
    "## 2. Text preprocessing\n",
    "\n",
    "Preprocessing aims to represent similar textual forms in a consistent way.\n",
    "\n",
    "Typical steps:\n",
    "- **Lowercasing**: makes \"Learning\" and \"learning\" identical.\n",
    "- **Removing punctuation / non-alphabetic characters**: reduces noise.\n",
    "- **Tokenization**: splits text into individual terms (tokens).\n",
    "\n",
    "The choices here depend on the task: aggressive cleaning may remove useful information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c0b557",
   "metadata": {},
   "source": [
    "## 3. Vocabulary and vector representation\n",
    "\n",
    "To represent documents as vectors, we define a **vocabulary**: the set of all unique terms in the corpus.\n",
    "\n",
    "Each term is assigned an index so that:\n",
    "- every document can be represented as a fixed-length vector\n",
    "- the same term always corresponds to the same vector position\n",
    "\n",
    "`token2idx` maps each term to its vector index, and `idx2token` provides the inverse mapping.\n",
    "This is the standard way to build a Bag-of-Words / TF-IDF representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ad17c",
   "metadata": {},
   "source": [
    "## 4. Term Frequency (TF)\n",
    "\n",
    "**TF** measures how much a term matters inside a document.\n",
    "\n",
    "If a word appears many times in a document, it is likely related to that document’s topic.\n",
    "However, longer documents naturally have more occurrences, so we commonly normalize:\n",
    "\n",
    "$$\n",
    "TF(t, d) = \\frac{\\text{count}(t \\in d)}{|d|}\n",
    "$$\n",
    "\n",
    "where \\(|d|\\) is the number of tokens in the document.\n",
    "This produces comparable values across documents of different lengths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f8e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_vector(tokens, token2idx):\n",
    "    vec = np.zeros(len(token2idx), dtype=float)\n",
    "    for t in tokens:\n",
    "        if t in token2idx:\n",
    "            vec[token2idx[t]] += 1.0\n",
    "    denom = max(len(tokens), 1)\n",
    "    return vec / denom\n",
    "\n",
    "##Generative AI tools were used to help refine wording and structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25373e",
   "metadata": {},
   "source": [
    "## 5. Document Frequency (DF) and Inverse Document Frequency (IDF)\n",
    "\n",
    "TF alone cannot distinguish between informative terms and terms that appear everywhere.\n",
    "\n",
    "- **DF(t)** counts how many documents contain a term:\n",
    "\\[\n",
    "DF(t) = |\\{d \\in D : t \\in d\\}|\n",
    "\\]\n",
    "\n",
    "- **IDF(t)** down-weights terms that appear in many documents and up-weights rarer terms:\n",
    "\n",
    "$$\n",
    "IDF(t) = \\log\\left(\\frac{1 + N}{1 + DF(t)}\\right) + 1\n",
    "$$\n",
    "\n",
    "where \\(N\\) is the number of documents.\n",
    "\n",
    "The `+1` smoothing avoids division by zero and keeps the scale stable for small corpora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3cb2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(tokenized_docs, token2idx):\n",
    "    df = np.zeros(len(token2idx), dtype=float)\n",
    "    for doc in tokenized_docs:\n",
    "        for t in set(doc):\n",
    "            if t in token2idx:\n",
    "                df[token2idx[t]] += 1.0\n",
    "    return df\n",
    "\n",
    "##Generative AI tools were used to help refine wording and structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76a4b3",
   "metadata": {},
   "source": [
    "## 6. TF-IDF\n",
    "\n",
    "TF-IDF combines **local importance** (TF) with **global importance** (IDF).\n",
    "\n",
    "$$\n",
    "tfidf(t, d) = tf(t, d) \\cdot idf(t)\n",
    "$$\n",
    "\n",
    "- A word gets a **high TF-IDF score** if:\n",
    "  - it appears frequently in a document\n",
    "  - and appears in few documents overall\n",
    "\n",
    "- A word gets a **low TF-IDF score** if:\n",
    "  - it appears in almost every document (low discriminative power)\n",
    "\n",
    "TF-IDF is one of the most widely used techniques for classical text representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7160e",
   "metadata": {},
   "source": [
    "## 7. TF-IDF vectors and document similarity\n",
    "\n",
    "Once each document is represented as a TF-IDF vector, we can compare documents mathematically.\n",
    "\n",
    "A common choice is **cosine similarity**, which measures the angle between two vectors:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{x \\cdot y}{\\|x\\|\\|y\\|}\n",
    "$$\n",
    "\n",
    "Cosine similarity is preferred in text because it focuses on direction (term importance pattern)\n",
    "rather than raw magnitude (document length). This is useful for:\n",
    "- ranking documents for a search query\n",
    "- finding similar documents\n",
    "- clustering documents by topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f74dd",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "Nombre: Álvaro Aguilar Dávila  \n",
    "Asignatura: Apendizaje Automáico  \n",
    "Grado: Ingeniería Informática + ADE  \n",
    "Universidad: CUNEF  \n",
    "Año: 2026\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
