{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "373ff1b4",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "This assessment introduces the TF-IDF algorithm for representing text as vectors.\n",
    "We will implement TF, IDF and TF-IDF from scratch.\n",
    "\n",
    "Goals:\n",
    "- Understand the intuition behind TF-IDF\n",
    "- Implement TF, IDF and TF-IDF manually\n",
    "- Use TF-IDF for document similarity with cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a21c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d9343",
   "metadata": {},
   "source": [
    "## 1. Corpus\n",
    "\n",
    "We will start with a small set of short documents.\n",
    "\n",
    "**Task:** Define a corpus with at least 4 documents. Try to include repeated and rare words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc1645be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "1. Machine learning is powerful\n",
      "2. Deep learning is a branch of machine learning\n",
      "3. This document is about learning representations of text\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"Machine learning is powerful\",\n",
    "    \"Deep learning is a branch of machine learning\",\n",
    "    \"This document is about learning representations of text\",\n",
    "]\n",
    "\n",
    "print(\"Number of documents:\", len(docs))\n",
    "for i, d in enumerate(docs, 1):\n",
    "    print(f\"{i}. {d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91299c2",
   "metadata": {},
   "source": [
    "## 2. Text preprocessing\n",
    "\n",
    "We will:\n",
    "- lowercase\n",
    "- remove non alphabetic characters\n",
    "- tokenize by whitespace\n",
    "\n",
    "**Task:** Implement `tokenize(text)` returning a list of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7476a159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['machine', 'learning', 'is', 'powerful'],\n",
       " ['deep', 'learning', 'is', 'a', 'branch', 'of', 'machine', 'learning'],\n",
       " ['this',\n",
       "  'document',\n",
       "  'is',\n",
       "  'about',\n",
       "  'learning',\n",
       "  'representations',\n",
       "  'of',\n",
       "  'text']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.split()\n",
    "\n",
    "tokenized_docs = [tokenize(d) for d in docs]\n",
    "tokenized_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c0b557",
   "metadata": {},
   "source": [
    "## 3. Vocabulary\n",
    "\n",
    "The vocabulary is the set of unique tokens across all documents.\n",
    "\n",
    "**Task:** Build `vocab`, `token2idx` and `idx2token`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d397b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "First terms: ['a', 'about', 'branch', 'deep', 'document', 'is', 'learning', 'machine', 'of', 'powerful', 'representations', 'text', 'this']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(t for doc in tokenized_docs for t in doc))\n",
    "token2idx = {t: i for i, t in enumerate(vocab)}\n",
    "idx2token = {i: t for t, i in token2idx.items()}\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"First terms:\", vocab[:25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ad17c",
   "metadata": {},
   "source": [
    "## 4. Term Frequency (TF)\n",
    "\n",
    "Term Frequency (TF) measures how important a word is **within a single document**.\n",
    "\n",
    "The basic idea is:\n",
    "- Words that appear many times in a document are likely to be important for that document.\n",
    "- However, longer documents naturally contain more words, so we normalize by the document length.\n",
    "\n",
    "We define the normalized TF as:\n",
    "\n",
    "\\[\n",
    "tf(t, d) = \\frac{\\text{count}(t \\in d)}{|d|}\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(t\\) is a term (word)\n",
    "- \\(d\\) is a document\n",
    "- \\(|d|\\) is the number of tokens in the document\n",
    "\n",
    "This normalization allows fair comparison between documents of different lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99f8e157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF matrix shape: (3, 13)\n"
     ]
    }
   ],
   "source": [
    "def tf_vector(tokens, token2idx):\n",
    "    vec = np.zeros(len(token2idx), dtype=float)\n",
    "    for t in tokens:\n",
    "        if t in token2idx:\n",
    "            vec[token2idx[t]] += 1.0\n",
    "    denom = max(len(tokens), 1)\n",
    "    return vec / denom\n",
    "\n",
    "TF = np.vstack([tf_vector(doc, token2idx) for doc in tokenized_docs])\n",
    "print(\"TF matrix shape:\", TF.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25373e",
   "metadata": {},
   "source": [
    "## 5. Document Frequency (DF) and Inverse Document Frequency (IDF)\n",
    "\n",
    "While TF captures how important a word is inside a document, it does not consider\n",
    "how common the word is across the entire corpus.\n",
    "\n",
    "### Document Frequency (DF)\n",
    "\n",
    "Document Frequency counts in how many documents a term appears:\n",
    "\n",
    "\\[\n",
    "df(t) = |\\{ d \\in D : t \\in d \\}|\n",
    "\\]\n",
    "\n",
    "- Common words (e.g. *learning*) tend to have high DF.\n",
    "- Rare words (e.g. *pizza*) tend to have low DF.\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "Inverse Document Frequency reduces the weight of very common words and increases the\n",
    "importance of rare but informative words.\n",
    "\n",
    "We use a smoothed IDF:\n",
    "\n",
    "\\[\n",
    "idf(t) = \\log\\left(\\frac{1 + N}{1 + df(t)}\\right) + 1\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(N\\) is the total number of documents\n",
    "- Smoothing avoids division by zero and keeps values well-behaved\n",
    "\n",
    "IDF acts as a global weighting factor shared across all documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3cb2c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning   DF=3  IDF=1.000\n",
      "machine    DF=2  IDF=1.288\n",
      "text       DF=1  IDF=1.693\n"
     ]
    }
   ],
   "source": [
    "def document_frequency(tokenized_docs, token2idx):\n",
    "    df = np.zeros(len(token2idx), dtype=float)\n",
    "    for doc in tokenized_docs:\n",
    "        for t in set(doc):\n",
    "            if t in token2idx:\n",
    "                df[token2idx[t]] += 1.0\n",
    "    return df\n",
    "\n",
    "N = len(tokenized_docs)\n",
    "DF = document_frequency(tokenized_docs, token2idx)\n",
    "IDF = np.log((1 + N) / (1 + DF)) + 1\n",
    "\n",
    "# quick sanity check\n",
    "for term in [\"learning\", \"pizza\", \"machine\", \"text\"]:\n",
    "    if term in token2idx:\n",
    "        i = token2idx[term]\n",
    "        print(f\"{term:10s} DF={DF[i]:.0f}  IDF={IDF[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76a4b3",
   "metadata": {},
   "source": [
    "## 6. TF-IDF\n",
    "\n",
    "TF-IDF combines **local importance** (TF) with **global importance** (IDF).\n",
    "\n",
    "\\[\n",
    "tfidf(t, d) = tf(t, d) \\cdot idf(t)\n",
    "\\]\n",
    "\n",
    "- A word gets a **high TF-IDF score** if:\n",
    "  - it appears frequently in a document\n",
    "  - and appears in few documents overall\n",
    "\n",
    "- A word gets a **low TF-IDF score** if:\n",
    "  - it appears in almost every document (low discriminative power)\n",
    "\n",
    "TF-IDF is one of the most widely used techniques for classical text representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ef90b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (3, 13)\n"
     ]
    }
   ],
   "source": [
    "TFIDF = TF * IDF  # broadcasting multiplies each column by IDF\n",
    "print(\"TF-IDF matrix shape:\", TFIDF.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7160e",
   "metadata": {},
   "source": [
    "## 7. Document similarity with cosine similarity\n",
    "\n",
    "Cosine similarity:\n",
    "\\[\n",
    "\\cos(\\theta) = \\frac{x \\cdot y}{\\|x\\|\\|y\\|}\n",
    "\\]\n",
    "\n",
    "**Task:** Implement cosine similarity and rank documents for a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e7ecd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: machine learning\n",
      "doc 0  sim=0.638  -> Machine learning is powerful\n",
      "doc 1  sim=0.546  -> Deep learning is a branch of machine learning\n",
      "doc 2  sim=0.145  -> This document is about learning representations of text\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a, b, eps=1e-12):\n",
    "    num = float(np.dot(a, b))\n",
    "    den = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return num / den\n",
    "\n",
    "def tfidf_for_query(query: str, token2idx, idf):\n",
    "    q_tokens = tokenize(query)\n",
    "    q_tf = tf_vector(q_tokens, token2idx)\n",
    "    return q_tf * idf\n",
    "\n",
    "query = \"machine learning\"\n",
    "q_vec = tfidf_for_query(query, token2idx, IDF)\n",
    "\n",
    "sims = [cosine_similarity(q_vec, TFIDF[i]) for i in range(N)]\n",
    "ranking = sorted(list(enumerate(sims)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "for idx, score in ranking:\n",
    "    print(f\"doc {idx}  sim={score:.3f}  -> {docs[idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f74dd",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "Nombre: Álvaro Aguilar Dávila  \n",
    "Asignatura: Apendizaje Automáico  \n",
    "Grado: Ingeniería Informática + ADE  \n",
    "Universidad: CUNEF  \n",
    "Año: 2026"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
