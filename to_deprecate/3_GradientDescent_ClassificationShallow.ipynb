{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a631529-dccb-4592-bf0f-8e0e19eb6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a8aeb96-522c-468e-96f7-16448f2c6618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe7921ed190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## =============== ##\n",
    "## Define our data ##\n",
    "## =============== ##\n",
    "color_c0 = 'C0'\n",
    "color_c1 = 'C1'\n",
    "\n",
    "# input to our model. Represents time in seconds\n",
    "x_data = np.array([0,1.5,2,2.5,3,4,5]).reshape(7,1)\n",
    "# outputs associated to each input. Represents cantidad de lluvia in mm^3\n",
    "t_data = np.array([0,0,0,0,1,1,1]).reshape(7,1)\n",
    "\n",
    "## display\n",
    "idx_class0 = t_data == 0\n",
    "idx_class1 = t_data == 1\n",
    "plt.plot(x_data[idx_class0],t_data[idx_class0],'o', color = color_c0, markersize = 8, label = 'data observations class 0')\n",
    "plt.plot(x_data[idx_class1],t_data[idx_class1],'*', color = color_c1,markersize = 8, label = 'data observations class 1')\n",
    "plt.xlabel('imagen')\n",
    "plt.ylabel('clase asociada')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53fd02e-5b53-4d01-b40d-accc47c79bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======================================================= ##\n",
    "## ======== functionality for computational graph ======== ##\n",
    "## ======================================================= ##\n",
    "\n",
    "## activation function sigmoid\n",
    "def activation_function_sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "    \n",
    "## function that implements the computational graph\n",
    "def computation_graph_linear(x,w,b):\n",
    "    ''' This function represents a computational graph, a neural network, that implements a linear operation'''\n",
    "    # this is the W^0 x from the theory above implemented using a transposition ;)\n",
    "    y = activation_function_sigmoid(np.matmul(x,w) + b)\n",
    "    return y\n",
    "\n",
    "## function that implements the computational graph\n",
    "def computation_graph_linear_just_weight(x,w):\n",
    "    ''' This function represents a computational graph, a neural network, that implements a linear operation, with no weight'''\n",
    "    # this is the W^0 x from the theory above implemented using a transposition ;)\n",
    "    y = activation_function_sigmoid(np.matmul(x,w))\n",
    "    return y\n",
    "\n",
    "## function that initializes the values of a computational graph\n",
    "def create_computation_graph_linear(n_in,n_out):\n",
    "    ''' Create elements of the computational graph'''\n",
    "    # parameters\n",
    "    w = np.random.randn(n_in,n_out) + 1 # get a random value from standard normal distribution\n",
    "    b = np.random.randn(n_out,)*5 # get a random value from Gaussian with mean 0 and standard deviation 5.\n",
    "\n",
    "    return w,b\n",
    "\n",
    "## function implementing brier score loss function (yeah it is like squared loss)\n",
    "def brier_loss_function_just_weight(x,t,w):\n",
    "    y_pred = activation_function_sigmoid(np.matmul(x,w))\n",
    "    return (y_pred-t)**2\n",
    "\n",
    "## function implementing binary cross entropy loss\n",
    "def binary_cross_entropy_loss_function_just_weight(x,t,w):\n",
    "    y_pred = activation_function_sigmoid(np.matmul(x,w))\n",
    "    loss = np.zeros(y_pred.shape)\n",
    "\n",
    "    # expand dimension on t to match\n",
    "    t = np.tile(t, y_pred.shape[0:-2]+(1,1))\n",
    "    \n",
    "    loss[t==1] = t[t==1]*np.log(y_pred[t==1])\n",
    "    loss[t==0] = (1-t[t==0])*np.log(1-y_pred[t==0])\n",
    "    return -1*loss\n",
    "\n",
    "def grad_activation_function_sigmoid(x):\n",
    "    return activation_function_sigmoid(x) * (1-activation_function_sigmoid(x))\n",
    "\n",
    "def grad_brier_loss_just_weight(x,t,w):\n",
    "    ## forward operation\n",
    "    z = np.matmul(x,w)\n",
    "    y_pred = activation_function_sigmoid(z)\n",
    "    \n",
    "    ## Backward operation (compute gradients / backpropagation / reverse mode autodiff), \n",
    "    #  applying chain rule (just one gradient missing for real reverse mode autodiff) :)\n",
    "    dC_dy = 2*(y_pred-t)\n",
    "    dy_dz = grad_activation_function_sigmoid(z)\n",
    "    dz_dw = x\n",
    "\n",
    "    # compute the gradient by applying chain rule: dC/dW = dC/dy * dy/dz * dz/dw\n",
    "    grad_w = np.sum( dC_dy * dy_dz * dz_dw, axis = 0, keepdims = True)\n",
    "    \n",
    "    return grad_w\n",
    "\n",
    "def grad_binary_cross_entropy_loss_just_weight(x,t,w):\n",
    "    # forward operation\n",
    "    z = np.matmul(x,w)\n",
    "    y_pred = activation_function_sigmoid(z)\n",
    "    \n",
    "    ## Backward operation (compute gradients / backpropagation / reverse mode autodiff)\n",
    "    # applying chain rule (just one gradient missing for real reverse mode autodiff) :)\n",
    "    dC_dy = np.zeros((len(y_pred),1))\n",
    "    dC_dy[t==1] = t[t==1]*np.log(1/y_pred[t==1])\n",
    "    dC_dy[t==0] = -(1-t[t==0])*np.log(1/(1-y_pred[t==0])) \n",
    "    dC_dy = -1*dC_dy\n",
    "\n",
    "    dy_dz = grad_activation_function_sigmoid(z)\n",
    "    dz_dw = x\n",
    "\n",
    "    # compute the gradient by applying chain rule: dC/dW = dC/dy * dy/dz * dz/dw\n",
    "    grad_w = np.sum( dC_dy * dy_dz * dz_dw, axis = 0, keepdims = True)\n",
    "     \n",
    "    return grad_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79ed78ba-ebd0-410d-9503-3fcacb6919fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ====================================== ##\n",
    "## ========== Gradient Descent ========== ##\n",
    "## ====================================== ##\n",
    "## select loss function to work at\n",
    "loss_name = 'bce'\n",
    "sleep_time_simulation = 0.5\n",
    "\n",
    "## number of points in the domain used to plot the functions \n",
    "N_points_domain = 100\n",
    "x_range = np.linspace(-1,7, N_points_domain).reshape((N_points_domain,1))\n",
    "\n",
    "## specify our computational graph\n",
    "n_in = 1\n",
    "n_out = 1\n",
    "\n",
    "## first of all draw loss function against a set of parameters\n",
    "w_range = np.linspace(-2,4,500).reshape((500,n_in,n_out))\n",
    "\n",
    "if loss_name == 'brier':\n",
    "    loss_range = brier_loss_function_just_weight(x_data,t_data,w_range)\n",
    "elif loss_name == 'bce':\n",
    "    loss_range = binary_cross_entropy_loss_function_just_weight(x_data,t_data,w_range)\n",
    "else:\n",
    "    raise NotImplementedError(f\"Unkown loss {loss_name}, choose from brier or bce\")\n",
    "    \n",
    "## accumulate loss per datapoint\n",
    "loss_acc_range = np.sum(loss_range, axis = 1)\n",
    "\n",
    "## squeeze and display\n",
    "loss_acc_range = np.squeeze(loss_acc_range)\n",
    "w_range = np.squeeze(w_range)\n",
    "\n",
    "## get some limits for plots\n",
    "y_lim_min = np.min(loss_acc_range) - 0.5*np.std(loss_acc_range)\n",
    "y_lim_max = np.max(loss_acc_range) + 0.5*np.std(loss_acc_range)\n",
    "y_lim_inc = 0.1*np.std(loss_acc_range)\n",
    "\n",
    "# plot grid\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 6))\n",
    "\n",
    "# display loss function\n",
    "ax1.plot(w_range, loss_acc_range, color = 'C0')\n",
    "ax1.set_xlabel('Weight')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_ylim([y_lim_min,y_lim_max])\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.array([-0.75]).reshape(n_in,n_out) # initialize at 4.\n",
    "\n",
    "## gradient descent parameters\n",
    "lr = 0.1\n",
    "epochs = 20\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    ## forward plus backward\n",
    "    if loss_name == 'brier':\n",
    "        grad_w = grad_brier_loss_just_weight(x_data,t_data,w)\n",
    "    else:\n",
    "        grad_w = grad_binary_cross_entropy_loss_just_weight(x_data,t_data,w)\n",
    "\n",
    "    ## compute function at current parameter value\n",
    "    function = computation_graph_linear_just_weight(x_range, w)\n",
    "\n",
    "    ## compute predictions at current parameter value\n",
    "    y_pred = computation_graph_linear_just_weight(x_data, w)\n",
    "\n",
    "    ## compute loss at current parameter value\n",
    "    if loss_name == 'brier':\n",
    "        loss = brier_loss_function_just_weight(x_data,t_data,w)\n",
    "    else:\n",
    "        loss = binary_cross_entropy_loss_function_just_weight(x_data,t_data,w)\n",
    "    \n",
    "    loss_acc = np.sum(loss)\n",
    "\n",
    "    ## get the gradient function at the point w (tangent at the point)\n",
    "    gradient_function_w_at_current_w = grad_w * w_range + loss_acc - grad_w * w\n",
    "\n",
    "    ## ============= ##\n",
    "    ## ============= ##\n",
    "    ## START DRAWING ##\n",
    "    ## ============= ##\n",
    "    ## ============= ##\n",
    "    # Clear previous data\n",
    "    ax1.clear()  \n",
    "    ax2.clear()\n",
    "    \n",
    "    w_plot = np.squeeze(w)\n",
    "    grad_w_plot = np.squeeze(grad_w)\n",
    "    x_data_plot = np.squeeze(x_data)\n",
    "    t_data_plot = np.squeeze(t_data)\n",
    "    y_pred_plot = np.squeeze(y_pred)\n",
    "    loss_plot = np.squeeze(loss)\n",
    "    \n",
    "    # get new weight after grad descent. Just for illustration purposes, the real step is done at the end of the loop\n",
    "    w_new_plot = np.squeeze(w-lr*grad_w)\n",
    "\n",
    "    ## ================ ##\n",
    "    ## function picture ##\n",
    "    idx_class0 = t_data == 0\n",
    "    idx_class1 = t_data == 1\n",
    "    ax2.plot(x_range,function, color = 'C3', label = 'function: y = sigmoid(w*x)')\n",
    "    ax2.plot(x_data[idx_class0],t_data[idx_class0],'o', color = color_c0, markersize = 8, label = 'data observations class 0')\n",
    "    ax2.plot(x_data[idx_class1],t_data[idx_class1],'*', color = color_c1,markersize = 8, label = 'data observations class 1')\n",
    "    \n",
    "    ## plot loss associated at each point and draw line between dots to highliht what the loss measures\n",
    "    for idx, (xi, ti, yi, sl) in enumerate(zip(x_data_plot,t_data_plot,y_pred_plot,loss_plot)):\n",
    "        if idx == 0:\n",
    "            ax2.plot(xi,yi, 'x', color = 'C2', label = 'network prediction')\n",
    "        else:\n",
    "            ax2.plot(xi,yi, 'x', color = 'C2')\n",
    "        ax2.plot([xi,xi], [ti, yi], '--',color = f\"C1\", alpha = 0.5)\n",
    "        ax2.text(xi, yi, f'{sl:.2f}', fontsize=12, va='top', color = f\"C2\" ) \n",
    "\n",
    "    # label function with the weight at that moment\n",
    "    ax2.text(x_range[-20],function[-20], f'w = {w_plot}', color = 'k', fontsize = 12)\n",
    "    \n",
    "    ax2.text(1, 1.65, f\"Iteration {e}, {loss_name} loss = {loss_acc:.3f}\", fontsize=12, va='bottom', color = f\"C2\" ) \n",
    "    ax2.set_xlabel('imagen')\n",
    "    ax2.set_ylabel('clase asociada')\n",
    "    ax2.set_ylim([-0.5,1.5])\n",
    "    ax2.legend()\n",
    "\n",
    "    ## draw\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    \n",
    "    ## ===================== ##\n",
    "    ## loss function picture ##\n",
    "    \n",
    "    ## 0. label and axis limits\n",
    "    ax1.set_xlabel('Weight')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_ylim([y_lim_min,y_lim_max])\n",
    "\n",
    "    ## 1. display loss function\n",
    "    ax1.plot(w_range, loss_acc_range, color = 'C0', label = 'loss', zorder = 20)\n",
    "    \n",
    "    ## 2. display current weight\n",
    "    ax1.plot(w_plot, y_lim_min + 0.5*y_lim_inc, '*', color = 'C1', label = 'current weight', zorder = 50, markersize = 10)\n",
    "    ax1.text(w_plot, y_lim_min + 0.5*y_lim_inc , f\"w = {w_plot:.3f}\", fontsize=12, va='bottom', color = f\"C1\" , zorder = 50)\n",
    "    ax1.legend()\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(sleep_time_simulation)\n",
    "\n",
    "    ## animation by drawing horizontal lines on current parameter and updated parameter values\n",
    "    ax1.vlines(np.squeeze(w), ymin=0, ymax=loss_acc, color='k', linestyles='dotted', zorder = -50)\n",
    "\n",
    "    ## 3. display current loss\n",
    "    ax1.plot(w_plot, loss_acc, 'o', color = 'C0', label = 'loss at current weight', zorder = 20)\n",
    "    ax1.text(w_plot + 0.5, loss_acc , f\"loss = {loss_acc:.3f}\", fontsize=12, va='bottom', color = \"C0\" , zorder = 50)\n",
    "    ax1.legend()\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(sleep_time_simulation)\n",
    "    \n",
    "    ## 4. display the gradient function\n",
    "    ax1.plot(w_range, np.squeeze(gradient_function_w_at_current_w), color = 'C2', label = 'gradient function: f(w) = grad_w * w + loss - grad_w * w', zorder = 20)\n",
    "    ax1.text(w_range[-1], np.squeeze(gradient_function_w_at_current_w)[-1], f\"grad_w = {grad_w_plot:.3f}\", fontsize=12, va='bottom', color = f\"C2\" , zorder = 200) \n",
    "    ax1.legend()\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(sleep_time_simulation)\n",
    "\n",
    "    ## draw rest of lines to show update\n",
    "    ax1.hlines(y = loss_acc, xmin=w_new_plot, xmax=w_plot, color='k', linestyles='dotted', zorder = -50)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(sleep_time_simulation)\n",
    "    \n",
    "    ax1.vlines(w_new_plot, ymin=y_lim_min, ymax=loss_acc, color='k', linestyles='dotted', zorder = -50)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(sleep_time_simulation)\n",
    "    \n",
    "    ## 5. display new weight\n",
    "    ax1.plot(w_new_plot, y_lim_min+ 0.5*y_lim_inc, '*', color = 'C3', label = 'updated weight: w_new = w - lr*grad_w', zorder = 200, markersize = 10)\n",
    "    ax1.text(w_new_plot, y_lim_min+1.5*y_lim_inc, f\"w_new = {w_plot:.3f} -{lr:.3f}*{grad_w_plot:.3f} = {w_plot-lr*grad_w_plot:.3f}\", fontsize=12, va='bottom', color = f\"C3\" , zorder = 200) \n",
    "    ax1.legend()\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    time.sleep(sleep_time_simulation)\n",
    "\n",
    "    ## wait to see\n",
    "    time.sleep(sleep_time_simulation)\n",
    "\n",
    "    ## update parameter with gradient descent, for the next update\n",
    "    w = w-lr*grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11adf6da-e445-4afd-94af-de6f4adaf3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c925e1-5658-4237-ab8a-2bc061b0c5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "GPFLOW",
   "language": "python",
   "name": "gpflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
